{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Greenmask","text":""},{"location":"#dump-anonymization-and-synthetic-data-generation-tool","title":"Dump anonymization and synthetic data generation tool","text":"<p>Greenmask is a powerful open-source utility that is designed for logical database backup dumping, anonymization, synthetic data generation and restoration. It has ported PostgreSQL libraries, making it reliable. It is stateless and does not require any changes to your database schema. It is designed to be highly customizable and backward-compatible with existing PostgreSQL utilities, fast and reliable.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Deterministic transformers   \u2014 deterministic approach to data transformation based on the hash   functions. This ensures that the same input data will always produce the same output data. Almost each transformer   supports either <code>random</code> or <code>hash</code> engine making it universal for any use case.</li> <li>Dynamic parameters \u2014 almost each   transformer supports dynamic parameters, allowing to parametrize the   transformer dynamically from the table column value. This is helpful for resolving the functional dependencies   between columns and satisfying the constraints.</li> <li>Transformation validation and easy maintainable - During   configuration process, Greenmask provides validation   warnings, data transformation diff and schema diff features, allowing you to monitor and maintain transformations   effectively   throughout the software lifecycle. Schema diff helps to avoid data leakage when schema changed.</li> <li>Partitioned tables transformation inheritance   \u2014 Define transformation configurations once and apply them to all   partitions within partitioned tables (using <code>apply_for_inherited</code> parameter), simplifying the anonymization process.</li> <li>Stateless - Greenmask operates as a logical dump and does not impact your existing database schema.</li> <li>Cross-platform - Can be easily built and executed on any platform, thanks to its Go-based architecture,   which eliminates platform dependencies.</li> <li>Database type safe - Ensures data integrity by validating data and utilizing the database driver for   encoding and decoding operations. This approach guarantees the preservation of data formats.</li> <li>Backward compatible - It fully supports the same features and protocols as existing vanilla PostgreSQL utilities.   Dumps created by Greenmask can be successfully restored using the pg_restore utility.</li> <li>Extensible - Users have the flexibility   to implement domain-based transformations   in any programming language or   use predefined templates.</li> <li>Integrable - Integrate seamlessly into your CI/CD system for automated database anonymization and   restoration.</li> <li>Parallel execution - Take advantage of parallel dumping and restoration, significantly reducing the time required   to deliver results.</li> <li>Provide variety of storages - offers a variety of storage options for local and remote data storage,   including directories and S3-like storage solutions.</li> <li>Pgzip support for faster compression \u2014 by   setting <code>--pgzip</code>, it can speeds up the dump and restoration   processes through parallel compression.</li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<p>Greenmask is ideal for various scenarios, including:</p> <ul> <li>Backup and restoration. Use Greenmask for your daily routines involving logical backup dumping and restoration. It   seamlessly handles tasks like table restoration after truncation. Its functionality closely mirrors that of pg_dump   and pg_restore, making it a straightforward replacement.</li> <li>Anonymization, transformation, and data masking. Employ Greenmask for anonymizing, transforming, and masking   backups, especially when setting up a staging environment or for analytical purposes. It simplifies the deployment of   a pre-production environment with consistently anonymized data, facilitating faster time-to-market in the development   lifecycle.</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>Greenmask Roadmap</li> <li>Email</li> <li>Twitter</li> <li>Telegram</li> <li>Discord</li> <li>DockerHub</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#introduction","title":"Introduction","text":"<p>It is evident that the most appropriate approach for executing logical backup dumping and restoration is by leveraging the core PostgreSQL utilities, specifically <code>pg_dump</code> and <code>pg_restore</code>. Greenmask has been purposefully designed to align with PostgreSQL's native utilities, ensuring compatibility. Greenmask primarily handles data dumping operations independently and delegates the responsibilities of schema dumping and restoration to <code>pg_dump</code> and <code>pg_restore</code> respectively, maintaining seamless integration with PostgreSQL's standard tools.</p>"},{"location":"architecture/#backup-process","title":"Backup process","text":"<p>The process of backing up PostgreSQL databases is divided into three distinct sections:</p> <ul> <li>Pre-data \u2014 this section encompasses the raw schema of tables, excluding primary keys (PK) and foreign keys (FK).</li> <li>Data \u2014 the data section contains the actual table data in COPY format, including information about sequence current values and Large Objects data.</li> <li>Post-data \u2014 in this section, you'll find the definitions of indexes, triggers, rules, and constraints (such as PK and FK).</li> </ul> <p>Greenmask focuses exclusively on the data section during runtime. It delegates the handling of the <code>pre-data</code> and <code>post-data</code> sections to the core PostgreSQL utilities, <code>pg_dump</code> and <code>pg_restore</code>.</p> <p>Greenmask employs the directory format of <code>pg_dump</code> and <code>pg_restore</code>. This format is particularly suitable for parallel execution and partial restoration, and it includes clear metadata files that aid in determining the backup and restoration steps. Greenmask has been optimized to work seamlessly with remote storage systems and anonymization procedures.</p> <p>When performing data dumping, Greenmask utilizes the COPY command in TEXT format, maintaining reliability and compatibility with the vanilla PostgreSQL utilities.</p> <p>Additionally, Greenmask supports parallel execution, significantly reducing the time required for the dumping process.</p>"},{"location":"architecture/#storage-options","title":"Storage options","text":"<p>The core PostgreSQL utilities, <code>pg_dump</code> and <code>pg_restore</code>, traditionally operate with files in a directory format, offering no alternative methods. To meet modern backup requirements and provide flexible approaches, Greenmask introduces the concept of storages.</p> <ul> <li><code>s3</code> \u2014 this option supports any S3-like storage system, including AWS S3, which makes it versatile and adaptable to various cloud-based storage solutions.</li> <li><code>directory</code> \u2014 this is the standard choice, representing the ordinary filesystem directory for local storage.</li> </ul>"},{"location":"architecture/#restoration-process","title":"Restoration process","text":"<p>In the restoration process, Greenmask combines the capabilities of different tools:</p> <ul> <li>For schema restoration Greenmask utilizes <code>pg_restore</code> to restore the database schema. This ensures that the schema is accurately reconstructed.</li> <li>For data restoration Greenmask independently applies the data using the COPY protocol. This allows Greenmask to handle the data efficiently, especially when working with various storage solutions. Greenmask is aware of the restoration metadata, which enables it to download only the necessary data. This feature is particularly useful for partial restoration scenarios, such as restoring a single table from a complete backup.</li> </ul> <p>Greenmask also supports parallel restoration, which can significantly reduce the time required to complete the restoration process. This parallel execution enhances the efficiency of restoring large datasets.</p>"},{"location":"architecture/#data-anonymization-and-validation","title":"Data anonymization and validation","text":"<p>Greenmask works with COPY lines, collects schema metadata using the Golang driver, and employs this driver in the encoding and decoding process. The validate command offers a way to assess the impact on both schema (validation warnings) and data (transformation and displaying differences). This command allows you to validate the schema and data transformations, ensuring the desired outcomes during the anonymization process.</p>"},{"location":"architecture/#customization","title":"Customization","text":"<p>If your table schema relies on functional dependencies between columns, you can address this challenge using the TemplateRecord transformer. This transformer enables you to define transformation logic for entire tables, offering type-safe operations when assigning new values.</p> <p>Greenmask provides a framework for creating your custom transformers, which can be reused efficiently. These transformers can be seamlessly integrated without requiring recompilation, thanks to the PIPE (stdin/stdout) interaction.</p> <p>Note</p> <p>Furthermore, Greenmask's architecture is designed to be highly extensible, making it possible to introduce other interaction protocols, such as HTTP or Socket, for conducting anonymization procedures.</p>"},{"location":"architecture/#postgresql-version-compatibility","title":"PostgreSQL version compatibility","text":"<p>Greenmask is compatible with PostgreSQL versions 11 and higher.</p>"},{"location":"configuration/","title":"Configuration","text":"<pre><code># Configuration\n</code></pre> <p>The configuration is organized into six sections:</p> <ul> <li><code>common</code> \u2014 settings that can be used for both the <code>dump</code> and <code>restore</code> commands</li> <li><code>log</code> \u2014 settings for the logging subsystem</li> <li><code>storage</code> \u2014 settings for the storage locations where dumps are stored</li> <li><code>dump</code> \u2014 settings for the <code>dump</code> command. This section includes <code>pg_dump</code> options and transformation parameters.</li> <li><code>restore</code> \u2014 settings for the <code>restore</code> command. It contains <code>pg_restore</code> options and additional restoration   scripts.</li> <li><code>custom_transformers</code> \u2014 definitions of the custom transformers that interact through <code>stdin</code> and <code>stdout</code>. Once a custom transformer is configured, it becomes accessible via the <code>greenmask list-transformers</code> command.</li> </ul>"},{"location":"configuration/#common-section","title":"<code>common</code> section","text":"<p>In the <code>common</code> section of the configuration, you can specify the following settings:</p> <ul> <li><code>pg_bin_path</code> \u2014 path to the PostgreSQL binaries. Note that the PostgreSQL server version must match the provided binaries.</li> <li><code>tmp_dir</code> \u2014 temporary directory for storing the table of contents files. Default value is <code>/tmp</code></li> </ul> <p>Note</p> <p>Greenmask exclusively manages data dumping and data restoration processes, delegating schema dumping to the <code>pg_dump</code>utility and schema restoration to the <code>pg_restore</code> utility. Both <code>pg_dump</code> and <code>pg_restore</code> rely on a <code>toc.dat</code> file located in a specific directory, which contains metadata and object definitions. Therefore, the <code>tmp_dir</code> parameter is essential for storing the <code>toc.dat</code> file during the dumping or restoration procedure. It is important to note that all artifacts in this directory will be automatically deleted once the Greenmask command is completed.</p>"},{"location":"configuration/#log-section","title":"<code>log</code> section","text":"<p>In the <code>log</code> section of the configuration, you can specify the following settings:</p> <ul> <li><code>level</code> \u2014 specifies the level of logging, which can be one of the following: <code>debug</code>, <code>info</code>, or <code>error</code>. The default level is <code>info</code>.</li> <li><code>format</code> \u2014 defines the logging format, which can be either <code>json</code> or <code>text</code>. The default format is <code>text</code>.</li> </ul>"},{"location":"configuration/#storage-section","title":"<code>storage</code> section","text":"<p>In the <code>storage</code> section, you can configure the storage driver for storing the dumped data. Currently, two storage <code>type</code> options are supported: <code>directory</code> and <code>s3</code>.</p> <code>directory</code> option<code>s3</code> option <p>The directory storage option refers to a filesystem directory where the dump data will be stored.</p> <p>Parameters include <code>path</code> which specifies the path to the directory in the filesystem where the dumps will be stored.</p> directory storage config example<pre><code>storage:\n  type: \"directory\"\n  directory:\n    path: \"/home/user_name/storage_dir\" # (1)\n</code></pre> <p>By choosing the <code>s3</code> storage option, you can store dump data in an S3-like remote storage service, such as Amazon S3 or Azure Blob Storage. Here are the parameters you can configure for S3 storage:</p> <ul> <li><code>endpoint</code> \u2014 overrides the default AWS endpoint to a custom one for making requests</li> <li><code>bucket</code> \u2014 the name of the bucket where the dump data will be stored</li> <li><code>prefix</code> \u2014 a prefix for objects in the bucket, specified in path format</li> <li><code>region</code> \u2014 the S3 service region</li> <li><code>storage_class</code> \u2014 the storage class for performing object requests</li> <li><code>no_verify_ssl</code> \u2014 disable SSL certificate verification</li> <li><code>access_key_id</code> \u2014 access key for authentication</li> <li><code>secret_access_key</code> \u2014 secret access key for authentication</li> <li><code>session_token</code> \u2014 session token for authentication</li> <li><code>role_arn</code> \u2014 Amazon resource name for role-based authentication</li> <li><code>session_name</code> \u2014 role session name to uniquely identify a session</li> <li><code>max_retries</code> \u2014 the number of retries on request failures</li> <li><code>cert_file</code> \u2014 the path to the SSL certificate for making requests</li> <li><code>max_part_size</code> \u2014 the maximum part length for one request</li> <li><code>concurrency</code> \u2014 the number of goroutines to use in parallel for each upload call when sending parts</li> <li><code>use_list_objects_v1</code> \u2014 use the old v1 <code>ListObjects</code> request instead of v2 one</li> <li><code>force_path_style</code> \u2014 force the request to use path-style addressing (e. g., <code>http://s3.amazonaws.com/BUCKET/KEY</code>) instead of virtual hosted bucket addressing (e. g., <code>http://BUCKET.s3.amazonaws.com/KEY</code>)</li> <li><code>use_accelerate</code> \u2014 enable S3 Accelerate feature</li> </ul> s3 storage config example for Minio running in Docker<pre><code>storage:  \n  type: \"s3\"\n  s3:\n    endpoint: \"http://localhost:9000\"\n    bucket: \"testbucket\"\n    region: \"us-east-1\"\n    access_key_id: \"Q3AM3UQ867SPQQA43P2F\"\n    secret_access_key: \"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\"\n</code></pre>"},{"location":"configuration/#dump-section","title":"<code>dump</code> section","text":"<p>In the <code>dump</code> section of the configuration, you configure the <code>greenmask dump</code> command. It includes the following parameters:</p> <ul> <li><code>pg_dump_options</code> \u2014 a map of <code>pg_dump</code> options to configure the behavior of the command itself. You can refer to the list of supported <code>pg_dump</code> options in the Greenmask dump command documentation.</li> <li> <p><code>transformation</code> \u2014 this section contains configuration for applying transformations to table columns during the dump operation. It includes the following sub-parameters:</p> <ul> <li><code>schema</code> \u2014 the schema name of the table</li> <li><code>name</code> \u2014 the name of the table</li> <li><code>subset_conds</code> - list of the conditions to filter the rows to be dumped. The conditions are combined with <code>AND</code> operator. For details read Database subset</li> <li> <p><code>query</code> \u2014 an optional parameter for specifying a custom query to be used in the COPY command. By default, the entire table is dumped, but you can use this parameter to set a custom query.</p> <p>Warning</p> <p>Be cautious when using the <code>query</code> parameter, as it may lead to constraint violation errors during restoration, and Greenmask currently cannot handle query validation.</p> </li> <li> <p><code>columns_type_override</code> \u2014 allows you to override the column types explicitly. You can associate a column with another type that is supported by your transformer. This is useful when the transformer works strictly with specific types of columns. For example, if a column named <code>post_code</code> is of the TEXT type, but the <code>RandomInt</code> transformer works only with INT family types, you can override it as shown in the example provided.   column type overridden example<pre><code>  columns_type_override:\n    post_code: \"int4\"  # (1)\n</code></pre></p> <ol> <li>Change the data type of the post_code column to <code>INT4</code> (<code>INTEGER</code>)</li> </ol> </li> <li> <p><code>apply_for_inherited</code> \u2014 an optional parameter to apply the same transformation to all partitions if the table is partitioned. This can save you from defining the transformation for each partition manually.</p> <p>Warning</p> <p>It is recommended to use the <code>--load-via-partition-root</code> parameter when dealing with partitioned tables, as the partition key value might change.</p> </li> <li> <p><code>transformers</code> \u2014 a list of transformers to apply to the table, along with their parameters. Each transformation item includes the following sub-parameters:</p> <ul> <li><code>name</code> \u2014 the name of the transformer</li> <li><code>params</code> \u2014 a map of the provided transformer parameters</li> </ul> transformers config example<pre><code>   transformers:\n    - name: \"RandomDate\"\n      params:\n        min: \"2023-01-01 00:00:00.0+03\"\n        max: \"2023-01-02 00:00:00.0+03\"\n        column: \"scheduled_departure\"\n\n    - name: \"NoiseDate\"\n      params:\n        ratio: \"01:00:00\"\n        column: \"scheduled_arrival\"\n</code></pre> </li> </ul> </li> </ul> <p>Here is an example configuration for the <code>dump</code> section:</p> <p>dump section config example<pre><code>dump:\n  pg_dump_options:\n    dbname: \"host=/run/postgresql user=postgres dbname=demo\"\n    jobs: 10\n    exclude-schema: \"(\\\"teSt\\\"*|test*)\"\n    table: \"bookings.flights\"\n    load-via-partition-root: true\n\n  transformation:\n    - schema: \"bookings\"\n      name: \"flights\"\n      query: \"select * from bookings.flights3 limit 1000000\"\n      columns_type_override:\n        post_code: \"int4\" # (1)\n      transformers:\n        - name: \"RandomDate\"\n          params:\n            min: \"2023-01-01 00:00:00.0+03\"\n            max: \"2023-01-02 00:00:00.0+03\"\n            column: \"scheduled_departure\"\n\n        - name: \"NoiseDate\"\n          params:\n            ratio: \"01:00:00\"\n            column: \"scheduled_arrival\"\n\n        - name: \"RegexpReplace\"\n          params:\n            column: \"status\"\n            regexp: \"On Time\"\n            replace: \"Delayed\"\n\n        - name: \"RandomInt\" # (2)\n          params:\n            column: \"post_code\"\n            min: \"11\"\n            max: \"99\"\n\n    - schema: \"bookings\"\n      name: \"aircrafts_data\"\n      subset_conds: # (3)\n        - \"bookings.aircrafts_data.model = 'Boeing 777-300-2023'\"\n      transformers:\n        - name: \"Json\"\n          params:\n            column: \"model\"\n            operations:\n              - operation: \"set\"\n                path: \"en\"\n                value: \"Boeing 777-300-2023\"\n              - operation: \"set\"\n                path: \"crewSize\"\n                value: 10\n\n        - name: \"NoiseInt\"\n          params:\n            ratio: 0.9\n            column: \"range\"\n</code></pre></p> <ol> <li>Override the <code>post_code</code> column type to <code>int4</code> (INTEGER). This is necessary because the <code>post_code</code> column    originally has a <code>TEXT</code> type, but it contains values that resemble integers. By explicitly overriding the type to <code>int4</code>, we ensure compatibility with transformers that work with integer types, such as <code>RandomInt</code>.</li> <li>After the type is overridden, we can apply a compatible transformer.</li> <li>Database subset condition applied to the <code>aircrafts_data</code> table. The subset condition filters the data based on the <code>model</code> column.</li> </ol>"},{"location":"configuration/#validate-section","title":"<code>validate</code> section","text":"<p>In the <code>validate</code> section of the configuration, you can specify parameters for the <code>greenmask validate</code> command. Here is an example of the validate section configuration:</p> <p>validate section config example<pre><code>validate:\n  tables: # (1)\n    - \"orders\"\n    - \"public.cart\"\n  data: true # (2)\n  diff: true # (3)\n  rows_limit: 10 # (4)\n  resolved_warnings: # (5)\n    - \"8d436fae67b2b82b36bd3afeb0c93f30\"\n  table_format: \"horizontal\" # (7)\n  format: \"text\" # (6)\n  schema: true # (8)\n  transformed_only: true # (9)\n  warnings: true # (10)\n</code></pre></p> <ol> <li>A list of tables to validate. If this list is not empty, the validation operation will only be performed for the specified tables. Tables can be written with or without the schema name (e. g., <code>\"public.cart\"</code> or <code>\"orders\"</code>).</li> <li>Specifies whether to perform data transformation for a limited set of rows. If set to <code>true</code>, data transformation will be performed, and the number of rows transformed will be limited to the value specified in the <code>rows_limit</code> parameter (default is <code>10</code>).</li> <li>Specifies whether to perform diff operations for the transformed data. If set to <code>true</code>, the validation process will find the differences between the original and transformed data. See more details in the validate command documentation.</li> <li>Limits the number of rows to be transformed during validation. The default limit is <code>10</code> rows, but you can change it by modifying this parameter.</li> <li>A hash list of resolved warnings. These warnings have been addressed and resolved in a previous validation run.</li> <li>Specifies the format of the transformation output. Possible values are <code>[horizontal|vertical]</code>. The default format is <code>horizontal</code>. You can choose the format that suits your needs. See more details in the validate command documentation.</li> <li>The output format (json or text)</li> <li>Specifies whether to validate the schema current schema with the previous and print the differences if any.</li> <li>If set to <code>true</code>, transformation output will be only with the transformed columns and primary keys</li> <li>If set to then all the warnings be printed</li> </ol>"},{"location":"configuration/#restore-section","title":"<code>restore</code> section","text":"<p>In the <code>restore</code> section of the configuration, you can specify parameters for the <code>greenmask restore</code> command. It contains <code>pg_restore</code> settings and custom script execution settings. Below you can find the available parameters:</p> <ul> <li><code>pg_restore_options</code> \u2014 a map of <code>pg_restore</code> options that are used to configure the behavior of   the <code>pg_restore</code> utility during the restoration process. You can refer to the list of supported <code>pg_restore</code> options in the Greenmask restore command documentation.</li> <li><code>scripts</code> \u2014 a map of custom scripts to be executed during different restoration stages. Each script is associated with a specific restoration stage and includes the following attributes:<ul> <li><code>[pre-data|data|post-data]</code> \u2014 the name of the restoration stage when the script should be executed; has the following parameters:<ul> <li><code>name</code> \u2014 the name of the script</li> <li><code>when</code> \u2014 specifies when to execute the script, which can be either <code>\"before\"</code> or <code>\"after\"</code> the   specified restoration stage</li> <li><code>query</code> \u2014 an SQL query string to be executed</li> <li><code>query_file</code> \u2014 the path to an SQL query file to be executed</li> <li><code>command</code> \u2014 a command with parameters to be executed. It is provided as a list, where the first item is the command name.</li> </ul> </li> </ul> </li> <li><code>insert_error_exclusions</code> \u2014 a list of error codes that should be ignored during the restoration process. This is  useful when you want to skip specific errors that are not critical for the restoration process.</li> </ul> <p>As mentioned in the architecture, a backup contains three sections: pre-data, data, and post-data. The custom script execution allows you to customize and control the restoration process by executing scripts or commands at specific stages. The available restoration stages and their corresponding execution conditions are as follows:</p> <ul> <li><code>pre-data</code> \u2014 scripts or commands can be executed before or after restoring the pre-data section</li> <li><code>data</code> \u2014 scripts or commands can be executed before or after restoring the data section</li> <li><code>post-data</code> \u2014 scripts or commands can be executed before or after restoring the post-data section</li> </ul> <p>Each stage can have a <code>\"when\"</code> condition with one of the following possible values:</p> <ul> <li><code>before</code> \u2014 execute the script or SQL command before the mentioned restoration stage</li> <li><code>after</code> \u2014 execute the script or SQL command after the mentioned restoration stage</li> </ul> <p>Below you can find one of the possible versions for the <code>scripts</code> part of the <code>restore</code> section:</p> <p>scripts definition example<pre><code>scripts:\n  pre-data: # (1)\n    - name: \"pre-data before script [1] with query\"\n      when: \"before\"\n      query: \"create table script_test(stage text)\"\n    - name: \"pre-data before script [2]\"\n      when: \"before\"\n      query: \"insert into script_test values('pre-data before')\"\n    - name: \"pre-data after test script [1]\"\n      when: \"after\"\n      query: \"insert into script_test values('pre-data after')\"\n    - name: \"pre-data after script with query_file [1]\"\n      when: \"after\"\n      query_file: \"pre-data-after.sql\"\n  data: # (2)\n    - name: \"data before script with command [1]\"\n      when: \"before\"\n      command: # (4)\n        - \"data-after.sh\"\n        - \"param1\"\n        - \"param2\"\n    - name: \"data after script [1]\"\n      when: \"after\"\n      query_file: \"data-after.sql\"\n  post-data: # (3)\n    - name: \"post-data before script [1]\"\n      when: \"before\"\n      query: \"insert into script_test values('post-data before')\"\n    - name: \"post-data after script with query_file [1]\"\n      when: \"after\"\n      query_file: \"post-data-after.sql\"\n</code></pre></p> <ol> <li>List of pre-data stage scripts. This section contains scripts that are executed before or after the restoration of the pre-data section. The scripts include SQL queries and query files.</li> <li>List of data stage scripts. This section contains scripts that are executed before or after the restoration of the data section. The scripts include shell commands with parameters and SQL query files.</li> <li>List of post-data stage scripts. This section contains scripts that are executed before or after the restoration of the post-data section. The scripts include SQL queries and query files.</li> <li>Command in the first argument and the parameters in the rest of the list. When specifying a command to be executed in the scripts section, you provide the command name as the first item in a list, followed by any parameters or arguments for that command. The command and its parameters are provided as a list within the script configuration.</li> </ol>"},{"location":"configuration/#restoration-error-exclusion","title":"restoration error exclusion","text":"<p>You can configure which errors to ignore during the restoration process by setting the insert_error_exclusions parameter. This parameter can be applied globally or per table. If both global and table-specific settings are defined, the table-specific settings will take precedence. Below is an example of how to configure the insert_error_exclusions parameter. You can specify constraint names from your database schema or the error codes returned by PostgreSQL. codes in the PostgreSQL documentation.</p> parameter defintion<pre><code>insert_error_exclusions:\n\n  global:\n    error_codes: [\"23505\"] # (1)\n    constraints: [\"PK_ProductReview_ProductReviewID\"] # (2)\n  tables: # (3)\n    - schema: \"production\"\n      name: \"productreview\"\n      constraints: [\"PK_ProductReview_ProductReviewID\"]\n      error_codes: [\"23505\"]\n</code></pre> <ol> <li>List of strings that contains postgresql error codes</li> <li>List of strings that contains constraint names (globally)</li> <li>List of tables with their schema, name, constraints, and error codes</li> </ol> <p>Here is an example configuration for the <code>restore</code> section:</p> <pre><code>restore:\n  scripts:\n      pre-data: # (1)\n        - name: \"pre-data before script [1] with query\"\n          when: \"before\"\n          query: \"create table script_test(stage text)\"\n\n  insert_error_exclusions:\n    tables:\n      - schema: \"production\"\n        name: \"productreview\"\n        constraints:\n          - \"PK_ProductReview_ProductReviewID\"\n        error_codes:\n          - \"23505\"\n    global:\n      error_codes:\n        - \"23505\"\n\n  pg_restore_options:\n    jobs: 10\n    exit-on-error: false\n    dbname: \"postgresql://postgres:example@localhost:54316/transformed\"\n    table: \n      - \"productreview\"\n    pgzip: true\n    inserts: true\n    on-conflict-do-nothing: true\n    restore-in-order: true\n</code></pre>"},{"location":"configuration/#environment-variable-configuration","title":"Environment variable configuration","text":"<p>It's also possible to configure Greenmask through environment variables. </p> <p>Greenmask will automatically parse any environment variable that matches the configuration in the config file by substituting the dot (<code>.</code>) separator for an underscore (<code>_</code>) and uppercasing it. As an example, the config file below would apply the same configuration as defining the <code>LOG_LEVEL=debug</code> environment variable</p> config.yaml<pre><code>log:\n  level: debug\n</code></pre>"},{"location":"configuration/#global-configuration-variables","title":"Global configuration variables","text":"<ul> <li><code>GREENMASK_GLOBAL_SALT</code> - global salt value hex encoded with variadic length, used for the <code>hash</code> engine. For details   read Transformation engines section.</li> </ul>"},{"location":"configuration/#postgres-connection-variables","title":"Postgres connection variables","text":"<p>Additionaly, there are some environment variables exposed by the <code>dump</code> and <code>restore</code> commands to facilitate the connection configuration with a Postgres database</p> <ul> <li><code>PGHOST</code> - host used to connect to the postgres database</li> <li><code>PGPORT</code> - port where postgres is exposed</li> <li><code>PGDATABASE</code> - name of the database to dump/restore</li> <li><code>PGUSER</code> - username used to connect to the postgres database</li> <li><code>PGPASSWORD</code> - password used to authenticate to the postgres database</li> </ul>"},{"location":"database_subset/","title":"Database subset","text":"<p>Greenmask allows you to define a subset condition for filtering data during the dump process. This feature is useful when you need to dump only a part of the database, such as a specific table or a set of tables. It automatically ensures data consistency by including all related data from other tables that are required to maintain the integrity of the subset. The subset condition can be defined using <code>subset_conds</code> attribute that can be defined on the table in the <code>transformation</code> section (see examples).</p> <p>Info</p> <p>Greenmask genrates queries for subset conditions based on the introspected schema using joins and recursive queries. It cannot be responsible for query optimization. The subset quries might be slow due to the complexity of the queries and/or lack of indexes. Circular are resolved using recursive queries.</p>"},{"location":"database_subset/#detail","title":"Detail","text":"<p>The subset is a list of SQL conditions that are applied to table. The conditions are combined with <code>AND</code> operator. You need to specify the schema, table and column name when pointing out the column to filter by to avoid ambiguity. The subset condition must be a valid SQL condition.</p> Subset condition example<pre><code>subset_conds:\n  - 'person.businessentity.businessentityid IN (274, 290, 721, 852)'\n</code></pre>"},{"location":"database_subset/#use-cases","title":"Use cases","text":"<ul> <li>Database scale down - create anonymized dump but for the limited and consistent set of tables</li> <li>Data migration - migrate only some records from one database to another</li> <li>Data anonymization - dump and anonymize only a specific records in the database</li> <li>Database catchup - catchup your another instance of database logically by adding a new records. In this case it   is recommended to restore tables in topological order using   <code>--restore-in-order</code>.</li> </ul>"},{"location":"database_subset/#references-with-null-values","title":"References with NULL values","text":"<p>For references that do not have <code>NOT NULL</code> constraints, Greenmask will automatically generate <code>LEFT JOIN</code> queries with the appropriate conditions to ensure integrity checks. You can rely on Greenmask to handle such cases correctly\u2014no special configuration is needed, as it performs this automatically based on the introspected schema.</p>"},{"location":"database_subset/#circular-reference","title":"Circular reference","text":"<p>Greenmask supports circular references between tables. You can define a subset condition for any table, and Greenmask will automatically generate the appropriate queries for the table subset using recursive queries. The subset system ensures data consistency by validating all records found through the recursive queries. If a record does not meet the subset condition, it will be excluded along with its parent records, preventing constraint violations.</p> <p>Warning</p> <p>Currently (v0.2b2), Greenmask can resolve multi-cylces in one strogly connected component, but only for one group  of vertexes. If you have SSC that contains 2 groups of vertexes, Greenmask will not be able to  resolve it. For instance we have 2 cycles with tables <code>A, B, C</code> (first group) and <code>B, C, E</code> (second group).  Greenmask will not be able to resolve it. But if you have only one group of vertexes one and more cycles in the  same group of tables (for instance <code>A, B, C</code>), Greenmask works with it. This will be fixed in the future. See second example below. In practice this is quite rare situation and 99% of people will not face this issue. </p> <p>You can read the Wikipedia article about Circular reference here.</p>"},{"location":"database_subset/#virtual-references","title":"Virtual references","text":"<p>During the development process, there are situations where foreign keys need to be removed. The reasons can vary\u2014from improving performance to simplifying the database structure. Additionally, some foreign keys may exist within loosely structured data, such as JSON, where PostgreSQL cannot create foreign keys at all. These limitations could significantly hinder the capabilities of a subset system. Greenmask offers a flexible solution to this problem by allowing the declaration of virtual references in the configuration, enabling the preservation and management of logical relationships between tables, even in the absence of explicit foreign keys. Virtual reference can be called virtual foreign key as well.</p> <p>The <code>virtual_references</code> can be defined in <code>dump</code> section. It contains the list of virtual references. First you set the table where you want to define virtual reference. In the attribute <code>references</code> define the list of tables that are referenced by the table. In the <code>columns</code> attribute define the list of columns that are used in the foreign key reference. The <code>not_null</code> attribute is optional and defines if the FK has not null constraint. If <code>true</code> Greenmask will generate <code>INNER JOIN</code> instead of <code>LEFT JOIN</code> by default it is <code>false</code>. The <code>expression</code> needs to be used when you want to use some expression to get the value of the column in the referencing table. For instance, if you have JSONB column in the <code>audit_logs</code> table that contains <code>order_id</code> field, you can use this field as FK reference.</p> <p>Info</p> <p>You do not need to define primry key of the referenced table. Greenmask will automatically resolve it and use it in the join condition.</p> Virtual references example<pre><code>dump:\n  virtual_references:\n    - schema: \"public\" # (1)\n      name: \"orders\" # (2)\n      references: # (3)\n        - schema: \"public\" # (4) \n          name: \"customers\" # (5)\n          columns: # (6)\n            - name: \"customer_id\"\n          not_null: false # (7)\n\n    - schema: \"public\"\n      name: \"audit_logs\"\n      references:\n        - schema: \"public\"\n          name: \"orders\"\n          columns:\n            - expression: \"(public.audit_logs.log_data -&gt;&gt; 'order_id')::INT\" # (8)\n</code></pre> <ol> <li>The schema name of table that has foreign key reference (table that own FK reference)</li> <li>The table name that has foreign key reference (table that own FK reference)</li> <li>List of virtual references</li> <li>The schema name of the table that has foreign key reference (referencing table)</li> <li>The table name that has foreign key reference (referencing table)</li> <li> <p>List of columns that are used in the foreign key reference. Each column has one of property defined at the same time:</p> <ul> <li><code>name</code> - column name in the referencing table</li> <li><code>expression</code> - expression that is used to get the value of the column in the referencing table</li> </ul> </li> <li> <p><code>not_null</code> - is FK has not null constraint. If <code>true</code> Default it is <code>false</code></p> </li> <li><code>expression</code> - expression that is used to get the value of the column in the referencing table</li> </ol>"},{"location":"database_subset/#polymorphic-references","title":"Polymorphic references","text":"<p>Greenmask supports polymorphic references. You can define a virtual reference for a table with polymorphic references using <code>polymorphic_exprs</code> attribute. The <code>polymorphic_exprs</code> attribute is a list of expressions that are used to make a polymorphic reference. For instance we might have a table <code>comments</code> that has polymorphic reference to <code>posts</code> and <code>videos</code>. The table comments might have <code>commentable_id</code> and <code>commentable_type</code> columns. The <code>commentable_type</code> column contains the type of the table that is referenced by the <code>commentable_id</code> column. The example of the config:</p> Polymorphic references example<pre><code>dump:\n  virtual_references:\n    - schema: \"public\"\n      name: \"comments\"\n      references:\n        - schema: \"public\"\n          name: \"videos\"\n          polymorphic_exprs:\n            - \"public.comments.commentable_type = 'video'\"\n          columns:\n            - name: \"commentable_id\"\n        - schema: \"public\"\n          name: \"posts\"\n          polymorphic_exprs:\n            - \"public.comments.commentable_type = 'post'\"\n          columns:\n            - name: \"commentable_id\"\n</code></pre> <p>Warning</p> <p>The plimorphic references cannot be non_null because the <code>commentable_id</code> column can be <code>NULL</code> if the  <code>commentable_type</code> is not set or different that the values defined in the <code>polymorphic_exprs</code> attribute.</p>"},{"location":"database_subset/#troubleshooting","title":"Troubleshooting","text":""},{"location":"database_subset/#exclude-the-records-that-has-null-values-in-the-referenced-column","title":"Exclude the records that has NULL values in the referenced column","text":"<p>If you want to exclude records that have NULL values in the referenced column, you can manually add this condition to the subset condition for the table. Greenmask does not automatically exclude records with NULL values because it applies a <code>LEFT OUTER JOIN</code> on nullable foreign keys.</p>"},{"location":"database_subset/#some-table-is-not-filtered-by-the-subset-condition","title":"Some table is not filtered by the subset condition","text":"<p>Greenmask builds a table dependency graph based on the introspected schema and existing foreign keys. If a table is not filtered by the subset condition, it means that the table either does not reference another table that is filtered by the subset condition or the table itself does not have a subset condition applied.</p> <p>If you have a table with a removed foreign key and want to filter it by the subset condition, you need to define a virtual reference. For more information on virtual references, refer to the Virtual References section.</p> <p>Info</p> <p>If you find any issues related to the code or greenmask is not working as expected, do not hesitate to contact us   directly or by creating an issue in the repository.</p>"},{"location":"database_subset/#error-column-reference-id-is-ambiguous","title":"ERROR: column reference \"id\" is ambiguous","text":"<p>If you see the error message <code>ERROR: column reference \"{column name}\" is ambiguous</code>, you have specified the column name without the table and/or schema name. To avoid ambiguity, always specify the schema and table name when pointing out the column to filter by. For instance if you want to filter employees by <code>employee_id</code> column, you should use <code>public.employees.employee_id</code> instead of <code>employee_id</code>.</p> Valid subset condition<pre><code>public.employees.employee_id IN (1, 2, 3)\n</code></pre>"},{"location":"database_subset/#the-subset-condition-is-not-working-correctly-how-can-i-verify-it","title":"The subset condition is not working correctly. How can I verify it?","text":"<p>Run greenmask with <code>--log-level=debug</code> to see the generated SQL queries. You will find the generated SQL queries in the log output. Validate this query in your database client to ensure that the subset condition is working as expected.</p> <p>For example:</p> <pre><code>$ greenmask dump --config config.yaml --log-level=debug\n\n2024-08-29T19:06:18+03:00 DBG internal/db/postgres/context/context.go:202 &gt; Debug query Schema=person Table=businessentitycontact pid=1638339\n2024-08-29T19:06:18+03:00 DBG internal/db/postgres/context/context.go:203 &gt; SELECT \"person\".\"businessentitycontact\".* FROM \"person\".\"businessentitycontact\"  INNER JOIN \"person\".\"businessentity\" ON \"person\".\"businessentitycontact\".\"businessentityid\" = \"person\".\"businessentity\".\"businessentityid\" AND ( person.businessentity.businessentityid between 400 and 800 OR person.businessentity.businessentityid between 800 and 900 ) INNER JOIN \"person\".\"person\" ON \"person\".\"businessentitycontact\".\"personid\" = \"person\".\"person\".\"businessentityid\" WHERE TRUE AND ((\"person\".\"person\".\"businessentityid\") IN (SELECT \"person\".\"businessentity\".\"businessentityid\" FROM \"person\".\"businessentity\"   WHERE ( ( person.businessentity.businessentityid between 400 and 800 OR person.businessentity.businessentityid between 800 and 900 ) )))\n pid=1638339\n</code></pre>"},{"location":"database_subset/#dump-is-too-slow","title":"Dump is too slow","text":"<p>If the dump process is too slow the generated query might be too complex. In this case you can:</p> <ul> <li>Check if the database has indexes on the columns used in the subset condition. Create them if possible.</li> <li>Move database dumping on the replica to avoid the performance impact on the primary.</li> </ul>"},{"location":"database_subset/#example-dump-a-subset-of-the-database","title":"Example: Dump a subset of the database","text":"<p>Info</p> <p>All examples based on playground database. Read more about the playground database in the  Playground section.</p> <p>The following example demonstrates how to dump a subset of the <code>person</code> schema. The subset condition is applied to the <code>businessentity</code> and <code>password</code> tables. The subset condition filters the data based on the <code>businessentityid</code> and <code>passwordsalt</code> columns, respectively.</p> Subset configuration example<pre><code>transformation:\n  - schema: \"person\"\n    name: \"businessentity\"\n    subset_conds:\n      - 'person.businessentity.businessentityid IN (274, 290, 721, 852)'\n    transformers:\n      - name: \"RandomDate\"\n        params:\n          column: \"modifieddate\"\n          min: \"2020-01-01 00:00:00\"\n          max: \"2024-06-26 00:00:00\"\n          truncate: \"day\"\n          keep_null: false\n\n  - schema: \"person\"\n    name: \"password\"\n    subset_conds:\n      - &gt;\n        person.password.passwordsalt = '329eacbe-c883-4f48-b8b6-17aa4627efff'\n</code></pre>"},{"location":"database_subset/#example-dump-a-subset-with-circular-reference","title":"Example: Dump a subset with circular reference","text":"Create tables with multi cyles<pre><code>-- Step 1: Create tables without foreign keys\nDROP TABLE IF EXISTS employees CASCADE;\nCREATE TABLE employees\n(\n    employee_id   SERIAL PRIMARY KEY,\n    name          VARCHAR(100) NOT NULL,\n    department_id INT -- Will reference departments(department_id)\n);\n\nDROP TABLE IF EXISTS departments CASCADE;\nCREATE TABLE departments\n(\n    department_id SERIAL PRIMARY KEY,\n    name          VARCHAR(100) NOT NULL,\n    project_id    INT -- Will reference projects(project_id)\n);\n\nDROP TABLE IF EXISTS projects CASCADE;\nCREATE TABLE projects\n(\n    project_id       SERIAL PRIMARY KEY,\n    name             VARCHAR(100) NOT NULL,\n    lead_employee_id INT, -- Will reference employees(employee_id)\n    head_employee_id INT  -- Will reference employees(employee_id)\n);\n\n-- Step 2: Alter tables to add foreign key constraints\nALTER TABLE employees\n    ADD CONSTRAINT fk_department\n        FOREIGN KEY (department_id) REFERENCES departments (department_id);\n\nALTER TABLE departments\n    ADD CONSTRAINT fk_project\n        FOREIGN KEY (project_id) REFERENCES projects (project_id);\n\nALTER TABLE projects\n    ADD CONSTRAINT fk_lead_employee\n        FOREIGN KEY (lead_employee_id) REFERENCES employees (employee_id);\n\nALTER TABLE projects\n    ADD CONSTRAINT fk_lead_employee2\n        FOREIGN KEY (head_employee_id) REFERENCES employees (employee_id);\n\n-- Insert projects\nINSERT INTO projects (name, lead_employee_id)\nSELECT 'Project ' || i, NULL\nFROM generate_series(1, 10) AS s(i);\n\n-- Insert departments\nINSERT INTO departments (name, project_id)\nSELECT 'Department ' || i, i\nFROM generate_series(1, 10) AS s(i);\n\n-- Insert employees and assign 10 of them as project leads\nINSERT INTO employees (name, department_id)\nSELECT 'Employee ' || i, (i / 10) + 1\nFROM generate_series(1, 99) AS s(i);\n\n-- Assign 10 employees as project leads\nUPDATE projects\nSET lead_employee_id = (SELECT employee_id\n                        FROM employees\n                        WHERE employees.department_id = projects.project_id\n                        LIMIT 1),\n    head_employee_id = 3\nWHERE project_id &lt;= 10;\n</code></pre> <p>This schema has two cycles:</p> <ul> <li><code>employees (department_id) -&gt; departments (project_id) -&gt; projects (lead_employee_id) -&gt; employees (employee_id)</code></li> <li><code>employees (department_id) -&gt; departments (project_id) -&gt; projects (head_employee_id) -&gt; employees (employee_id)</code></li> </ul> <p>Greenmask can simply resolve it by generating a recursive query with integrity checks for subset and join conditions.</p> <p>The example below will fetch the data for both 3 employees and related departments and projects.</p> Subset configuration example<pre><code>transformation:\n  - schema: \"public\"\n    name: \"employees\"\n    subset_conds:\n      - \"public.employees.employee_id in (1, 2, 3)\"\n</code></pre> <p>But this will return empty result, because the subset condition is not met for all related tables because project with <code>project_id=1</code> has reference to employee with <code>employee_id=3</code> that is invalid for subset condition.</p> Subset configuration example<pre><code>transformation:\n  - schema: \"public\"\n    name: \"employees\"\n    subset_conds:\n      - \"public.employees.employee_id in (1, 2)\"\n</code></pre>"},{"location":"database_subset/#example-dump-a-subset-with-virtual-references","title":"Example: Dump a subset with virtual references","text":"<p>In this example, we will create a subset of the tables with virtual references. The subset will include the <code>orders</code> table and its related tables <code>customers</code> and <code>audit_logs</code>. The <code>orders</code> table has a virtual reference to the <code>customers</code> table, and the <code>audit_logs</code> table has a virtual reference to the <code>orders</code> table.</p> Create tables with virtual references<pre><code>-- Create customers table\nCREATE TABLE customers\n(\n    customer_id   SERIAL PRIMARY KEY,\n    customer_name VARCHAR(100)\n);\n\n-- Create orders table\nCREATE TABLE orders\n(\n    order_id    SERIAL PRIMARY KEY,\n    customer_id INT, -- This should reference customers.customer_id, but no FK constraint is defined\n    order_date  DATE\n);\n\n-- Create payments table\nCREATE TABLE payments\n(\n    payment_id     SERIAL PRIMARY KEY,\n    order_id       INT, -- This should reference orders.order_id, but no FK constraint is defined\n    payment_amount DECIMAL(10, 2),\n    payment_date   DATE\n);\n\n-- Insert test data into customers table\nINSERT INTO customers (customer_name)\nVALUES ('John Doe'),\n       ('Jane Smith'),\n       ('Alice Johnson');\n\n-- Insert test data into orders table\nINSERT INTO orders (customer_id, order_date)\nVALUES (1, '2023-08-01'), -- Related to customer John Doe\n       (2, '2023-08-05'), -- Related to customer Jane Smith\n       (3, '2023-08-07');\n-- Related to customer Alice Johnson\n\n-- Insert test data into payments table\nINSERT INTO payments (order_id, payment_amount, payment_date)\nVALUES (1, 100.00, '2023-08-02'), -- Related to order 1 (John Doe's order)\n       (2, 200.50, '2023-08-06'), -- Related to order 2 (Jane Smith's order)\n       (3, 300.75, '2023-08-08');\n-- Related to order 3 (Alice Johnson's order)\n\n\n-- Create a table with a multi-key reference (composite key reference)\nCREATE TABLE order_items\n(\n    order_id     INT,               -- Should logically reference orders.order_id\n    item_id      INT,               -- Composite part of the key\n    product_name VARCHAR(100),\n    quantity     INT,\n    PRIMARY KEY (order_id, item_id) -- Composite primary key\n);\n\n-- Create a table with a JSONB column that contains a reference value\nCREATE TABLE audit_logs\n(\n    log_id   SERIAL PRIMARY KEY,\n    log_data JSONB -- This JSONB field will contain references to other tables\n);\n\n-- Insert data into order_items table with multi-key reference\nINSERT INTO order_items (order_id, item_id, product_name, quantity)\nVALUES (1, 1, 'Product A', 3), -- Related to order_id = 1 from orders table\n       (1, 2, 'Product B', 5), -- Related to order_id = 1 from orders table\n       (2, 1, 'Product C', 2), -- Related to order_id = 2 from orders table\n       (3, 1, 'Product D', 1);\n-- Related to order_id = 3 from orders table\n\n-- Insert data into audit_logs table with JSONB reference value\nINSERT INTO audit_logs (log_data)\nVALUES ('{\n  \"event\": \"order_created\",\n  \"order_id\": 1,\n  \"details\": {\n    \"customer_name\": \"John Doe\",\n    \"total\": 100.00\n  }\n}'),\n       ('{\n         \"event\": \"payment_received\",\n         \"order_id\": 2,\n         \"details\": {\n           \"payment_amount\": 200.50,\n           \"payment_date\": \"2023-08-06\"\n         }\n       }'),\n       ('{\n         \"event\": \"item_added\",\n         \"order_id\": 1,\n         \"item\": {\n           \"item_id\": 2,\n           \"product_name\": \"Product B\",\n           \"quantity\": 5\n         }\n       }');\n</code></pre> <p>The following example demonstrates how to make a subset for keys that does not have FK constraints but a data relationship exists.</p> <ul> <li>The <code>orders</code> table has a virtual reference to the <code>customers</code> table, and the <code>audit_logs</code> table   has a virtual reference to the <code>orders</code> table.</li> <li>The <code>payments</code> table has a virtual reference to the <code>orders</code> table.</li> <li>The <code>order_items</code> table has two keys that reference the <code>orders</code> and <code>products</code> tables.</li> <li>The <code>audit_logs</code> table has a JSONB column that contains two references to the <code>orders</code> and <code>order_items</code> tables.</li> </ul> <pre><code>dump:\n  virtual_references:\n    - schema: \"public\"\n      name: \"orders\"\n      references:\n        - schema: \"public\"\n          name: \"customers\"\n          columns:\n            - name: \"customer_id\"\n          not_null: true\n\n    - schema: \"public\"\n      name: \"payments\"\n      references:\n        - schema: \"public\"\n          name: \"orders\"\n          columns:\n            - name: \"order_id\"\n          not_null: true\n\n    - schema: \"public\"\n      name: \"order_items\"\n      references:\n        - schema: \"public\"\n          name: \"orders\"\n          columns:\n            - name: \"order_id\"\n          not_null: true\n        - schema: \"public\"\n          name: \"products\"\n          columns:\n            - name: \"product_id\"\n          not_null: true\n\n    - schema: \"public\"\n      name: \"audit_logs\"\n      references:\n        - schema: \"public\"\n          name: \"orders\"\n          columns:\n            - expression: \"(public.audit_logs.log_data -&gt;&gt; 'order_id')::INT\"\n          not_null: false\n        - schema: \"public\"\n          name: \"order_items\"\n          columns:\n            - expression: \"(public.audit_logs.log_data -&gt; 'item' -&gt;&gt; 'item_id')::INT\"\n            - expression: \"(public.audit_logs.log_data -&gt;&gt; 'order_id')::INT\"\n          not_null: false\n\n  transformation:\n\n    - schema: \"public\"\n      name: \"customers\"\n      subset_conds:\n        - \"public.customers.customer_id in (1)\"\n</code></pre> <p>As a result, the <code>customers</code> table will be dumped with the <code>orders</code> table and its related tables <code>payments</code>, <code>order_items</code>, and <code>audit_logs</code>. The subset condition will be applied to the <code>customers</code> table, and the data will be filtered based on the <code>customer_id</code> column.</p>"},{"location":"database_subset/#example-dump-a-subset-with-polymorphic-references","title":"Example: Dump a subset with polymorphic references","text":"<p>In this example, we will create a subset of the tables with polymorphic references. This example includes the <code>comments</code> table and its related tables <code>posts</code> and <code>videos</code>.</p> Create tables with polymorphic references and insert data<pre><code>-- Create the Posts table\nCREATE TABLE posts\n(\n    id      SERIAL PRIMARY KEY,\n    title   VARCHAR(255) NOT NULL,\n    content TEXT         NOT NULL\n);\n\n-- Create the Videos table\nCREATE TABLE videos\n(\n    id    SERIAL PRIMARY KEY,\n    title VARCHAR(255) NOT NULL,\n    url   VARCHAR(255) NOT NULL\n);\n\n-- Create the Comments table with a polymorphic reference\nCREATE TABLE comments\n(\n    id               SERIAL PRIMARY KEY,\n    commentable_id   INT         NOT NULL, -- Will refer to either posts.id or videos.id\n    commentable_type VARCHAR(50) NOT NULL, -- Will store the type of the associated record\n    body             TEXT        NOT NULL,\n    created_at       TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n\n-- Insert data into the Posts table\nINSERT INTO posts (title, content)\nVALUES ('First Post', 'This is the content of the first post.'),\n       ('Second Post', 'This is the content of the second post.');\n\n-- Insert data into the Videos table\nINSERT INTO videos (title, url)\nVALUES ('First Video', 'https://example.com/video1'),\n       ('Second Video', 'https://example.com/video2');\n\n-- Insert data into the Comments table, associating some comments with posts and others with videos\n-- For posts:\nINSERT INTO comments (commentable_id, commentable_type, body)\nVALUES (1, 'post', 'This is a comment on the first post.'),\n       (2, 'post', 'This is a comment on the second post.');\n\n-- For videos:\nINSERT INTO comments (commentable_id, commentable_type, body)\nVALUES (1, 'video', 'This is a comment on the first video.'),\n       (2, 'video', 'This is a comment on the second video.');\n</code></pre> <p>The <code>comments</code> table has a polymorphic reference to the <code>posts</code> and <code>videos</code> tables. Depending on the value of the <code>commentable_type</code> column, the <code>commentable_id</code> column will reference either the <code>posts.id</code> or <code>videos.id</code> column.</p> <p>The following example demonstrates how to make a subset for tables with polymorphic references.</p> Subset configuration example<pre><code>dump:\n  virtual_references:\n    - schema: \"public\"\n      name: \"comments\"\n      references:\n        - schema: \"public\"\n          name: \"posts\"\n          polymorphic_exprs:\n            - \"public.comments.commentable_type = 'post'\"\n          columns:\n            - name: \"commentable_id\"\n        - schema: \"public\"\n          name: \"videos\"\n          polymorphic_exprs:\n            - \"public.comments.commentable_type = 'video'\"\n          columns:\n            - name: \"commentable_id\"\n\n  transformation:\n    - schema: \"public\"\n      name: \"posts\"\n      subset_conds:\n        - \"public.posts.id in (1)\"\n</code></pre> <p>This example selects only the first post from the <code>posts</code> table and its related comments from the <code>comments</code> table.  The comments are associated with <code>videos</code> are included without filtering because the subset condition is applied only to the <code>posts</code> table and related comments. </p> <p>The resulted records will be:</p> <pre><code>transformed=# select * from comments;\n id | commentable_id | commentable_type |                 body                  |         created_at         \n----+----------------+------------------+---------------------------------------+----------------------------\n  1 |              1 | post             | This is a comment on the first post.  | 2024-09-18 05:27:54.217405\n  2 |              2 | post             | This is a comment on the second post. | 2024-09-18 05:27:54.217405\n  3 |              1 | video            | This is a comment on the first video. | 2024-09-18 05:27:54.229794\n(3 rows)\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Ensure that you have PostgreSQL utilities preinstalled, matching the major version   of your destination server.</p> </li> <li> <p>If you are building Greenmask from source, make sure you have the <code>make</code> utility installed.</p> </li> </ul>"},{"location":"installation/#via-docker","title":"Via docker","text":"<p>You can find the docker images in the:</p> <ol> <li>Docker-hub page</li> </ol> <p>To run the greenmask container from DockerHub, use the following command: <pre><code>docker run -it greenmask/greenmask:latest\n</code></pre></p> <ol> <li>GitHub container registry </li> </ol> <p>To run the greenmask container from Github registry, use the following command: <pre><code>docker run -it ghcr.io/greenmaskio/greenmask:latest\n</code></pre></p> <p>Info</p> <p>For pre-releases (rc, beta, etc.), use explicit tags like <code>v0.2.0b2</code>.</p>"},{"location":"installation/#via-brew","title":"Via brew","text":"<p>The greenmask build is available in brew,  but only a production build is available. To install the greenmask via brew, use the following command:</p> <pre><code>brew install greenmask\n</code></pre>"},{"location":"installation/#from-source","title":"From source","text":"<ol> <li> <p>Clone the Greenmask repository by using the following command:</p> <pre><code>git clone git@github.com:GreenmaskIO/greenmask.git\n</code></pre> </li> <li> <p>Once the repository is cloned, execute the following command to build Greenmask:</p> <pre><code>make build\n</code></pre> </li> </ol> <p>After completing the build process, you will find the binary named <code>greenmask</code> in the root directory of the repository. Execute the binary to start using Greenmask.</p>"},{"location":"installation/#playground","title":"Playground","text":"<p>Greenmask Playground is a sandbox environment for your experiments in Docker with sample databases included to help you try Greenmask without any additional actions. Read the Playground guide to learn more.</p>"},{"location":"playground/","title":"Greenmask Playground","text":"<p>Greenmask Playground is a sandbox environment in Docker with sample databases included to help you try Greenmask without any additional actions. It includes the following components:</p> <ul> <li>Original database \u2014 the source database you'll be working with.</li> <li>Empty database for restoration \u2014 an empty database where the restored data will be placed.</li> <li>MinIO storage \u2014 used for storage purposes.</li> <li>Greenmask Utility \u2014 Greenmask itself, ready for use.</li> </ul> <p>Warning</p> <p>To complete this guide, you must have Docker and docker-compose installed.</p>"},{"location":"playground/#setting-up-greenmask-playground","title":"Setting up Greenmask Playground","text":"<ol> <li> <p>Clone the <code>greenmask</code> repository and navigate to its directory by running the following commands:</p> <pre><code>git clone git@github.com:GreenmaskIO/greenmask.git &amp;&amp; cd greenmask\n</code></pre> </li> <li> <p>Once you have cloned the repository, start the environment by running Docker Compose:</p> <pre><code>docker-compose run greenmask\n</code></pre> </li> </ol> <p>Tip</p> <p>If you're experiencing problems with pulling images from Docker Hub, you can build the Greenmask image from source by running the following command:</p> <pre><code>docker-compose run greenmask-from-source\n</code></pre> <p>Now you have Greenmask Playground up and running with a shell prompt inside the container. All further operations will be carried out within this container's shell.</p>"},{"location":"playground/#commands","title":"Commands","text":"<p>Below you can see Greenmask commands:</p> <ul> <li> <p><code>dump</code> \u2014 performs a logical data dump, transforms the data, and stores it in the designated storage.</p> </li> <li> <p><code>list-dumps</code> \u2014 retrieves a list of all stored dumps within the chosen storage.</p> </li> <li> <p><code>delete</code> \u2014 removes a dump with a specific ID from the storage.</p> </li> <li> <p><code>list-transformers</code> \u2014 displays a list of approved transformers and their documentation.</p> </li> <li> <p><code>restore</code> \u2014 restores a dump either by specifying its ID or using the latest available dump to the target database.</p> </li> <li> <p><code>show-dump</code> \u2014 presents metadata information about a specific dump (equivalent to <code>pg_restore -l ./</code>).</p> </li> <li> <p><code>validate</code> \u2014 executes a validation process and generates a data diff for the transformation.</p> </li> <li> <p><code>completion</code> \u2014 generates the autocompletion script for the specified shell.</p> </li> </ul> <p>To learn more about them, see Commands.</p>"},{"location":"playground/#transformers","title":"Transformers","text":"<p>A configuration file is mandatory for Greenmask functioning. The pre-defined configuration file is stored at the repository root directory (<code>./playground/config.yml</code>). It also serves to define transformers which you can update to your liking in order to use Greenmask Playground more effectively and to get better understanding of the tool itself. To learn how to customize a configuration file, see Configuration</p> <p>The pre-defined configuration file uses the NoiseDate transformer as an example. To learn more about other transformers and how to use them, see Transformers.</p>"},{"location":"built_in_transformers/","title":"About transformers","text":"<p>Transformers in Greenmask are methods which are applied to anonymize sensitive data. All Greenmask transformers are split into the following groups:</p> <ul> <li>Dynamic parameters \u2014 transformers that require an input of parameters and generate   random data based on them.</li> <li>Transformation engines \u2014 the type of generator used in transformers. Hash (deterministic)   and random (randomization)</li> <li>Parameters templating \u2014 generate static parameters values from templates.</li> <li>Transformation conditions \u2014 conditions that can be applied to transformers. If the   condition is not met, the transformer will not be applied.</li> <li>Transformation Inheritance \u2014 transformation inheritance for partitioned tables and   tables with foreign keys. Define once and apply to all.</li> <li>Standard transformers \u2014 transformers that require only an input of parameters.</li> <li>Advanced transformers \u2014 transformers that can be modified according to user's needs   with the help of custom functions.</li> <li>Custom transformers \u2014 coming soon...</li> </ul>"},{"location":"built_in_transformers/dynamic_parameters/","title":"Dynamic parameters","text":""},{"location":"built_in_transformers/dynamic_parameters/#description","title":"Description","text":"<p>Most transformers in Greenmask have dynamic parameters. This functionality is possible because Greenmask utilizes a database driver that can encode and decode raw values into their actual type representations.</p> <p>This allows you to retrieve parameter values directly from the records. This capability is particularly beneficial when you need to resolve functional dependencies between fields or satisfy constraints. Greenmask processes transformations sequentially. Therefore, when you reference a field that was transformed in a previous step, you will access the transformed value.</p>"},{"location":"built_in_transformers/dynamic_parameters/#definition","title":"Definition","text":"<pre><code>dynamic_params:\n  - column: \"column_name\" # (1)\n    cast_to: \"cast_function\" # (2)\n    template: \"template_function\" # (3)\n    default_value: any # (4)\n</code></pre> <ol> <li>Name of the column from which the value is retrieved.</li> <li>Function used to cast the column value to the desired type.</li> <li>Default value used if the column's value is <code>NULL</code>.</li> <li>Template used for casting the column value to the desired type.</li> </ol>"},{"location":"built_in_transformers/dynamic_parameters/#dynamic-parameter-options","title":"Dynamic parameter options","text":"<ul> <li> <p><code>column</code> - Specifies the column name. The value from each record in this column will be passed to the transformer as a   parameter.</p> </li> <li> <p><code>cast_to</code> - Indicates the function used to cast the column value to the desired type. Before being passed to the   transformer, the value is cast to this type. For more details, see  Cast functions.</p> </li> <li> <p><code>template</code> - Defines the template used for casting the column value to the desired type. You can create your own   template and incorporate predefined functions and operators to implement the casting logic or other logic required for   passing the value to the transformer. For more details,   see Template functions.</p> </li> <li> <p><code>default_value</code> - Determines the default value used if the column's value is <code>NULL</code>. This value is represented in raw   format appropriate to the type specified in the <code>column</code> parameter.</p> </li> </ul>"},{"location":"built_in_transformers/dynamic_parameters/#cast-functions","title":"Cast functions","text":"name description input type output type UnixNanoToDate Cast int value as Unix Timestamp in Nano Seconds to date type int2, int4, int8, numeric, float4, float8 date UnixMicroToDate Cast int value as Unix Timestamp in Micro Seconds to date type int2, int4, int8, numeric, float4, float8 date UnixMilliToDate Cast int value as Unix Timestamp in Milli Seconds to date type int2, int4, int8, numeric, float4, float8 date UnixSecToDate Cast int value as Unix Timestamp in Seconds to date type int2, int4, int8, numeric, float4, float8 date UnixNanoToTimestamp Cast int value as Unix Timestamp in Nano Seconds to timestamp type int2, int4, int8, numeric, float4, float8 timestamp UnixMicroToTimestamp Cast int value as Unix Timestamp in Micro Seconds to timestamp type int2, int4, int8, numeric, float4, float8 timestamp UnixMilliToTimestamp Cast int value as Unix Timestamp in Milli Seconds to timestamp type int2, int4, int8, numeric, float4, float8 timestamp UnixSecToTimestamp Cast int value as Unix Timestamp in Seconds to timestamp type int2, int4, int8, numeric, float4, float8 timestamp UnixNanoToTimestampTz Cast int value as Unix Timestamp in Nano Seconds to timestamptz type int2, int4, int8, numeric, float4, float8 timestamptz UnixMicroToTimestampTz Cast int value as Unix Timestamp in Micro Seconds to timestamptz type int2, int4, int8, numeric, float4, float8 timestamptz UnixMilliToTimestampTz Cast int value as Unix Timestamp in Milli Seconds to timestamptz type int2, int4, int8, numeric, float4, float8 timestamptz UnixSecToTimestampTz Cast int value as Unix Timestamp in Seconds to timestamptz type int2, int4, int8, numeric, float4, float8 timestamptz DateToUnixNano Cast date value to int value as a Unix Timestamp in Nano Seconds date int2, int4, int8, numeric, float4, float8 DateToUnixMicro Cast date value to int value as a Unix Timestamp in Micro Seconds date int2, int4, int8, numeric, float4, float8 DateToUnixMilli Cast date value to int value as a Unix Timestamp in Milli Seconds date int2, int4, int8, numeric, float4, float8 DateToUnixSec Cast date value to int value as a Unix Timestamp in Seconds date int2, int4, int8, numeric, float4, float8 TimestampToUnixNano Cast timestamp value to int value as a Unix Timestamp in Nano Seconds timestamp int2, int4, int8, numeric, float4, float8 TimestampToUnixMicro Cast timestamp value to int value as a Unix Timestamp in Micro Seconds timestamp int2, int4, int8, numeric, float4, float8 TimestampToUnixMilli Cast timestamp value to int value as a Unix Timestamp in Milli Seconds timestamp int2, int4, int8, numeric, float4, float8 TimestampToUnixSec Cast timestamp value to int value as a Unix Timestamp in Seconds timestamp int2, int4, int8, numeric, float4, float8 TimestampTzToUnixNano Cast timestamptz value to int value as a Unix Timestamp in Nano Seconds timestamptz int2, int4, int8, numeric, float4, float8 TimestampTzToUnixMicro Cast timestamptz value to int value as a Unix Timestamp in Micro Seconds timestamptz int2, int4, int8, numeric, float4, float8 TimestampTzToUnixMilli Cast timestamptz value to int value as a Unix Timestamp in Milli Seconds timestamptz int2, int4, int8, numeric, float4, float8 TimestampTzToUnixSec Cast timestamptz value to int value as a Unix Timestamp in Seconds timestamptz int2, int4, int8, numeric, float4, float8 FloatToInt Cast float value to one of integer type. The fractional part will be discarded numeric, float4, float8 int2, int4, int8, numeric IntToFloat Cast int value to one of integer type int2, int4, int8, numeric numeric, float4, float8 IntToBool Cast int value to boolean. The value with 0 is false, 1 is true int2, int4, int8, numeric, float4, float8 bool BoolToInt Cast boolean value to int. The value false is 0, true is 1 bool int2, int4, int8, numeric, float4, float8"},{"location":"built_in_transformers/dynamic_parameters/#example-functional-dependency-resolution-between-columns","title":"Example: Functional dependency resolution between columns","text":"<p>There is simplified schema of the table <code>humanresources.employee</code> from the playground:</p> <pre><code>       Column      |            Type                      \n------------------+-----------------------------\n businessentityid | integer                      \n jobtitle         | character varying(50)        \n birthdate        | date                        \n hiredate         | date                         \nCheck constraints:\n    CHECK (birthdate &gt;= '1930-01-01'::date AND birthdate &lt;= (now() - '18 years'::interval))\n</code></pre> <p>As you can see, there is a functional dependency between the <code>birthdate</code> and <code>hiredate</code> columns. Logically, the <code>hiredate</code> should be later than the <code>birthdate</code>. Additionally, the <code>birthdate</code> should range from <code>1930-01-01</code> to <code>18</code> years prior to the current date.</p> <p>Imagine that you need to generate random <code>birthdate</code> and <code>hiredate</code> columns. To ensure these dates satisfy the constraints, you can use dynamic parameters in the <code>RandomDate</code> transformer:</p> <pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n\n    - name: \"RandomDate\" # (1)\n      params:\n        column: \"birthdate\"\n        min: '{{ now | tsModify \"-30 years\" | .EncodeValue }}' # (2)\n        max: '{{ now | tsModify \"-18 years\" | .EncodeValue }}' # (3)\n\n    - name: \"RandomDate\" # (4)\n      params:\n        column: \"hiredate\"\n        max: \"{{ now | .EncodeValue }}\" # (5)\n      dynamic_params:\n        min:\n          column: \"birthdate\" # (6)\n          template: '{{ .GetValue | tsModify \"18 years\" | .EncodeValue }}' # (7)\n</code></pre> <ol> <li>Firstly we generate the <code>RadnomDate</code> for birthdate column. The result of the transformation will used as the minimum    value for the next transformation for <code>hiredate</code> column.</li> <li>Apply the template for static parameter. It calculates the now date and subtracts <code>30</code> years from it. The result    is <code>1994</code>. The function tsModify return not a raw data, but time.Time object. For getting the raw value suitable for    birthdate type we need to pass this value to <code>.EncodeValue</code> function. This value is used as the minimum value for    the <code>birthdate</code> column.</li> <li>The same as the previous step, but we subtract <code>18</code> years from the now date. The result is <code>2002</code>.</li> <li>Generate the <code>RadnomDate</code> for <code>hiredate</code> column based on the value from the <code>birthdate</code>.</li> <li>Set the maximum value for the <code>hiredate</code> column. The value is the current date.</li> <li>The <code>min</code> parameter is set to the value of the <code>birthdate</code> column from the previous step. </li> <li>The template gets the value of the randomly generated <code>birthdate</code> value and adds <code>18</code> years to it.</li> </ol> <p>Below is the result of the transformation:</p> <p></p> <p>From the result, you can see that all functional dependencies and constraints are satisfied.</p>"},{"location":"built_in_transformers/parameters_templating/","title":"Parameters templating","text":""},{"location":"built_in_transformers/parameters_templating/#description","title":"Description","text":"<p>It is allowed to generate parameter values from templates. It is useful when you don't want to write values manually, but instead want to generate and initialize them dynamically.</p> <p>Here you can find the list of template functions that can be used in the template Custom functions.</p> <p>You can encode and decode objects using the driver function bellow.</p>"},{"location":"built_in_transformers/parameters_templating/#template-functions","title":"Template functions","text":"Function Description Signature <code>.GetColumnType</code> Returns a string with the column type. <code>.GetColumnType(name string) (typeName string, err error)</code> <code>.EncodeValueByColumn</code> Encodes a value of any type into its raw string representation using the specified column name. Encoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.EncodeValueByColumn(name string, value any) (res any, err error)</code> <code>.DecodeValueByColumn</code> Decodes a value from its raw string representation to a Golang type using the specified column name. Decoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.DecodeValueByColumn(name string, value any) (res any, err error)</code> <code>.EncodeValueByType</code> Encodes a value of any type into its string representation using the specified type name. Encoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.EncodeValueByType(name string, value any) (res any, err error)</code> <code>.DecodeValueByType</code> Decodes a value from its raw string representation to a Golang type using the specified type name. Decoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.DecodeValueByType(name string, value any) (res any, err error)</code> <code>.DecodeValue</code> Decodes a value from its raw string representation to a Golang type using the data type assigned to the table column specified in the <code>column</code> parameter. Decoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.DecodeValueByColumn(value any) (res any, err error)</code> <code>.EncodeValue</code> Encodes a value of any type into its string representation using the type assigned to the table column specified in the <code>column</code> parameter. Encoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.EncodeValue(value any) (res any, err error)</code> <p>Warning</p> <p>If column parameter is not linked to column parameter, then functions <code>.DecodeValue</code> and <code>.EncodeValue</code> will return  an error. You can use <code>.DecodeValueByType</code> and <code>.EncodeValueByType</code> or <code>.DecodeValueByColumn</code> and  <code>.EncodeValueByColumn</code> instead.</p>"},{"location":"built_in_transformers/parameters_templating/#example","title":"Example","text":"<p>In the example below, the min and max values for the <code>birth_date</code> column are generated dynamically using the <code>now</code> template function. The value returns the current date and time. The <code>tsModify</code> function is then used to subtract 30 (and 18) years. But because the parameter type is mapped on <code>column</code> parameter type, the <code>EncodeValue</code> function is used to encode the value into the column type.</p> <p>For example, if we have the now date as <code>2021-01-01</code>, the dynamically calculated <code>min</code> value will be <code>1994-01-01</code> and the <code>max</code> value will be <code>2006-01-01</code>.</p> <pre><code>CREATE TABLE account\n(\n    id         SERIAL PRIMARY KEY,\n    gender     VARCHAR(1) NOT NULL,\n    email      TEXT       NOT NULL NOT NULL UNIQUE,\n    first_name TEXT       NOT NULL,\n    last_name  TEXT       NOT NULL,\n    birth_date DATE,\n    created_at TIMESTAMP  NOT NULL DEFAULT NOW()\n);\n\nINSERT INTO account (first_name, gender, last_name, birth_date, email)\nVALUES ('John', 'M', 'Smith', '1980-01-01', 'john.smith@gmail.com');\n</code></pre> <pre><code>- schema: \"public\"\n  name: \"account\"\n  transformers:\n    - name: \"RandomDate\"\n      params:\n        column: \"birth_date\"\n        min: '{{ now | tsModify \"-30 years\" | .EncodeValue }}' # 1994\n        max: '{{ now | tsModify \"-18 years\" | .EncodeValue }}' # 2006\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue birth_date1980-01-011995-09-06"},{"location":"built_in_transformers/transformation_condition/","title":"Transformation Condition","text":""},{"location":"built_in_transformers/transformation_condition/#description","title":"Description","text":"<p>The transformation condition feature allows you to execute a defined transformation only if a specified condition is met. The condition must be defined as a boolean expression that evaluates to <code>true</code> or <code>false</code>. Greenmask uses expr-lang/expr under the hood. You can use all functions and syntax provided by the <code>expr</code> library.</p> <p>You can use the same functions that are described in the built-in transformers</p> <p>The transformers are executed one by one - this helps you create complex transformation pipelines. For instance depending on value chosen in the previous transformer, you can decide to execute the next transformer or not.</p>"},{"location":"built_in_transformers/transformation_condition/#record-descriptors","title":"Record descriptors","text":"<p>To improve the user experience, Greenmask offers special namespaces for accessing values in different formats: either the driver-encoded value in its real type or as a raw string.</p> <ul> <li><code>record</code>: This namespace provides the record value in its actual type.</li> <li><code>raw_record</code>: This namespace provides the record value as a string.</li> </ul> <p>You can access a specific column\u2019s value using <code>record.column_name</code> for the real type or <code>raw_record.column_name</code> for the raw string value.</p> <p>Warning</p> <p>A record may always be modified by previous transformers before the condition is evaluated. This means Greenmask does not retain the original record value and instead provides the current modified value for condition evaluation.</p>"},{"location":"built_in_transformers/transformation_condition/#null-values-condition","title":"Null values condition","text":"<p>To check if the value is null, you can use <code>null</code> value for the comparisson. This operation works compatibly with SQL operator <code>IS NULL</code> or <code>IS NOT NULL</code>.</p> Is null cond example<pre><code>record.accountnumber == null &amp;&amp; record.date &gt; now()\n</code></pre> Is not null cond example<pre><code>record.accountnumber != null &amp;&amp; record.date &lt;= now()\n</code></pre>"},{"location":"built_in_transformers/transformation_condition/#expression-scope","title":"Expression scope","text":"<p>Expression scope can be on table or specific transformer. If you define the condition on the table scope, then the condition will be evaluated before any transformer is executed. If you define the condition on the transformer scope, then the condition will be evaluated before the specified transformer is executed.</p> Table scope<pre><code>- schema: \"purchasing\"\n  name: \"vendor\"\n  when: 'record.accountnumber == null || record.accountnumber == \"ALLENSON0001\"'\n  transformers:\n    - name: \"RandomString\"\n      params:\n        column: \"accountnumber\"\n        min_length: 9\n        max_length: 12\n        symbols: \"1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n</code></pre> Transformer scope<pre><code>- schema: \"purchasing\"\n  name: \"vendor\"\n  transformers:\n    - name: \"RandomString\"\n      when: 'record.accountnumber != null || record.accountnumber == \"ALLENSON0001\"'\n      params:\n        column: \"accountnumber\"\n        min_length: 9\n        max_length: 12\n        symbols: \"1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n</code></pre>"},{"location":"built_in_transformers/transformation_condition/#int-and-float-value-definition","title":"Int and float value definition","text":"<p>It is important to create the integer or float value in the correct format. If you want to define the integer value you must write a number without dot (<code>1</code>, <code>2</code>, etc.). If you want to define the float value you must write a number with dot (<code>1.0</code>, <code>2.0</code>, etc.).</p> <p>Warning</p> <p>You may see a wrong comparison result if you compare int and float, for example <code>1 == 1.0</code> will return <code>false</code>. </p>"},{"location":"built_in_transformers/transformation_condition/#architecture","title":"Architecture","text":"<p>Greenmask encodes the way only when evaluating the condition - this allows to optimize the performance of the transformation if you have a lot of conditions that uses or (<code>||</code>) or and (<code>&amp;&amp;</code>) operators.</p>"},{"location":"built_in_transformers/transformation_condition/#example-chose-random-value-and-execute-one-of","title":"Example: Chose random value and execute one of","text":"<p>In the following example, the <code>RandomChoice</code> transformer is used to choose a random value from the list of values. Depending on the chosen value, the <code>Replace</code> transformer is executed to set the <code>activeflag</code> column to <code>true</code> or <code>false</code>.</p> <p>In this case the condition scope is on the transformer level.</p> <pre><code>- schema: \"purchasing\"\n  name: \"vendor\"\n  transformers:\n    - name: \"RandomChoice\"\n      params:\n        column: \"name\"\n        values:\n          - \"test1\"\n          - \"test2\"\n\n    - name: \"Replace\"\n      when: 'record.name == \"test1\"'\n      params:\n        column: \"activeflag\"\n        value: \"false\"\n\n    - name: \"Replace\"\n      when: 'record.name == \"test2\"'\n      params:\n        column: \"activeflag\"\n        value: \"true\"\n</code></pre>"},{"location":"built_in_transformers/transformation_condition/#example-do-not-transform-specific-columns","title":"Example: Do not transform specific columns","text":"<p>In the following example, the <code>RandomString</code> transformer is executed only if the <code>businessentityid</code> column value is not equal to <code>1492</code> or <code>1</code>.</p> <pre><code>  - schema: \"purchasing\"\n    name: \"vendor\"\n    when: '!(record.businessentityid | has([1492, 1]))'\n    transformers:\n      - name: \"RandomString\"\n        params:\n          column: \"accountnumber\"\n          min_length: 9\n          max_length: 12\n          symbols: \"1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n</code></pre>"},{"location":"built_in_transformers/transformation_condition/#example-check-the-json-attribute-value","title":"Example: Check the json attribute value","text":"<p>In the following example, the <code>RandomString</code> transformer is executed only if the <code>a</code> attribute in the <code>json_data</code> column is equal to <code>1</code>.</p> <pre><code>- schema: \"public\"\n  name: \"jsondata\"\n  when: 'raw_record.json_data | jsonGet(\"a\") == 1'\n  transformers:\n    - name: \"RandomString\"\n      params:\n        column: \"accountnumber\"\n        min_length: 9\n        max_length: 12\n        symbols: \"1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n</code></pre>"},{"location":"built_in_transformers/transformation_engines/","title":"Transformation engine","text":"<p>The greenmask provides two engines <code>random</code> and <code>hash</code>. Most of the transformers has <code>engine</code> parameters that by default is set to <code>random</code>. Use <code>hash</code> engine when you need to generate deterministic data - the same input will always produce the same output.</p> <p>Info</p> <p>Greenmask employs the <code>SHA-3</code> algorithm to hash input values. While this function is cryptographically secure, it does exhibit lower performance. We plan to introduce additional hash functions in the future to offer a balance between security and performance. For example, <code>SipHash</code>, which provides a good trade-off between security and performance, is currently in development and is expected to be included in the stable <code>v0.2</code> release of Greenmask.</p> <p>Warning</p> <p>The hash engine does not guarantee the uniqueness of generated values. Although transformers such as <code>Hash</code>,  <code>RandomEmail</code>, and <code>RandomUuid</code> typically have a low probability of producing duplicate values The feature to  ensure uniqueness is currently under development at Greenmask and is expected to be released in future updates.  For the latest status, please visit the Greenmask roadmap.</p>"},{"location":"built_in_transformers/transformation_engines/#details","title":"Details","text":""},{"location":"built_in_transformers/transformation_engines/#example-schema","title":"Example schema","text":"<p>The next examples will be run on the following schema and sample data:</p> <pre><code>CREATE TABLE account\n(\n    id         SERIAL PRIMARY KEY,\n    gender     VARCHAR(1) NOT NULL,\n    email      TEXT       NOT NULL NOT NULL UNIQUE,\n    first_name TEXT       NOT NULL,\n    last_name  TEXT       NOT NULL,\n    birth_date DATE,\n    created_at TIMESTAMP  NOT NULL DEFAULT NOW()\n);\n\nINSERT INTO account (first_name, gender, last_name, birth_date, email)\nVALUES ('John', 'M', 'Smith', '1980-01-01', 'john.smith@gmail.com');\n\nCREATE TABLE orders\n(\n    id          SERIAL PRIMARY KEY,\n    account_id  INTEGER REFERENCES account (id),\n    total_price NUMERIC(10, 2),\n    created_at  TIMESTAMP NOT NULL DEFAULT NOW(),\n    paid_at     TIMESTAMP\n);\n\nINSERT INTO orders (account_id, total_price, created_at, paid_at)\nVALUES (1, 100.50, '2024-05-01', '2024-05-02'),\n       (1, 200.75, '2024-05-03', NULL);\n</code></pre>"},{"location":"built_in_transformers/transformation_engines/#random-engine","title":"Random engine","text":"<p>The random engine serves as the default engine for the greenmask. It operates using a pseudo-random number generator, which is initialized with a random seed sourced from a cryptographically secure random number generator. Employ the random engine when you need to generate random data and do not require reproducibility of the same transformation results with the same input.</p> <p>The following example demonstrates how to configure the <code>RandomDate</code> transformer to generate random.</p> <pre><code>- schema: \"public\"\n  name: \"account\"\n  transformers:\n    - name: \"RandomDate\"\n      params:\n        column: \"birth_date\"\n        engine: \"random\" # (1)\n        min: '1970-01-01'\n        max: '2000-01-01'\n</code></pre> <ol> <li><code>random</code> engine is explicitly specified, although it is the default value.</li> </ol> <p>Results:</p> ColumnOriginalValueTransformedValue birth_date1980-01-011970-02-23 <p>Keep in mind that the <code>random</code> engine is always generates different values for the same input. For instance in we run the previous example multiple times we will get different results.</p>"},{"location":"built_in_transformers/transformation_engines/#hash-engine","title":"Hash engine","text":"<p>The hash engine is designed to generate deterministic data. It uses the <code>SHA-3</code> algorithm to hash the input value. The hash engine is particularly useful when you need to generate the same output for the same input. For example, when you want to transform values that are used as primary or foreign keys in a database.</p> <p>For secure reason it is suggested set global greenmask salt via <code>GREENMASK_GLOBAL_SALT</code> environment variable. The salt is added to the hash input to prevent the possibility of reverse engineering the original value from the hashed output. The value is hex encoded with variadic length. For example, <code>GREENMASK_GLOBAL_SALT=a5eddc84e762e810</code>. Generate a strong random salt and keep it secret.</p> <p>The following example demonstrates how to configure the <code>RandomInt</code> transformer to generate deterministic data using the <code>hash</code> engine. The <code>public.account.id</code> and <code>public.orders.account_id</code> columns will have the same values.</p> <pre><code>- schema: \"public\"\n  name: \"account\"\n  transformers:\n\n    - name: \"RandomInt\"\n      params:\n        column: \"id\"\n        engine: hash\n        min: 1\n        max: 2147483647\n\n- schema: \"public\"\n  name: \"orders\"\n  transformers:\n\n    - name: \"RandomInt\"\n      params:\n        column: \"account_id\"\n        engine: hash\n        min: 1\n        max: 2147483647\n</code></pre> <p>Result:</p> <ul> <li>public.account</li> </ul> ColumnOriginalValueTransformedValue id1130162079 <ul> <li>public.orders</li> </ul> ColumnOriginalValueTransformedValue account_id1130162079"},{"location":"built_in_transformers/transformation_inheritance/","title":"Transformation Inheritance","text":""},{"location":"built_in_transformers/transformation_inheritance/#description","title":"Description","text":"<p>If you have partitioned tables or want to apply a transformation to a primary key and propagate it to all tables referencing that column, you can do so with Greenmask.</p>"},{"location":"built_in_transformers/transformation_inheritance/#apply-for-inherited","title":"Apply for inherited","text":"<p>Using <code>apply_for_inherited</code>, you can apply transformations to all partitions of a partitioned table, including any subpartitions.</p>"},{"location":"built_in_transformers/transformation_inheritance/#configuration-conflicts","title":"Configuration conflicts","text":"<p>When a partition has a transformation defined manually via config, and <code>apply_for_inherited</code> is set on the parent table, Greenmask will merge both the inherited and manually defined configurations. The manually defined transformation will execute last, giving it higher priority.</p> <p>If this situation occurs, you will see the following information in the log:</p> <pre><code>{\n  \"level\": \"info\",\n  \"ParentTableSchema\": \"public\",\n  \"ParentTableName\": \"sales\",\n  \"ChildTableSchema\": \"public\",\n  \"ChildTableName\": \"sales_2022_feb\",\n  \"ChildTableConfig\": [\n    {\n      \"name\": \"RandomDate\",\n      \"params\": {\n        \"column\": \"sale_date\",\n        \"engine\": \"random\",\n        \"max\": \"2005-01-01\",\n        \"min\": \"2001-01-01\"\n      }\n    }\n  ],\n  \"time\": \"2024-11-03T22:14:01+02:00\",\n  \"message\": \"config will be merged: found manually defined transformers on the partitioned table\"\n}\n</code></pre>"},{"location":"built_in_transformers/transformation_inheritance/#apply-for-references","title":"Apply for references","text":"<p>Using <code>apply_for_references</code>, you can apply transformations to columns involved in a primary key or in tables with a foreign key that references that column. This simplifies the transformation process by requiring you to define the transformation only on the primary key column, which will then be applied to all tables referencing that column.</p> <p>The transformer must be deterministic or support <code>hash</code> engine and the <code>hash</code> engin must be set in the configuration file.</p> <p>List of transformers that supports <code>apply_for_references</code>:</p> <ul> <li>Hash</li> <li>NoiseDate</li> <li>NoiseFloat</li> <li>NoiseInt</li> <li>NoiseNumeric</li> <li>RandomBool</li> <li>RandomDate</li> <li>RandomEmail</li> <li>RandomFloat</li> <li>RandomInt</li> <li>RandomIp</li> <li>RandomMac</li> <li>RandomNumeric</li> <li>RandomString</li> <li>RandomUuid</li> <li>RandomUnixTimestamp</li> </ul>"},{"location":"built_in_transformers/transformation_inheritance/#end-to-end-identifiers","title":"End-to-End Identifiers","text":"<p>End-to-end identifiers in databases are unique identifiers that are consistently used across multiple tables in a relational database schema, allowing for a seamless chain of references from one table to another. These identifiers typically serve as primary keys in one table and are propagated as foreign keys in other tables, creating a direct, traceable link from one end of a data relationship to the other.</p> <p>Greenmask can detect end-to-end identifiers and apply transformations across the entire sequence of tables. These identifiers are detected when the following condition is met: the foreign key serves as both a primary key and a foreign key in the referenced table.</p>"},{"location":"built_in_transformers/transformation_inheritance/#configuration-conflicts_1","title":"Configuration conflicts","text":"<p>When on the referenced column a transformation is manually defined via config, and the <code>apply_for_references</code> is set on parent table, the transformation defined will be chosen and the inherited transformation will be ignored. You will receive a <code>INFO</code> message in the logs.</p> <pre><code>{\n  \"level\": \"info\",\n  \"TransformerName\": \"RandomInt\",\n  \"ParentTableSchema\": \"public\",\n  \"ParentTableName\": \"tablea\",\n  \"ChildTableSchema\": \"public\",\n  \"ChildTableName\": \"tablec\",\n  \"ChildColumnName\": \"id2\",\n  \"TransformerConfig\": {\n    \"name\": \"RandomInt\",\n    \"apply_for_references\": true\n  },\n  \"time\": \"2024-11-03T21:28:10+02:00\",\n  \"message\": \"skipping apply transformer for reference: found manually configured transformer\"\n}\n</code></pre>"},{"location":"built_in_transformers/transformation_inheritance/#limitations","title":"Limitations","text":"<ul> <li>The transformation must be deterministic.</li> <li>The transformation condition will not be applied to the referenced column.</li> <li>Not all transformers support <code>apply_for_references</code></li> </ul> <p>Warning</p> <p>We do not recommend using <code>apply_for_references</code> with transformation conditions, as these conditions are not  inherited by transformers on the referenced columns. This may lead to inconsistencies in the data.</p>"},{"location":"built_in_transformers/transformation_inheritance/#example-1-partitioned-tables","title":"Example 1. Partitioned tables","text":"<p>In this example, we have a partitioned table <code>sales</code> that is partitioned by year and then by month. Each partition contains a subset of data based on the year and month of the sale. The <code>sales</code> table has a primary key <code>sale_id</code> and is partitioned by <code>sale_date</code>. The <code>sale_date</code> column is transformed using the <code>RandomDate</code> transformer.</p> <pre><code>CREATE TABLE sales\n(\n    sale_id   SERIAL         NOT NULL,\n    sale_date DATE           NOT NULL,\n    amount    NUMERIC(10, 2) NOT NULL\n) PARTITION BY RANGE (EXTRACT(YEAR FROM sale_date));\n\n-- Step 2: Create first-level partitions by year\nCREATE TABLE sales_2022 PARTITION OF sales\n    FOR VALUES FROM (2022) TO (2023)\n    PARTITION BY LIST (EXTRACT(MONTH FROM sale_date));\n\nCREATE TABLE sales_2023 PARTITION OF sales\n    FOR VALUES FROM (2023) TO (2024)\n    PARTITION BY LIST (EXTRACT(MONTH FROM sale_date));\n\n-- Step 3: Create second-level partitions by month for each year, adding PRIMARY KEY on each partition\n\n-- Monthly partitions for 2022\nCREATE TABLE sales_2022_jan PARTITION OF sales_2022 FOR VALUES IN (1)\n    WITH (fillfactor = 70);\nCREATE TABLE sales_2022_feb PARTITION OF sales_2022 FOR VALUES IN (2);\nCREATE TABLE sales_2022_mar PARTITION OF sales_2022 FOR VALUES IN (3);\n-- Continue adding monthly partitions for 2022...\n\n-- Monthly partitions for 2023\nCREATE TABLE sales_2023_jan PARTITION OF sales_2023 FOR VALUES IN (1);\nCREATE TABLE sales_2023_feb PARTITION OF sales_2023 FOR VALUES IN (2);\nCREATE TABLE sales_2023_mar PARTITION OF sales_2023 FOR VALUES IN (3);\n-- Continue adding monthly partitions for 2023...\n\n-- Step 4: Insert sample data\nINSERT INTO sales (sale_date, amount)\nVALUES ('2022-01-15', 100.00);\nINSERT INTO sales (sale_date, amount)\nVALUES ('2022-02-20', 150.00);\nINSERT INTO sales (sale_date, amount)\nVALUES ('2023-03-10', 200.00);\n</code></pre> <p>To transform the <code>sale_date</code> column in the <code>sales</code> table and all its partitions, you can use the following configuration:</p> <pre><code>- schema: public\n  name: sales\n  apply_for_inherited: true\n  transformers:\n    - name: RandomDate\n      params:\n        min: \"2022-01-01\"\n        max: \"2022-03-01\"\n        column: \"sale_date\"\n        engine: \"random\"\n</code></pre>"},{"location":"built_in_transformers/transformation_inheritance/#example-2-simple-table-references","title":"Example 2. Simple table references","text":"<p>This is ordinary table references where the primary key of the <code>users</code> table is referenced in the <code>orders</code> table.</p> <pre><code>-- Enable the extension for UUID generation (if not enabled)\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n\nCREATE TABLE users\n(\n    user_id  UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    username VARCHAR(50) NOT NULL\n);\n\nCREATE TABLE orders\n(\n    order_id   UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    user_id    UUID REFERENCES users (user_id),\n    order_date DATE NOT NULL\n);\n\nINSERT INTO users (username)\nVALUES ('john_doe');\nINSERT INTO users (username)\nVALUES ('jane_smith');\n\nINSERT INTO orders (user_id, order_date)\nVALUES ((SELECT user_id FROM users WHERE username = 'john_doe'), '2024-10-31'),\n       ((SELECT user_id FROM users WHERE username = 'jane_smith'), '2024-10-30');\n</code></pre> <p>To transform the <code>username</code> column in the <code>users</code> table, you can use the following configuration:</p> <pre><code>- schema: public\n  name: users\n  apply_for_inherited: true\n  transformers:\n    - name: RandomUuid\n      apply_for_references: true\n      params:\n        column: \"user_id\"\n        engine: \"hash\"\n</code></pre> <p>This will apply the <code>RandomUuid</code> transformation to the <code>user_id</code> column in the <code>orders</code> table automatically.</p>"},{"location":"built_in_transformers/transformation_inheritance/#example-3-references-on-tables-with-end-to-end-identifiers","title":"Example 3. References on tables with end-to-end identifiers","text":"<p>In this example, we have three tables: <code>tablea</code>, <code>tableb</code>, and <code>tablec</code>. All tables have a composite primary key. In the tables <code>tableb</code> and <code>tablec</code>, the primary key is also a foreign key that references the primary key of <code>tablea</code>. This means that all PKs are end-to-end identifiers.</p> <pre><code>CREATE TABLE tablea\n(\n    id1  INT,\n    id2  INT,\n    data VARCHAR(50),\n    PRIMARY KEY (id1, id2)\n);\n\nCREATE TABLE tableb\n(\n    id1    INT,\n    id2    INT,\n    detail VARCHAR(50),\n    PRIMARY KEY (id1, id2),\n    FOREIGN KEY (id1, id2) REFERENCES tablea (id1, id2) ON DELETE CASCADE\n);\n\nCREATE TABLE tablec\n(\n    id1         INT,\n    id2         INT,\n    description VARCHAR(50),\n    PRIMARY KEY (id1, id2),\n    FOREIGN KEY (id1, id2) REFERENCES tableb (id1, id2) ON DELETE CASCADE\n);\n\nINSERT INTO tablea (id1, id2, data)\nVALUES (1, 1, 'Data A1'),\n       (2, 1, 'Data A2'),\n       (3, 1, 'Data A3');\n\nINSERT INTO tableb (id1, id2, detail)\nVALUES (1, 1, 'Detail B1'),\n       (2, 1, 'Detail B2'),\n       (3, 1, 'Detail B3');\n\nINSERT INTO tablec (id1, id2, description)\nVALUES (1, 1, 'Description C1'),\n       (2, 1, 'Description C2'),\n       (3, 1, 'Description C3');\n</code></pre> <p>To transform the <code>data</code> column in <code>tablea</code>, you can use the following configuration:</p> <pre><code>- schema: public\n  name: \"tablea\"\n  apply_for_inherited: true\n  transformers:\n    - name: RandomInt\n      apply_for_references: true\n      params:\n        min: 0\n        max: 100\n        column: \"id1\"\n        engine: \"hash\"\n    - name: RandomInt\n      apply_for_references: true\n      params:\n        min: 0\n        max: 100\n        column: \"id2\"\n        engine: \"hash\"\n</code></pre> <p>This will apply the <code>RandomInt</code> transformation to the <code>id1</code> and <code>id2</code> columns in <code>tableb</code> and <code>tablec</code> automatically.</p>"},{"location":"built_in_transformers/advanced_transformers/","title":"Advanced transformers","text":"<p>Advanced transformers are modifiable anonymization methods that users can adjust based on their needs by using custom functions.</p> <p>Below you can find an index of all advanced transformers currently available in Greenmask.</p> <ol> <li>Json \u2014 changes a JSON content by using <code>delete</code> and <code>set</code> operations.</li> <li>Template \u2014 executes a Go template of your choice and applies the result to a specified column.</li> <li>TemplateRecord \u2014 modifies records by using a Go template of your choice and applies the changes via the PostgreSQL driver.</li> </ol>"},{"location":"built_in_transformers/advanced_transformers/json/","title":"Json","text":"<p>Change a JSON document using <code>delete</code> and <code>set</code> operations. <code>NULL</code> values are kept.</p>"},{"location":"built_in_transformers/advanced_transformers/json/#parameters","title":"Parameters","text":"Name Properties Description Default Required Supported DB types column The name of the column to be affected Yes json, jsonb operations A list of operations that contains editing <code>delete</code> and <code>set</code> Yes - \u221f operation Specifies the operation type: <code>set</code> or <code>delete</code> Yes - \u221f path The path to an object to be modified. See path syntax below. Yes - \u221f value A value to be assigned to the provided path No - \u221f value_template A Golang template to be assigned to the provided path. See the list of template functions below. No - \u221f error_not_exist Throws an error if the key does not exist by the provided path. Disabled by default. <code>false</code> No -"},{"location":"built_in_transformers/advanced_transformers/json/#description","title":"Description","text":"<p>The <code>Json</code> transformer applies a sequence of changing operations (<code>set</code> and/or <code>delete</code>) to a JSON document. The value can be static or dynamic. For the <code>set</code> operation type, a static value is provided in the <code>value</code> parameter, while a dynamic value is provided in the <code>value_template</code> parameter, taking the data received after template execution as a result. Both the <code>value</code> and <code>value_template</code> parameters are mandatory for the <code>set</code> operation.</p>"},{"location":"built_in_transformers/advanced_transformers/json/#path-syntax","title":"Path syntax","text":"<p>The Json transformer is based on tidwall/sjson and supports the same path syntax. See their documentation for syntax rules.</p>"},{"location":"built_in_transformers/advanced_transformers/json/#template-functions","title":"Template functions","text":"Function Description Signature <code>.GetPath</code> Returns the current path to which the operation is being applied <code>.GetPath() (path string)</code> <code>.GetOriginalValue</code> Returns the original value to which the current operation path is pointing. If the value at the specified path does not exist, it returns  <code>nil</code>. <code>.GetOriginalValue() (value any)</code> <code>.OriginalValueExists</code> Returns a boolean value indicating whether the specified path exists or not. <code>.OriginalValueExists() (exists bool)</code> <code>.GetColumnValue</code> Returns an encoded into Golang type value for a specified column or throws an error. A value can be any of <code>int</code>, <code>float</code>, <code>time</code>, <code>string</code>, <code>bool</code>, or <code>slice</code> or <code>map</code>. <code>.GetColumnValue(name string) (value any, err error)</code> <code>.GetRawColumnValue</code> Returns a raw value for a specified column as a string or throws an error <code>.GetRawColumnValue(name string) (value string, err error)</code> <code>.EncodeValueByColumn</code> Encodes a value of any type into its raw string representation using the specified column name. Encoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.EncodeValueByColumn(name string, value any) (res any, err error)</code> <code>.DecodeValueByColumn</code> Decodes a value from its raw string representation to a Golang type using the specified column name. Decoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.DecodeValueByColumn(name string, value any) (res any, err error)</code> <code>.EncodeValueByType</code> Encodes a value of any type into its string representation using the specified type name. Encoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.EncodeValueByType(name string, value any) (res any, err error)</code> <code>.DecodeValueByType</code> Decodes a value from its raw string representation to a Golang type using the specified type name. Decoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.DecodeValueByType(name string, value any) (res any, err error)</code>"},{"location":"built_in_transformers/advanced_transformers/json/#example-changing-json-document","title":"Example: Changing JSON document","text":"Json transformer example<pre><code>- schema: \"bookings\"\n  name: \"aircrafts_data\"\n  transformers:\n    - name: \"Json\"\n      params:\n        column: \"model\"\n        operations:\n          - operation: \"set\"\n            path: \"en\"\n            value: \"Boeing 777-300-2023\"\n          - operation: \"set\"\n            path: \"seats\"\n            error_not_exist: True\n            value_template: \"{{ randomInt 100 400 }}\"\n          - operation: \"set\"\n            path: \"details.preperties.1\"\n            value: {\"name\": \"somename\", \"description\": null}\n          - operation: \"delete\"\n            path: \"values.:2\"\n</code></pre>"},{"location":"built_in_transformers/advanced_transformers/template/","title":"Template","text":"<p>Execute a Go template and automatically apply the result to a specified column.</p>"},{"location":"built_in_transformers/advanced_transformers/template/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes any template A Go template string Yes - validate Validates the template result using the PostgreSQL driver decoding procedure. Throws an error if a custom type does not have an encode-decoder implementation. false No -"},{"location":"built_in_transformers/advanced_transformers/template/#description","title":"Description","text":"<p>The <code>Template</code> transformer executes Go templates and automatically applies the template result to a specified column. Go template system is designed to be extensible, enabling developers to access data objects and incorporate custom functions programmatically. For more information, you can refer to the official Go Template documentation.</p> <p>With the <code>Template</code> transformer, you can implement complicated transformation logic using basic or custom template functions. Below you can get familiar with the basic template functions for the <code>Template</code> transformer. For more information about available custom template functions, see Custom functions.</p> <p>Warning</p> <p>Pay attention to the whitespaces in templates. Use dash-wrapped - brackets <code>{{- -}}</code> for trimming the spaces. For example, the value <code>\"2023-12-19\"</code> is not the same as <code>\" 2023-12-19  \"</code> and it may throw an error when restoring.</p>"},{"location":"built_in_transformers/advanced_transformers/template/#template-functions","title":"Template functions","text":"Function Description Signature <code>.GetColumnType</code> Returns a string with the column type. <code>.GetColumnType(name string) (typeName string, err error)</code> <code>.GetValue</code> Returns the column value for column assigned in the <code>column</code> parameter, encoded by the PostgreSQL driver into any type along with any associated error. Supported types include <code>int</code>, <code>float</code>, <code>time</code>, <code>string</code>, <code>bool</code>, as well as <code>slice</code> or <code>map</code> of any type. <code>.GetValue() (value any, err error)</code> <code>.GetRawValue</code> Returns a raw value as a string for column assigned in the <code>column</code> parameter. <code>.GetRawColumnValue(name string) (value string, err error)</code> <code>.GetColumnValue</code> Returns an encoded value for a specified column or throws an error. A value can be any of <code>int</code>, <code>float</code>, <code>time</code>, <code>string</code>, <code>bool</code>, or <code>slice</code> or <code>map</code>. <code>.GetColumnValue(name string) (value any, err error)</code> <code>.GetRawColumnValue</code> Returns a raw value for a specified column as a string or throws an error <code>.GetRawColumnValue(name string) (value string, err error)</code> <code>.EncodeValue</code> Encodes a value of any type into its string representation using the type assigned to the table column specified in the <code>column</code> parameter. Encoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.EncodeValue(value any) (res any, err error)</code> <code>.DecodeValue</code> Decodes a value from its raw string representation to a Golang type using the data type assigned to the table column specified in the <code>column</code> parameter. Decoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.DecodeValueByColumn(value any) (res any, err error)</code> <code>.EncodeValueByColumn</code> Encodes a value of any type into its raw string representation using the specified column name. Encoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.EncodeValueByColumn(name string, value any) (res any, err error)</code> <code>.DecodeValueByColumn</code> Decodes a value from its raw string representation to a Golang type using the specified column name. Decoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.DecodeValueByColumn(name string, value any) (res any, err error)</code> <code>.EncodeValueByType</code> Encodes a value of any type into its string representation using the specified type name. Encoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.EncodeValueByType(name string, value any) (res any, err error)</code> <code>.DecodeValueByType</code> Decodes a value from its raw string representation to a Golang type using the specified type name. Decoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.DecodeValueByType(name string, value any) (res any, err error)</code>"},{"location":"built_in_transformers/advanced_transformers/template/#example-update-the-firstname-column","title":"Example: Update the <code>firstname</code> column","text":"<p>Below you can see the table structure:</p> <p></p>"},{"location":"built_in_transformers/advanced_transformers/template/#change-rule","title":"Change rule","text":"<p>The goal is to modify the <code>firstname</code> column based on the following conditions:</p> <ul> <li>If the current value of the <code>firstname</code> column is equal to <code>Terri</code>, replace it with <code>Mary</code>.</li> <li>For all other cases, generate a random name and append <code>Jr</code>.</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/template/#using-a-template-function","title":"Using a template function","text":"<p>To generate random names, you can use the <code>fakerFirstName</code> template function, which is designed to create synthetic names.</p> Template transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformation:\n    - name: \"Template\"\n      params:\n        column: \"firstname\"\n        template: &gt;\n          {{- if eq .GetValue \"Terri\" -}}\n            Mary\n          {{- else -}}\n            {{- fakerFirstName -}} Jr\n          {{- end -}}\n\n        validate: true\n</code></pre> <p>Expected result:</p> Value = TerryValue != Terri column name original value transformed firstname Terri Mary column name original value transformed firstname Ken Jr Mike"},{"location":"built_in_transformers/advanced_transformers/template_record/","title":"TemplateRecord","text":"<p>Modify records using a Go template and apply changes by using the PostgreSQL driver functions. This transformer provides a way to implement custom transformation logic.</p>"},{"location":"built_in_transformers/advanced_transformers/template_record/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types columns A list of columns to be affected by the template. The list of columns will be checked for constraint violations. No any template A Go template string Yes - validate Validate the template result via PostgreSQL driver decoding procedure. Throws an error if a custom type does not have an encode-decoder implementation. false No -"},{"location":"built_in_transformers/advanced_transformers/template_record/#description","title":"Description","text":"<p><code>TemplateRecord</code> uses Go templates to change data. However, while the Template transformer operates with a single column and automatically applies results, the <code>TemplateRecord</code> transformer can make changes to a set of columns in the string, and using driver functions <code>.SetValue</code> or <code>.SetRawValue</code> is mandatory to do that.</p> <p>With the <code>TemplateRecord</code> transformer, you can implement complicated transformation logic using basic or custom template functions. Below you can get familiar with the basic template functions for the <code>TemplateRecord</code> transformer. For more information about available custom template functions, see Custom functions.</p>"},{"location":"built_in_transformers/advanced_transformers/template_record/#template-functions","title":"Template functions","text":"Function Description Signature <code>.GetColumnType</code> Returns a string with the column type. <code>.GetColumnType(name string) (typeName string, err error)</code> <code>.GetColumnValue</code> Returns an encoded value for a specified column or throws an error. A value can be any of <code>int</code>, <code>float</code>, <code>time</code>, <code>string</code>, <code>bool</code>, or <code>slice</code> or <code>map</code>. <code>.GetColumnValue(name string) (value any, err error)</code> <code>.GetRawColumnValue</code> Returns a raw value for a specified column as a string or throws an error <code>.GetRawColumnValue(name string) (value string, err error)</code> <code>.SetColumnValue</code> Sets a new value of a specific data type to the column. The value assigned must be compatible with the PostgreSQL data type of the column. For example, it is allowed to assign an <code>int</code> value to an <code>INTEGER</code> column, but you cannot assign a <code>float</code> value to a <code>timestamptz</code> column. <code>SetColumnValue(name string, v any) (bool, error)</code> <code>.SetRawColumnValue</code> Sets a new raw value for a column, inheriting the column's existing data type, without performing data type validation. This can lead to errors when restoring the dump if the assigned value is not compatible with the column type. To ensure compatibility, consider using the <code>.DecodeValueByColumn</code> function followed by <code>.SetColumnValue</code>, for example, <code>{{ \"13\" \\| .DecodeValueByColumn \"items_amount\" \\| .SetColumnValue \"items_amount\" }}</code>. <code>.SetRawColumnValue(name string, value any) (err error)</code> <code>.EncodeValueByColumn</code> Encodes a value of any type into its raw string representation using the specified column name. Encoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.EncodeValueByColumn(name string, value any) (res any, err error)</code> <code>.DecodeValueByColumn</code> Decodes a value from its raw string representation to a Golang type using the specified column name. Decoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.DecodeValueByColumn(name string, value any) (res any, err error)</code> <code>.EncodeValueByType</code> Encodes a value of any type into its string representation using the specified type name. Encoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.EncodeValueByType(name string, value any) (res any, err error)</code> <code>.DecodeValueByType</code> Decodes a value from its raw string representation to a Golang type using the specified type name. Decoding is performed through the PostgreSQL driver. Throws an error if types are incompatible. <code>.DecodeValueByType(name string, value any) (res any, err error)</code>"},{"location":"built_in_transformers/advanced_transformers/template_record/#example-generate-a-random-created_at-and-updated_at-dates","title":"Example: Generate a random <code>created_at</code> and <code>updated_at</code> dates","text":"<p>Below you can see the table structure:</p> <p></p> <p>The goal is to modify the <code>\"created_at\"</code> and <code>\"updated_at\"</code> columns based on the following rules:</p> <ul> <li>Do not change the value if the <code>created_at</code> is Null.</li> <li>If the <code>created_at</code> is not Null, generate the current time and use it as the minimum threshold for randomly   generating the <code>updated_at</code> value.</li> <li>Assign all generated values using the <code>.SetColumnValue</code> function.</li> </ul> Template transformer example<pre><code>- name: \"TemplateRecord\"\n  params:\n    columns:\n      - \"created_at\"\n      - \"updated_at\"\n    template: &gt;\n      {{ $val := .GetColumnValue \"created_at\" }}\n      {{ if isNotNull $val }}\n          {{ $createdAtValue := now }}\n          {{ $maxUpdatedDate := date_modify \"24h\" $createdAtValue }}\n          {{ $updatedAtValue := randomDate $createdAtValue $maxUpdatedDate }}\n          {{ .SetColumnValue \"created_at\" $createdAtValue }}\n          {{ .SetColumnValue \"updated_at\" $updatedAtValue }}\n      {{ end }}\n    validate: true\n</code></pre> <p>Expected result:</p> column name original value transformed created_at 2021-01-20 07:01:00.513325+00 2023-12-17 19:37:29.910054Z updated_at 2021-08-09 21:27:00.513325+00 2023-12-18 10:05:25.828498Z"},{"location":"built_in_transformers/advanced_transformers/custom_functions/","title":"Template custom functions","text":"<p>Within Greenmask, custom functions play a crucial role, providing a wide array of options for implementing diverse logic. Under the hood, the custom functions are based on the sprig Go's template functions. Greenmask enhances this capability by introducing additional functions and transformation functions. These extensions mirror the logic found in the standard transformers but offer you the flexibility to implement intricate and comprehensive logic tailored to your specific needs.</p> <p>Currently, you can use template custom functions for the advanced transformers:</p> <ul> <li>Json</li> <li>Template</li> <li>TemplateRecord</li> </ul> <p>and for the Transformation condition feature as well.</p> <p>Custom functions are arbitrarily divided into 2 groups:</p> <ul> <li>Core functions \u2014 custom functions that vary in purpose and include PostgreSQL driver, JSON   output, testing, and transformation functions.</li> <li>Faker functions \u2014 custom function of a faker type which generate synthetic data.</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/","title":"Core functions","text":"<p>Below you can find custom core functions which are divided into categories based on the transformation purpose.</p>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#postgresql-driver-functions","title":"PostgreSQL driver functions","text":"Function Description <code>null</code> Returns the <code>NULL</code> value that can be used for the driver encoding-decoding operations <code>isNull</code> Returns <code>true</code> if the checked value is <code>NULL</code> <code>isNotNull</code> Returns <code>true</code> if the checked value is not <code>NULL</code> <code>sqlCoalesce</code> Works as a standard SQL <code>coalesce</code> function. It allows you to choose the first non-NULL argument from the list."},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#json-output-function","title":"JSON output function","text":"Function Description <code>jsonExists</code> Checks if the path value exists in JSON. Returns <code>true</code> if the path exists. <code>mustJsonGet</code> Gets the JSON attribute value by path and throws an error if the path does not exist <code>mustJsonGetRaw</code> Gets the JSON attribute raw value by path and throws an error if the path does not exist <code>jsonGet</code> Gets the JSON attribute value by path and returns nil if the path does not exist <code>jsonGetRaw</code> Gets the JSON attribute raw value by path and returns nil if the path does not exist <code>jsonSet</code> Sets the value for the JSON document by path <code>jsonSetRaw</code> Sets the raw value for the JSON document by path <code>jsonDelete</code> Deletes an attribute from the JSON document by path <code>jsonValidate</code> Validates the JSON document syntax and throws an error if there are any issues <code>jsonIsValid</code> Checks the JSON document for validity and returns <code>true</code> if it is valid <code>toJsonRawValue</code> Casts any type of value to the raw JSON value"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#testing-functions","title":"Testing functions","text":"Function Description <code>isInt</code> Checks if the value of an integer type <code>isFloat</code> Checks if the value of a float type <code>isNil</code> Checks if the value is nil <code>isString</code> Checks if the value of a string type <code>isMap</code> Checks if the value of a map type <code>isSlice</code> Checks if the value of a slice type <code>isBool</code> Checks if the value of a boolean type"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#transformation-and-generators","title":"Transformation and generators","text":""},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#masking","title":"masking","text":"<p>Replaces characters with asterisk <code>*</code> symbols depending on the provided masking rule. If the value is <code>NULL</code>, it is kept unchanged. This function is based on ggwhite/go-masker.</p> Masking rulesSignatureParametersReturn values Rule Description Example input Example output <code>default</code> Returns the sequence of <code>*</code> symbols of the same length <code>test1234</code> <code>********</code> <code>name</code> Masks the second and the third letters <code>ABCD</code> <code>A**D</code> <code>password</code> Always returns a sequence of <code>*</code> <code>address</code> Keeps first 6 letters, masks the rest <code>Larnaca, makarios st</code> <code>Larnac*************</code> <code>email</code> Keeps a domain and the first 3 letters, masks the rest <code>ggw.chang@gmail.com</code> <code>ggw****@gmail.com</code> <code>mobile</code> Masks 3 digits starting from the 4th digit <code>0987654321</code> <code>0987***321</code> <code>telephone</code> Removes <code>(</code>, <code>)</code>, <code></code>, <code>-</code> symbols, masks last 4 digits of a telephone number, and formats it to <code>(??)????-????</code> <code>0227993078</code> <code>(02)2799-****</code> <code>id</code> Masks last 4 digits of an ID <code>A123456789</code> <code>A12345****</code> <code>credit_card</code> Masks 6 digits starting from the 7th digit <code>1234567890123456</code> <code>123456******3456</code> <code>url</code> Masks the password part of the URL (if applicable) <code>http://admin:mysecretpassword@localhost:1234/uri</code> <code>http://admin:xxxxx@localhost:1234/uri</code> <p><code>masking(dataType string, value string) (res string, err error)</code></p> <ul> <li><code>dataType</code> \u2014 one of the masking rules (see previous tab)</li> <li><code>value</code> \u2014 the original string value</li> </ul> <ul> <li><code>res</code> \u2014 a masked string</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#truncatedate","title":"truncateDate","text":"<p>Truncates datetime up to the provided <code>part</code>.</p> SignatureParametersReturn values <p><code>truncateDate(part string, original time.Time) (res time.Time, err error)</code></p> <ul> <li><code>part</code> \u2014 the truncation part. Must be one of <code>nano</code>, <code>second</code>, <code>minute</code>, <code>hour</code>, <code>day</code>, <code>month</code>, or <code>year</code></li> <li><code>original</code> \u2014 the original datetime value</li> </ul> <ul> <li><code>res</code> \u2014 a truncated datetime</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#noisedatepginterval","title":"noiseDatePgInterval","text":"<p>Adds or subtracts a random duration in the provided <code>interval</code> to or from the original date value.</p> SignatureParametersReturn values <p><code>noiseDate(interval string, original time.Time) (res time.Time, err error)</code></p> <ul> <li><code>interval</code> \u2014 the maximum value of <code>ratio</code> that is added to the original value. The format is the same as in the PostgreSQL interval format.</li> <li><code>original</code> \u2014 the original time value</li> </ul> <ul> <li><code>res</code> \u2014 a noised date</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#noisefloat","title":"noiseFloat","text":"<p>Adds or subtracts a random fraction to or from the original float value. Multiplies the original float value by a provided random value that is not higher than the <code>ratio</code> parameter and adds it to the original value with the option to specify the decimal via the <code>decimal</code> parameter.</p> SignatureParametersReturn values <p><code>noiseFloat(ratio float, decimal int, value float) (res float64, err error)</code></p> <ul> <li><code>ratio</code> \u2014 the maximum multiplier value in the interval (0:1). The value will be randomly generated up to <code>ratio</code>, multiplied by the original value, and the result will be added to the original value.</li> <li><code>decimal</code> \u2014 the decimal of the resulted value</li> <li><code>value</code> \u2014 the original value</li> </ul> <ul> <li><code>res</code> \u2014 a noised float value</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#noiseint","title":"noiseInt","text":"<p>Adds or subtracts a random fraction to or from the original integer value. Multiplies the original integer value by a provided random value that is not higher than the <code>ratio</code> parameter and adds it to the original value.</p> SignatureParametersReturn values <p><code>noiseInt(ratio float, value float) (res int, err error)</code></p> <ul> <li><code>ratio</code> \u2014 the max multiplier value in the interval (0:1). The value will be generated randomly up to <code>ratio</code>, multiplied by the original value, and the result will be added to the original value.</li> <li><code>value</code> \u2014 the original value</li> </ul> <ul> <li><code>res</code> \u2014 a noised integer value</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#randombool","title":"randomBool","text":"<p>Generates a random boolean value.</p>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#randomdate","title":"randomDate","text":"<p>Generates a random date within the provided interval.</p> SignatureParametersReturn values <p><code>randomDate(min time.Time, max time.Time) (res time.Time, err error)</code></p> <ul> <li><code>min</code> \u2014 the minimum random value threshold</li> <li><code>max</code> \u2014 the maximum random value threshold</li> </ul> <ul> <li><code>res</code> \u2014 a randomly generated date value</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#randomfloat","title":"randomFloat","text":"<p>Generates a random float value within the provided interval.</p> SignatureParametersReturn values <p><code>randomFloat(min any, max any, decimal int) (res float, err error)</code></p> <ul> <li><code>min</code> \u2014 the minimum random value threshold</li> <li><code>max</code> \u2014 the maximum random value threshold</li> <li><code>decimal</code> \u2014 the decimal of the resulted value</li> </ul> <ul> <li><code>res</code> \u2014 a randomly generated float value</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#randomint","title":"randomInt","text":"<p>Generates a random integer value within the provided interval.</p> SignatureParametersReturn values <p><code>randomInt(min int, max int) (res int, err error)</code></p> <ul> <li><code>min</code> \u2014 the minimum random value threshold</li> <li><code>max</code> \u2014 the maximum random value threshold</li> </ul> <ul> <li><code>res</code> \u2014 a randomly generated int value</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#randomstring","title":"randomString","text":"<p>Generates a random string using the provided characters within the specified length range.</p> SignatureParametersReturn values <p><code>randomString(minLength int, maxLength int, symbols string) (res string, err error)</code></p> <ul> <li><code>minLength</code> \u2014 the minimum string length</li> <li><code>maxLength</code> \u2014 the maximum string length</li> <li><code>symbols</code> \u2014 a string with a set of symbols which can be used. The default value is   <code>abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890</code></li> </ul> <ul> <li><code>res</code> \u2014 a randomly generated string value</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#roundfloat","title":"roundFloat","text":"<p>Rounds a float value up to provided decimal.</p> SignatureParametersReturn values <p><code>roundFloat(decimal int, original float) (res float, err error)</code></p> <ul> <li><code>decimal</code> \u2014 the decimal of the value</li> <li><code>original</code> \u2014 the original float value</li> </ul> <ul> <li><code>res</code> \u2014 a rounded float value</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/core_functions/#tsmodify","title":"tsModify","text":"<p>Modify original time value by adding or subtracting the provided interval. The interval is a string in the format of the PostgreSQL interval.</p> SignatureParametersReturn values <p><code>tsModify(interval string, val time.Time) (time.Time, error)</code></p> <ul> <li><code>interval</code> \u2014 the maximum value of <code>ratio</code> that is added to the original value. The format is the same as in the PostgreSQL interval format.</li> <li><code>original</code> \u2014 the original time value</li> </ul> <ul> <li><code>res</code> \u2014 a modified date</li> <li><code>err</code> \u2014 an error if there is an issue</li> </ul>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/faker_function/","title":"Faker functions","text":"<p>Greenmask uses go-faker/faker under the hood for generating of synthetic data.</p>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/faker_function/#faker-functions-address","title":"Faker functions: Address","text":"Function Description Signature <code>fakerRealAddress</code> Generates a random real-world address that includes: city, state, postal code, latitude, and longitude <code>fakerRealAddress() (res ReadAddress)</code> <code>fakerLatitude</code> Generates random fake latitude <code>fakerLatitude() (res float64)</code> <code>fakerLongitude</code> Generates random fake longitude <code>fakerLongitude() (res float64)</code>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/faker_function/#faker-functions-datetime","title":"Faker functions: Datetime","text":"Function Description Signature <code>fakerUnixTime</code> Generates random Unix time in seconds <code>fakerLongitude() (res int64)</code> <code>fakerDate</code> Generates random date with the pattern of <code>YYYY-MM-DD</code> <code>fakerDate() (res string)</code> <code>fakerTimeString</code> Generates random time <code>fakerTimeString() (res string)</code> <code>fakerMonthName</code> Generates a random month <code>fakerMonthName() (res string)</code> <code>fakerYearString</code> Generates a random year <code>fakerYearString() (res string)</code> <code>fakerDayOfWeek</code> Generates a random day of a week <code>fakerDayOfWeek() (res string)</code> <code>fakerDayOfMonth</code> Generates a random day of a month <code>fakerDayOfMonth() (res string)</code> <code>fakerTimestamp</code> Generates a random timestamp with the pattern of <code>YYYY-MM-DD HH:MM:SS</code> <code>fakerTimestamp() (res string)</code> <code>fakerCentury</code> Generates a random century <code>fakerCentury() (res string)</code> <code>fakerTimezone</code> Generates a random timezone name <code>fakerTimezone() (res string)</code> <code>fakerTimeperiod</code> Generates a random time period with the patter of either <code>AM</code> or <code>PM</code> <code>fakerTimeperiod() (res string)</code>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/faker_function/#faker-functions-internet","title":"Faker functions: Internet","text":"Function Description Signature <code>fakerEmail</code> Generates a random email <code>fakerEmail() (res string)</code> <code>fakerMacAddress</code> Generates a random MAC address <code>fakerMacAddress() (res string)</code> <code>fakerDomainName</code> Generates a random domain name <code>fakerDomainName() (res string)</code> <code>fakerURL</code> Generates a random URL with the pattern of <code>https://www.domainname.some/somepath</code> <code>fakerURL() (res string)</code> <code>fakerUsername</code> Generates a random username <code>fakerUsername() (res string)</code> <code>fakerIPv4</code> Generates a random IPv4 address <code>fakerIPv4() (res string)</code> <code>fakerIPv6</code> Generates a random IPv6 address <code>fakerIPv6() (res string)</code> <code>fakerPassword</code> Generates a random password <code>fakerPassword() (res string)</code>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/faker_function/#faker-functions-words-and-sentences","title":"Faker functions: words and sentences","text":"Function Description Signature <code>fakerWord</code> Generates a random word <code>fakerWord() (res string)</code> <code>fakerSentence</code> Generates a random sentence <code>fakerSentence() (res string)</code> <code>fakerParagraph</code> Generates a random sequence of sentences as a paragraph <code>fakerParagraph() (res string)</code>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/faker_function/#faker-functions-payment","title":"Faker functions: Payment","text":"Function Description Signature <code>fakerCCType</code> Generates a random credit card type, e.g. VISA, MasterCard, etc. <code>fakerCCType() (res string)</code> <code>fakerCCNumber</code> Generates a random credit card number <code>fakerCCNumber() (res string)</code> <code>fakerCurrency</code> Generates a random currency name <code>fakerCurrency() (res string)</code> <code>fakerAmountWithCurrency</code> Generates random amount preceded with random currency <code>fakerAmountWithCurrency() (res string)</code>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/faker_function/#faker-functions-person","title":"Faker functions: Person","text":"Function Description Signature <code>fakerTitleMale</code> Generates a random male title from the predefined list <code>fakerTitleMale() (res string)</code> <code>fakerTitleFemale</code> Generates a random female title from the predefined list <code>fakerTitleFemale() (res string)</code> <code>fakerFirstName</code> Generates a random first name <code>fakerFirstName() (res string)</code> <code>fakerFirstNameMale</code> Generates a random male first name <code>fakerFirstNameMale() (res string)</code> <code>fakerFirstNameFemale</code> Generates a random female first name <code>fakerFirstNameFemale() (res string)</code> <code>fakerFirstLastName</code> Generates a random last name <code>fakerFirstLastName() (res string)</code> <code>fakerName</code> Generates a random full name preceded with a title <code>fakerName() (res string)</code>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/faker_function/#faker-functions-phone","title":"Faker functions: Phone","text":"Function Description Signature <code>fakerPhoneNumber</code> Generates a random phone number <code>fakerPhoneNumber() (res string)</code> <code>fakerTollFreePhoneNumber</code> Generates a random phone number with the pattern of <code>(123) 456-7890</code> <code>fakerTollFreePhoneNumber() (res string)</code> <code>fakerE164PhoneNumber</code> Generates a random phone number with the pattern of <code>+12345678900</code> <code>fakerE164PhoneNumber() (res string)</code>"},{"location":"built_in_transformers/advanced_transformers/custom_functions/faker_function/#faker-functions-uuid","title":"Faker functions: UUID","text":"Function Description Signature <code>fakerUUIDHyphenated</code> Generates a random unique user ID separated by hyphens <code>fakerUUID() (res string)</code> <code>fakerUUIDDigit</code> Generates a random unique user ID in the HEX format <code>fakerUUIDDigit() (res string)</code>"},{"location":"built_in_transformers/standard_transformers/","title":"Standard transformers","text":"<p>Standard transformers are ready-to-use methods that require no customization and perform with just as little as parameters input. Below you can find an index of all standard transformers currently available in Greenmask.</p> <ol> <li>Cmd \u2014 transforms data via external program using <code>stdin</code> and <code>stdout</code> interaction.</li> <li>Dict \u2014 replaces values matched by dictionary keys.</li> <li>Hash \u2014 generates a hash of the text value.</li> <li>Masking \u2014 masks a value using one of the masking behaviors depending on your domain.</li> <li>NoiseDate \u2014 randomly adds or subtracts a duration within the provided ratio interval to the original date value.</li> <li>NoiseFloat \u2014 adds or subtracts a random fraction to the original float value.terval to the original date value.</li> <li>NoiseNumeric \u2014 adds or subtracts a random fraction to the original numeric value.</li> <li>NoiseInt \u2014 adds or subtracts a random fraction to the original integer value.</li> <li>RandomBool \u2014 generates random boolean values.</li> <li>RandomChoice \u2014 replaces values randomly chosen from a provided list.</li> <li>RandomDate \u2014 generates a random date in a specified interval.</li> <li>RandomFloat \u2014 generates a random float within the provided interval.</li> <li>RandomInt \u2014 generates a random integer within the provided interval.</li> <li>RandomString \u2014 generates a random string using the provided characters within the specified length range.</li> <li>RandomUuid \u2014 generates a random unique user ID.</li> <li>RandomLatitude \u2014 generates a random latitude value.</li> <li>RandomLongitude \u2014 generates a random longitude value.</li> <li>RandomUnixTimestamp \u2014 generates a random Unix timestamp.</li> <li>RandomDayOfWeek \u2014 generates a random day of the week.</li> <li>RandomDayOfMonth \u2014 generates a random day of the month.</li> <li>RandomMonthName \u2014 generates the name of a random month.</li> <li>RandomYearString \u2014 generates a random year as a string.</li> <li>RandomCentury \u2014 generates a random century.</li> <li>RandomTimezone \u2014 generates a random timezone.</li> <li>RandomEmail \u2014 generates a random email address.</li> <li>RandomUsername \u2014 generates a random username.</li> <li>RandomPassword \u2014 generates a random password.</li> <li>RandomDomainName \u2014 generates a random domain name.</li> <li>RandomURL \u2014 generates a random URL.</li> <li>RandomMac \u2014 generates a random MAC addresses.</li> <li>RandomIP \u2014 generates a random IPv4 or IPv6 addresses.</li> <li>RandomWord \u2014 generates a random word.</li> <li>RandomSentence \u2014 generates a random sentence.</li> <li>RandomParagraph \u2014 generates a random paragraph.</li> <li>RandomCCType \u2014 generates a random credit card type.</li> <li>RandomCCNumber \u2014 generates a random credit card number.</li> <li>RandomCurrency \u2014 generates a random currency code.</li> <li>RandomAmountWithCurrency \u2014 generates a random monetary amount with currency.</li> <li>RandomPerson \u2014 generates a random person data (first name, last name, etc.)</li> <li>RandomPhoneNumber \u2014 generates a random phone number.</li> <li>RandomTollFreePhoneNumber \u2014 generates a random toll-free phone number.</li> <li>RandomE164PhoneNumber \u2014 generates a random phone number in E.164 format.</li> <li>RealAddress \u2014 generates a real address.</li> <li>RegexpReplace \u2014 replaces a string using a regular expression.</li> <li>Replace \u2014 replaces an original value by the provided one.</li> <li>SetNull \u2014 sets <code>NULL</code> value to the column.</li> </ol>"},{"location":"built_in_transformers/standard_transformers/cmd/","title":"Cmd","text":"<p>Transform data via external program using <code>stdin</code> and <code>stdout</code> interaction.</p>"},{"location":"built_in_transformers/standard_transformers/cmd/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types columns A list of column names to be affected. If empty, the entire tuple is used. Read about the structure further. Yes Any executable The path to the <code>executable</code> parameter file Yes - args A list of parameters for the executable No - driver The row driver with parameters that is used for interacting with cmd. See details below. <code>{\"name\": \"csv\"}</code> No - validate Performs a decoding operation using the PostgreSQL driver for data received from the command to ensure the data format is correct <code>false</code> No - timeout Timeout for sending and receiving data from the external command <code>2s</code> No - expected_exit_code The expected exit code on SIGTERM signal. If the exit code is unexpected, the transformation exits with an error. <code>0</code> No - skip_on_behaviour Skips transformation call if one of the provided columns has a <code>null</code> value (<code>any</code>) or each of the provided columns has <code>null</code> values (<code>all</code>). This option works together with the <code>skip_on_null_input</code> parameter on columns. Possible values: <code>all</code>, <code>any</code>. <code>all</code> No - <p>Warning</p> <p>The parameter <code>validate_output=true</code> may cause an error if the type does not have a PostgreSQL driver decoder  implementation. Most of the types, such as <code>int</code>, <code>float</code>, <code>text</code>, <code>varchar</code>, <code>date</code>, <code>timestamp</code>, etc., have  encoders and decoders, as well as inherited types like domain types based on them.</p>"},{"location":"built_in_transformers/standard_transformers/cmd/#description","title":"Description","text":"<p>The <code>Cmd</code> transformer allows you to send original data to an external program via <code>stdin</code> and receive transformed data from <code>stdout</code>. It supports various interaction formats such as <code>json</code>, <code>csv</code>, or plain <code>text</code> for one-column transformations. The interaction is performed line by line, so at the end of each sent data, a new line symbol <code>\\n</code> must be included.</p>"},{"location":"built_in_transformers/standard_transformers/cmd/#types-of-interaction-modes","title":"Types of interaction modes","text":""},{"location":"built_in_transformers/standard_transformers/cmd/#text","title":"text","text":"<p>Textual driver that is used only for one column transformation, thus you cannot provide here more than one column. The value encodes into string laterally. For example, <code>2023-01-03 01:00:00.0+03</code>.</p>"},{"location":"built_in_transformers/standard_transformers/cmd/#json","title":"json","text":"<p>JSON line driver. It has two formats that can be passed through <code>driver.json_data_format</code>: <code>[text|bytes]</code>. Use the <code>bytes</code> format for binary datatypes. Use the <code>text</code> format for non-binary datatypes and for those that can be represented as string literals. The default <code>json_data_format</code> is <code>text</code>.</p> Text format with indexesBytes format with indexes <pre><code>{\n  \"column1\": {\n    \"d\": \"some_value1\",\n    \"n\": false,\n  },\n  \"column2\": {\n    \"d\": \"some_value2\",\n    \"n\": false,\n  }\n}\n</code></pre> <pre><code>{\n  \"column1\": {\n    \"d\": \"aGVsbG8gd29ybHNeODcxMjE5MCUlJSUlJQ==\",\n    \"n\": false,\n  },\n  \"column2\": {\n    \"d\": \"aGVsbG8gd29ybHNeODcxMjE5MCUlJSUlJQ==\",\n    \"n\": false,\n  }\n}\n</code></pre> <p>where:</p> <ul> <li>Each line is a JSON line with a map of attribute numbers to their values</li> <li><code>d</code> \u2014 the raw data represented as base64 encoding for the bytes format or Unicode text for the text format. The base64   encoding is needed because data can be binary.</li> <li><code>n</code> \u2014 indicates if NULL is present</li> </ul>"},{"location":"built_in_transformers/standard_transformers/cmd/#csv","title":"csv","text":"<p>CSV driver (comma-separated). The number of attributes is the same as the number of table columns, but the columns that were not mentioned in the <code>columns</code> list are empty. The <code>NULL</code> value is represented as <code>\\N</code>. Each attribute is escaped by a quote (<code>\"</code>). For example, if the transformed table has attributes <code>id</code>, <code>title</code>, and <code>created_at</code>, and only <code>id</code> and <code>created_at</code> require transformation, then the CSV line will look as follows:</p> csv line example<pre><code>\"123\",\"\",\"2023-01-03 01:00:00.0+03\"\n</code></pre>"},{"location":"built_in_transformers/standard_transformers/cmd/#column-object-attributes","title":"Column object attributes","text":"<ul> <li><code>name</code> \u2014 the name of the column. This value is required. Depending on the attributes that follows further, this column   may be used just as a value and is not affected in any way.</li> <li><code>not_affected</code> \u2014 indicates whether the column is affected in the transformation. This attribute is required for the   validation procedure when Greenmask is called with <code>greenmask dump --validate</code>. Setting <code>not_affected=true</code> can be   helpful when the command transformer transforms data depending on the value of another column. For example, if you   want to generate an <code>updated_at</code> column value depending on the <code>created_at</code> column value, you can set <code>created_at</code>   to <code>not_affected=true</code>. The default value is <code>false</code>.</li> <li><code>skip_original_data</code> \u2014 indicates whether the original data is required for the transformer. This attribute can be   helpful for decreasing the interaction time. One use case is when the command works as a generator and returns the   value without relying on the original data. The default value is <code>false</code>.</li> <li><code>skip_on_null_input</code> \u2014 specifies whether to skip transformation when the original value is <code>null</code>. This attribute   works in conjunction with the <code>skip_on_behaviour</code> parameter. For example, if you have two affected columns   with <code>skip_on_null_input=true</code> and one column is <code>null</code>, then, if <code>skip_on_behaviour=any</code>, the transformation will be   skipped, or, if <code>skip_on_behaviour=and</code>, the transformation will be performed. The default is <code>false</code>.</li> </ul>"},{"location":"built_in_transformers/standard_transformers/cmd/#example-apply-transformation-performed-by-external-command-in-text-format","title":"Example: Apply transformation performed by external command in TEXT format","text":"<p>In the following example, <code>jobtitle</code> columns is transformed via external command transformer.</p> External transformer in python example<pre><code>#!/usr/bin/env python3\nimport signal\nimport sys\n\nsignal.signal(signal.SIGTERM, lambda sig, frame: exit(0))\n\n\n# If we want to implement a simple generator, we need read the line from stdin and write any result to stdout\nfor _ in sys.stdin:\n    # Writing the result to stdout with new line and flushing the buffer\n    sys.stdout.write(\"New Job Title\")\n    sys.stdout.write(\"\\n\")\n    sys.stdout.flush()\n</code></pre> Cmd transformer config example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"Cmd\"\n      params:\n        driver:\n          name: \"text\"\n        expected_exit_code: -1\n        skip_on_null_input: true\n        validate: true\n        skip_on_behaviour: \"any\"\n        timeout: 60s\n        executable: \"/var/lib/playground/test.py\"\n        columns:\n          - name: \"jobtitle\"\n            skip_original_data: true\n            skip_on_null_input: true \n</code></pre>"},{"location":"built_in_transformers/standard_transformers/cmd/#example-apply-transformation-performed-by-external-command-in-json-format","title":"Example: Apply transformation performed by external command in JSON format","text":"<p>In the following example, <code>jobtitle</code> and <code>loginid</code> columns are transformed via external command transformer.</p> External transformer in python example<pre><code>#!/usr/bin/env python3\nimport json\nimport signal\nimport sys\n\nsignal.signal(signal.SIGTERM, lambda sig, frame: exit(0))\n\nfor line in sys.stdin:\n    res = json.loads(line)\n    # Setting dummy values\n    res[\"jobtitle\"] = {\"d\": \"New Job Title\", \"n\": False}\n    res[\"loginid\"][\"d\"] = \"123\"\n\n    # Writing the result to stdout with new line and flushing the buffer\n    sys.stdout.write(json.dumps(res))\n    sys.stdout.write(\"\\n\")\n    sys.stdout.flush()\n</code></pre> Cmd transformer config example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"Cmd\"\n      params:\n        driver:\n          name: \"json\" # (1)\n          json_data_format: \"text\" # (4)\n        expected_exit_code: -1\n        skip_on_null_input: true\n        validate: true\n        skip_on_behaviour: \"any\" # (2)\n        timeout: 60s\n        executable: \"/var/lib/playground/test.py\"\n        columns:\n          - name: \"jobtitle\"\n            skip_original_data: true\n            skip_on_null_input: true # (3)\n          - name: \"loginid\"\n            skip_original_data: false # (5)\n            skip_on_null_input: true # (3)\n</code></pre> <p>{ .annotate }</p> <ol> <li>Validate the received data via decode procedure using the PostgreSQL driver. Note that this may cause an error if the    type is not supported in the PostgreSQL driver.</li> <li>Skip transformation (keep the values) if one of the affected columns (<code>not_affected=false</code>) has a null value.</li> <li>If a column has a null value, then skip it. This works in conjunction with <code>skip_on_behaviour</code>. Since it has the    value any, if one of the columns (<code>jobtitle</code> or <code>loginid</code>) has a <code>null</code> value, then skip the    transformation call.</li> <li>The format of JSON can be either <code>text</code> or <code>bytes</code>. The default value is <code>text</code>.</li> <li>The <code>skip_original_data</code> attribute is set to <code>true</code> the date will not be transfered to the command. This column    will contain the empty original data</li> </ol>"},{"location":"built_in_transformers/standard_transformers/dict/","title":"Dict","text":"<p>Replace values matched by dictionary keys.</p>"},{"location":"built_in_transformers/standard_transformers/dict/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes any values Value replace mapping as in: <code>{\"string\": \"string\"}</code>. The string with value <code>\"\\N\"</code> is considered NULL. No - default Shown if no value has been matched with dict. The string with value <code>\"\\N\"</code> is considered NULL. By default is empty. No - fail_not_matched When no value is matched with the dict, fails the replacement process if set to <code>true</code>, or keeps the current value, if set to <code>false</code>. <code>true</code> No - validate Performs the encode-decode procedure using column type to ensure that values have correct type <code>true</code> No -"},{"location":"built_in_transformers/standard_transformers/dict/#description","title":"Description","text":"<p>The <code>Dict</code> transformer uses a user-provided key-value dictionary to replace values based on matches specified in the <code>values</code> parameter mapping. These provided values must align with the PostgreSQL type format. To validate the values format before application, you can utilize the <code>validate</code> parameter, triggering a decoding procedure via the PostgreSQL driver.</p> <p>If there are no matches by key, an error will be raised according to a default <code>fail_not_matched: true</code> parameter. You can change this behaviour by providing the <code>default</code> parameter, value from which will be shown in case of a missing match.</p> <p>In certain cases where the driver type does not support the validation operation, an error may occur. For setting or matching a NULL value, use a string with the <code>\\N</code> sequence.</p>"},{"location":"built_in_transformers/standard_transformers/dict/#example-replace-marital-status","title":"Example: Replace marital status","text":"<p>The following example replaces marital status from <code>S</code> to <code>M</code> or from <code>M</code> to <code>S</code> and raises an error if there is no match:</p> Dict transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"Dict\"\n      params:\n        column: \"maritalstatus\"\n        values:\n          \"S\": \"M\"\n          \"M\": \"S\"\n        validate: true\n        fail_not_matched: true\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue maritalstatusSM"},{"location":"built_in_transformers/standard_transformers/hash/","title":"Hash","text":"<p>Generate a hash of the text value using the <code>Scrypt</code> hash function under the hood. <code>NULL</code> values are kept.</p>"},{"location":"built_in_transformers/standard_transformers/hash/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar salt Hex encoded salt string. This value may be provided via environment variable <code>GREENMASK_GLOBAL_SALT</code> Yes text, varchar function Hash algorithm to anonymize data. Can be any of <code>md5</code>, <code>sha1</code>, <code>sha256</code>, <code>sha512</code>, <code>sha3-224</code>, <code>sha3-254</code>, <code>sha3-384</code>, <code>sha3-512</code>. <code>sha1</code> No - max_length Indicates whether to truncate the hash tail and specifies at what length. Can be any integer number, where <code>0</code> means \"no truncation\". <code>0</code> No -"},{"location":"built_in_transformers/standard_transformers/hash/#example-generate-hash-from-job-title","title":"Example: Generate hash from job title","text":"<p>The following example generates a hash from the <code>jobtitle</code> into sha1 and truncates the results after the 10th character.</p> <p>We can set the salt via the environment variable <code>GREENMASK_GLOBAL_SALT</code>:</p> <pre><code>export GREENMASK_GLOBAL_SALT=\"12343567baaa\"\n</code></pre> Hash transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"Hash\"\n      params:\n        column: \"jobtitle\"\n        function: \"sha1\"\n        max_length: 10\n</code></pre> Expected result<pre><code>| column name | original value                   | transformed |\n|-------------|----------------------------------|-------------|\n| jobtitle    | Research and Development Manager | 3a456da5c5  |\n</code></pre>"},{"location":"built_in_transformers/standard_transformers/masking/","title":"Masking","text":"<p>Mask a value using one of the masking rules depending on your domain. <code>NULL</code> values are kept.</p>"},{"location":"built_in_transformers/standard_transformers/masking/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar type Data type of attribute (<code>default</code>, <code>password</code>, <code>name</code>, <code>addr</code>, <code>email</code>, <code>mobile</code>, <code>tel</code>, <code>id</code>, <code>credit</code>, <code>url</code>) <code>default</code> No -"},{"location":"built_in_transformers/standard_transformers/masking/#description","title":"Description","text":"<p>The <code>Masking</code> transformer replaces characters with asterisk <code>*</code> symbols depending on the provided data type. If the value is <code>NULL</code>, it is kept unchanged. It is based on ggwhite/go-masker and supports the following masking rules:</p> Type Description default Returns <code>*</code> symbols with the same length, e.g. input: <code>test1234</code> output: <code>********</code> name Masks the second letter the third letter in a word, e. g. input: <code>ABCD</code> output: <code>A**D</code> password Always returns <code>************</code> address Keeps first 6 letters, masks the rest, e. g. input: <code>Larnaca, makarios st</code> output: <code>Larnac*************</code> email Keeps a domain and the first 3 letters, masks the rest, e. g. input: <code>ggw.chang@gmail.com</code> output: <code>ggw****@gmail.com</code> mobile Masks 3 digits starting from the 4th digit, e. g. input: <code>0987654321</code> output: <code>0987***321</code> telephone Removes <code>(</code>, <code>)</code>, <code></code>, <code>-</code> chart, and masks last 4 digits of telephone number, then formats it to <code>(??)????-????</code>, e. g. input: <code>0227993078</code> output: <code>(02)2799-****</code> id Masks last 4 digits of ID number, e. g. input: <code>A123456789</code> output: <code>A12345****</code> credit_cart Masks 6 digits starting from the 7th digit, e. g. input <code>1234567890123456</code> output <code>123456******3456</code> url Masks the password part of the URL, if applicable, e. g. <code>http://admin:mysecretpassword@localhost:1234/uri</code> output: <code>http://admin:xxxxx@localhost:1234/uri</code>"},{"location":"built_in_transformers/standard_transformers/masking/#example-masking-employee-national-id-number","title":"Example: Masking employee national ID number","text":"<p>In the following example, the national ID number of an employee is masked.</p> Masking transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"Masking\"\n      params:\n        column: \"nationalidnumber\"\n        type: \"id\"\n</code></pre> Expected result<pre><code>| column name      | original value | transformed |\n|------------------|----------------|-------------|\n| nationalidnumber | 295847284      | 295847****  |\n</code></pre>"},{"location":"built_in_transformers/standard_transformers/noise_date/","title":"NoiseDate","text":"<p>Randomly add or subtract a duration within the provided <code>ratio</code> interval to the original date value.</p>"},{"location":"built_in_transformers/standard_transformers/noise_date/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes date, timestamp, timestamptz min_ratio The minimum random value for noise. The value must be in PostgreSQL interval format, e. g. <code>1 year 2 mons 3 day 04:05:06.07</code> 5% from max_ration parameter No - max_ratio The maximum random value for noise. The value must be in PostgreSQL interval format, e. g. <code>1 year 2 mons 3 day 04:05:06.07</code> Yes - min Min threshold date (and/or time) of value. The value has the same format as <code>column</code> parameter No - max Max threshold date (and/or time) of value. The value has the same format as <code>column</code> parameter No - truncate Truncate the date to the specified part (<code>nanosecond</code>, <code>microsecond</code>, <code>millisecond</code>, <code>second</code>, <code>minute</code>, <code>hour</code>, <code>day</code>, <code>month</code>, <code>year</code>). The truncate operation is not applied by default. No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/noise_date/#dynamic-parameters","title":"Dynamic parameters","text":"Parameter Supported types min date, timestamp, timestamptz max date, timestamp, timestamptz"},{"location":"built_in_transformers/standard_transformers/noise_date/#description","title":"Description","text":"<p>The <code>NoiseDate</code> transformer randomly generates duration between <code>min_ratio</code> and <code>max_ratio</code> parameter and adds it to or subtracts it from the original date value. The <code>min_ratio</code> or <code>max_ratio</code> parameters must be written in the PostgreSQL interval format. You can also truncate the resulted date up to a specified part by setting the <code>truncate</code> parameter.</p> <p>In case you have constraints on the date range, you can set the <code>min</code> and <code>max</code> parameters to specify the threshold values. The values for <code>min</code> and <code>max</code> must have the same format as the <code>column</code> parameter. Parameters min and max support dynamic mode.</p> <p>Info</p> <p>If the noised value exceeds the <code>max</code> threshold, the transformer will set the value to <code>max</code>. If the noised value is lower than the <code>min</code> threshold, the transformer will set the value to <code>min</code>.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/noise_date/#example-adding-noise-to-the-modified-date","title":"Example: Adding noise to the modified date","text":"<p>In the following example, the original <code>timestamp</code> value of <code>modifieddate</code> will be noised up to <code>1 year 2 months 3 days 4 hours 5 minutes 6 seconds and 7 milliseconds</code> with truncation up to the <code>month</code> part.</p> NoiseDate transformer example<pre><code>- schema: \"humanresources\"\n  name: \"jobcandidate\"\n  transformers:\n    - name: \"NoiseDate\"\n      params:\n        column: \"hiredate\"\n        max_ratio: \"1 year 2 mons 3 day 04:05:06.07\"\n        truncate: \"month\"\n        max: \"2020-01-01 00:00:00\"\n</code></pre>"},{"location":"built_in_transformers/standard_transformers/noise_date/#example-adding-noise-to-the-modified-date-with-dynamic-min-parameter-with-hash-engine","title":"Example: Adding noise to the modified date with dynamic min parameter with hash engine","text":"<p>In the following example, the original <code>timestamp</code> value of <code>hiredate</code> will be noised up to <code>1 year 2 months 3 days 4 hours 5 minutes 6 seconds and 7 milliseconds</code> with truncation up to the <code>month</code> part. The <code>max</code> threshold is set to <code>2020-01-01 00:00:00</code>, and the <code>min</code> threshold is set to the <code>birthdate</code> column. If the <code>birthdate</code> column is <code>NULL</code>, the default value <code>1990-01-01</code> will be used. The hash engine is used for deterministic generation - the same input will always produce the same output.</p> NoiseDate transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"NoiseDate\"\n      params:\n        column: \"hiredate\"\n        max_ratio: \"1 year 2 mons 3 day 04:05:06.07\"\n        truncate: \"month\"\n        max: \"2020-01-01 00:00:00\"\n        engine: \"hash\"\n      dynamic_params:\n        min:\n          column: \"birthdate\"\n          default: \"1990-01-01\"\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue hiredate2009-01-142010-08-01"},{"location":"built_in_transformers/standard_transformers/noise_float/","title":"NoiseFloat","text":"<p>Add or subtract a random fraction to the original float value.</p>"},{"location":"built_in_transformers/standard_transformers/noise_float/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes float4, float8 decimal The decimal of the noised float value (number of digits after the decimal point) <code>4</code> No - min_ratio The minimum random percentage for noise, from <code>0</code> to <code>1</code>, e. g. <code>0.1</code> means \"add noise up to 10%\" <code>0.05</code> No - max_ratio The maximum random percentage for noise, from <code>0</code> to <code>1</code>, e. g. <code>0.1</code> means \"add noise up to 10%\" Yes - min Min threshold of noised value No - max Max threshold of noised value No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/noise_float/#dynamic-parameters","title":"Dynamic parameters","text":"Parameter Supported types min float4, float8, int2, int4, int8 max float4, float8, int2, int4, int8"},{"location":"built_in_transformers/standard_transformers/noise_float/#description","title":"Description","text":"<p>The <code>NoiseFloat</code> transformer multiplies the original float value by randomly generated value that is not higher than the <code>max_ratio</code> parameter and not less that <code>max_ratio</code> parameter and adds it to or subtracts it from the original value. Additionally, you can specify the number of decimal digits by using the <code>decimal</code> parameter.</p> <p>In case you have constraints on the float range, you can set the <code>min</code> and <code>max</code> parameters to specify the threshold values. The values for <code>min</code> and <code>max</code> must have the same format as the <code>column</code> parameter. Parameters min and max support dynamic mode. Engine parameter allows you to choose between random and hash engines for generating values. Read more about the engines</p> <p>Info</p> <p>If the noised value exceeds the <code>max</code> threshold, the transformer will set the value to <code>max</code>. If the noised value is lower than the <code>min</code> threshold, the transformer will set the value to <code>min</code>.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/noise_float/#example-adding-noise-to-the-purchase-price","title":"Example: Adding noise to the purchase price","text":"<p>In this example, the original value of <code>standardprice</code> will be noised up to <code>50%</code> and rounded up to <code>2</code> decimals.</p> NoiseFloat transformer example<pre><code>- schema: \"purchasing\"\n  name: \"productvendor\"\n  columns_type_override: # (1)\n    lastreceiptcost: \"float8\"\n    standardprice: \"float8\"\n  transformers:\n    - name: \"NoiseFloat\"\n      params:\n        column: \"lastreceiptcost\"\n        max_ratio: 0.15\n        decimal: 2\n      dynamic_params:\n        min:\n          column: \"standardprice\"\n</code></pre> <ol> <li>The type overrides applied for example because the playground database does not contain any tables with float    columns.</li> </ol> <p>Result</p> ColumnOriginalValueTransformedValue lastreceiptcost50.263547.87"},{"location":"built_in_transformers/standard_transformers/noise_int/","title":"NoiseInt","text":"<p>Add or subtract a random fraction to the original integer value.</p>"},{"location":"built_in_transformers/standard_transformers/noise_int/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes int2, int4, int8 min_ratio The minimum random percentage for noise, from <code>0</code> to <code>1</code>, e. g. <code>0.1</code> means \"add noise up to 10%\" <code>0.05</code> No - max_ratio The maximum random percentage for noise, from <code>0</code> to <code>1</code>, e. g. <code>0.1</code> means \"add noise up to 10%\" Yes - min Min threshold of noised value No - max Min threshold of noised value No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/noise_int/#dynamic-parameters","title":"Dynamic parameters","text":"Parameter Supported types min int2, int4, int8 max int2, int4, int8"},{"location":"built_in_transformers/standard_transformers/noise_int/#description","title":"Description","text":"<p>The <code>NoiseInt</code> transformer multiplies the original integer value by randomly generated value that is not higher than the <code>max_ratio</code> parameter and not less that <code>max_ratio</code> parameter and adds it to or subtracts it from the original value.</p> <p>In case you have constraints on the integer range, you can set the <code>min</code> and <code>max</code> parameters to specify the threshold values. The values for <code>min</code> and <code>max</code> must have the same format as the <code>column</code> parameter. Parameters min and max support dynamic mode.</p> <p>Info</p> <p>If the noised value exceeds the <code>max</code> threshold, the transformer will set the value to <code>max</code>. If the noised value is lower than the <code>min</code> threshold, the transformer will set the value to <code>min</code>.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/noise_int/#example-noise-vacation-hours-of-an-employee","title":"Example: Noise vacation hours of an employee","text":"<p>In the following example, the original value of <code>vacationhours</code> will be noised up to 40%. The transformer will set the value to <code>10</code> if the noised value is lower than <code>10</code> and to <code>1000</code> if the noised value exceeds <code>1000</code>.</p> NoiseInt transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"NoiseInt\"\n      params:\n        column: \"vacationhours\"\n        max_ratio: 0.4\n        min: 10\n        max: 1000\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue vacationhours9969"},{"location":"built_in_transformers/standard_transformers/noise_numeric/","title":"NoiseNumeric","text":"<p>Add or subtract a random fraction to the original numeric value.</p>"},{"location":"built_in_transformers/standard_transformers/noise_numeric/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes numeric, decimal decimal The decimal of the noised float value (number of digits after the decimal point) <code>4</code> No - min_ratio The minimum random percentage for noise, from <code>0</code> to <code>1</code>, e. g. <code>0.1</code> means \"add noise up to 10%\" <code>0.05</code> No - max_ratio The maximum random percentage for noise, from <code>0</code> to <code>1</code>, e. g. <code>0.1</code> means \"add noise up to 10%\" Yes - min Min threshold of noised value No - max Max threshold of noised value No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/noise_numeric/#dynamic-parameters","title":"Dynamic parameters","text":"Parameter Supported types min numeric, decimal, float4, float8, int2, int4, int8 max numeric, decimal, float4, float8, int2, int4, int8"},{"location":"built_in_transformers/standard_transformers/noise_numeric/#description","title":"Description","text":"<p>The <code>NoiseNumeric</code> transformer multiplies the original numeric (or decimal) value by randomly generated value that is not higher than the <code>max_ratio</code> parameter and not less that <code>max_ratio</code> parameter and adds it to or subtracts it from the original value. Additionally, you can specify the number of decimal digits by using the <code>decimal</code> parameter.</p> <p>In case you have constraints on the numeric range, you can set the <code>min</code> and <code>max</code> parameters to specify the threshold values. The values for <code>min</code> and <code>max</code> must have the same format as the <code>column</code> parameter. Parameters min and max support dynamic mode. Engine parameter allows you to choose between random and hash engines for generating values. Read more about the engines</p> <p>Info</p> <p>If the noised value exceeds the <code>max</code> threshold, the transformer will set the value to <code>max</code>. If the noised value is lower than the <code>min</code> threshold, the transformer will set the value to <code>min</code>.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p> <p>Warning</p> <p>Greenmask cannot parse the <code>numeric</code> type sitteng. For instance <code>NUMERIC(10, 2)</code>. You should set <code>min</code> and <code>max</code> treshholds manually as well as allowed <code>decimal</code>. This behaviour will be changed in the later versions. Grenmask will be able to determine the decimal and scale of the column and set the min and max treshholds automatically if were not set.</p>"},{"location":"built_in_transformers/standard_transformers/noise_numeric/#example-adding-noise-to-the-purchase-price","title":"Example: Adding noise to the purchase price","text":"<p>In this example, the original value of <code>standardprice</code> will be noised up to <code>50%</code> and rounded up to <code>2</code> decimals.</p> NoiseNumeric transformer example<pre><code>- schema: \"purchasing\"\n  name: \"productvendor\"\n  transformers:\n    - name: \"NoiseNumeric\"\n      params:\n        column: \"lastreceiptcost\"\n        max_ratio: 0.15\n        decimal: 2\n        max: 10000\n      dynamic_params:\n        min:\n          column: \"standardprice\"\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue lastreceiptcost50.263557.33"},{"location":"built_in_transformers/standard_transformers/random_amount_with_currency/","title":"RandomAmountWithCurrency","text":"<p>The <code>RandomAmountWithCurrency</code> transformer is specifically designed to populate specified database columns with random financial amounts accompanied by currency codes. Ideal for applications requiring the simulation of financial transactions, this utility enhances the realism of financial datasets by introducing variability in amounts and currencies.</p>"},{"location":"built_in_transformers/standard_transformers/random_amount_with_currency/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_amount_with_currency/#description","title":"Description","text":"<p>This transformer automatically generates random financial amounts along with corresponding global currency codes (e. g., <code>250.00 USD</code>, <code>300.00 EUR</code>), injecting them into the designated database column. It provides a straightforward solution for populating financial records with varied and realistic data, suitable for testing payment systems, data anonymization, and simulation of economic models.</p>"},{"location":"built_in_transformers/standard_transformers/random_amount_with_currency/#example-populate-the-payments-table-with-random-amounts-and-currencies","title":"Example: Populate the <code>payments</code> table with random amounts and currencies","text":"<p>This example shows how to configure the <code>RandomAmountWithCurrency</code> transformer to populate the <code>payment_details</code> column in the <code>payments</code> table with random amounts and currencies. It is an effective approach to simulating a diverse range of payment transactions.</p> RandomAmountWithCurrency transformer example<pre><code>- schema: \"public\"\n  name: \"payments\"\n  transformers:\n    - name: \"RandomAmountWithCurrency\"\n      params:\n        column: \"payment_details\"\n        keep_null: false\n</code></pre> <p>In this setup, the <code>payment_details</code> column will be updated with random financial amounts and currency codes for each entry, replacing any existing non-NULL values. The <code>keep_null</code> parameter, when set to <code>true</code>, ensures that existing NULL values in the column remain unchanged, preserving the integrity of records without specified payment details.</p>"},{"location":"built_in_transformers/standard_transformers/random_bool/","title":"RandomBool","text":"<p>Generate random boolean values.</p>"},{"location":"built_in_transformers/standard_transformers/random_bool/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes bool keep_null Indicates whether NULL values should be replaced with transformed values or not <code>true</code> No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_bool/#description","title":"Description","text":"<p>The <code>RandomBool</code> transformer generates a random boolean value. The behaviour for NULL values can be configured using the <code>keep_null</code> parameter. The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_bool/#example-generate-a-random-boolean-for-a-column","title":"Example: Generate a random boolean for a column","text":"<p>In the following example, the <code>RandomBool</code> transformer generates a random boolean value for the <code>salariedflag</code> column.</p> RandomBool transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"RandomBool\"\n      params:\n        column: \"salariedflag\"\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue salariedflagtf"},{"location":"built_in_transformers/standard_transformers/random_cc_number/","title":"RandomCCNumber","text":"<p>The <code>RandomCCNumber</code> transformer is specifically designed to populate specified database columns with random credit card numbers. This utility is crucial for applications that involve simulating financial data, testing payment systems, or anonymizing real credit card numbers in datasets.</p>"},{"location":"built_in_transformers/standard_transformers/random_cc_number/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_cc_number/#description","title":"Description","text":"<p>By leveraging algorithms capable of generating plausible credit card numbers that adhere to standard credit card validation rules (such as the Luhn algorithm), the <code>RandomCCNumber</code> transformer injects random credit card numbers into the designated database column. This approach ensures the generation of credit card numbers that are realistic for testing and development purposes, without compromising real-world applicability and security.</p>"},{"location":"built_in_transformers/standard_transformers/random_cc_number/#example-populate-random-credit-card-numbers-for-the-payment_information-table","title":"Example: Populate random credit card numbers for the <code>payment_information</code> table","text":"<p>This example demonstrates configuring the <code>RandomCCNumber</code> transformer to populate the <code>cc_number</code> column in the <code>payment_information</code> table with random credit card numbers. It is an effective strategy for creating a realistic set of payment data for application testing or data anonymization.</p> RandomCCNumber transformer example<pre><code>- schema: \"public\"\n  name: \"payment_information\"\n  transformers:\n    - name: \"RandomCCNumber\"\n      params:\n        column: \"cc_number\"\n        keep_null: false\n</code></pre> <p>With this setup, the <code>cc_number</code> column will be updated with random credit card numbers for each entry, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, it will ensure that existing NULL values in the column are preserved, maintaining the integrity of records where credit card information is not applicable or available.</p>"},{"location":"built_in_transformers/standard_transformers/random_cc_type/","title":"RandomCCType","text":"<p>The <code>RandomCCType</code> transformer is designed to populate specified database columns with random credit card types. This tool is essential for applications that require the simulation of financial transaction data, testing payment processing systems, or anonymizing credit card type information in datasets.</p>"},{"location":"built_in_transformers/standard_transformers/random_cc_type/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_cc_type/#description","title":"Description","text":"<p>Utilizing a predefined list of credit card types (e.g., VISA, MasterCard, American Express, Discover), the <code>RandomCCType</code> transformer injects random credit card type names into the designated database column. This feature allows for the creation of realistic and varied financial transaction datasets by simulating a range of credit card types without using real card data.</p>"},{"location":"built_in_transformers/standard_transformers/random_cc_type/#example-populate-random-credit-card-types-for-the-transactions-table","title":"Example: Populate random credit card types for the <code>transactions</code> table","text":"<p>This example shows how to configure the <code>RandomCCType</code> transformer to populate the <code>card_type</code> column in the <code>transactions</code> table with random credit card types. It is a straightforward method for simulating diverse payment methods across transactions.</p> RandomCCType transformer example<pre><code>- schema: \"public\"\n  name: \"transactions\"\n  transformers:\n    - name: \"RandomCCType\"\n      params:\n        column: \"card_type\"\n        keep_null: false\n</code></pre> <p>In this configuration, the <code>card_type</code> column will be updated with random credit card types for each entry, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, existing NULL values in the column will be preserved, maintaining the integrity of records where card type information is not applicable.</p>"},{"location":"built_in_transformers/standard_transformers/random_century/","title":"RandomCentury","text":"<p>The <code>RandomCentury</code> transformer is crafted to populate specified database columns with random century values. It is ideal for applications that require historical data simulation, such as generating random years within specific centuries for historical databases, testing datasets with temporal dimensions, or anonymizing dates in historical research data.</p>"},{"location":"built_in_transformers/standard_transformers/random_century/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_century/#description","title":"Description","text":"<p>The <code>RandomCentury</code> transformer utilizes an algorithm or a library function (hypothetical in this context) to generate random century values. Each value represents a century (e.g., <code>19th</code>, <code>20th</code>, <code>21st</code>), providing a broad temporal range that can be used to enhance datasets requiring a distribution across different historical periods without the need for precise date information.</p>"},{"location":"built_in_transformers/standard_transformers/random_century/#example-populate-random-centuries-for-the-historical_artifacts-table","title":"Example: Populate random centuries for the <code>historical_artifacts</code> table","text":"<p>This example shows how to configure the <code>RandomCentury</code> transformer to populate the <code>century</code> column in a <code>historical_artifacts</code> table with random century values, adding an element of variability and historical context to the dataset.</p> RandomCentury transformer example<pre><code>- schema: \"public\"\n  name: \"historical_artifacts\"\n  transformers:\n    - name: \"RandomCentury\"\n      params:\n        column: \"century\"\n        keep_null: false\n</code></pre> <p>In this setup, the <code>century</code> column will be filled with random century values, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, then existing NULL values in the column will remain untouched, preserving the original dataset's integrity where no temporal data is available.</p>"},{"location":"built_in_transformers/standard_transformers/random_choice/","title":"RandomChoice","text":"<p>Replace values randomly chosen from a provided list.</p>"},{"location":"built_in_transformers/standard_transformers/random_choice/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes any values A list of values in any format. The string with value <code>\\N</code> is considered NULL. Yes - validate Performs a decoding procedure via the PostgreSQL driver using the column type to ensure that values have correct type <code>true</code> No keep_null Indicates whether NULL values should be replaced with transformed values or not <code>true</code> No engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_choice/#description","title":"Description","text":"<p>The <code>RandomChoice</code> transformer replaces one randomly chosen value from the list provided in the <code>values</code> parameter. You can use the <code>validate</code> parameter to ensure that values are correct before applying the transformation. The behaviour for NULL values can be configured using the <code>keep_null</code> parameter.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_choice/#example-choosing-randomly-from-provided-dates","title":"Example: Choosing randomly from provided dates","text":"<p>In this example, the provided values undergo validation through PostgreSQL driver decoding, and one value is randomly chosen from the list.</p> RandomChoice transformer example<pre><code>- schema: \"humanresources\"\n  name: \"jobcandidate\"\n  transformers:\n    - name: \"RandomChoice\"\n      params:\n        column: \"modifieddate\"\n        validate: true\n        engine: hash\n        values:\n          - \"2023-12-21 07:41:06.891\"\n          - \"2023-12-21 07:41:06.896\"\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue modifieddate2007-06-23 00:00:002023-12-21 07:41:06.891"},{"location":"built_in_transformers/standard_transformers/random_currency/","title":"RandomCurrency","text":"<p>The <code>RandomCurrency</code> transformer is tailored to populate specified database columns with random currency codes. This tool is highly beneficial for applications involving the simulation of international financial data, testing currency conversion features, or anonymizing currency information in datasets.</p>"},{"location":"built_in_transformers/standard_transformers/random_currency/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_currency/#description","title":"Description","text":"<p>Utilizing a comprehensive list of global currency codes (e.g., USD, EUR, JPY), the <code>RandomCurrency</code> transformer injects random currency codes into the designated database column. This feature allows for the creation of diverse and realistic financial transaction datasets by simulating a variety of currencies without relying on actual financial data.</p>"},{"location":"built_in_transformers/standard_transformers/random_currency/#example-populate-random-currency-codes-for-the-transactions-table","title":"Example: Populate random currency codes for the <code>transactions</code> table","text":"<p>This example outlines configuring the <code>RandomCurrency</code> transformer to populate the <code>currency_code</code> column in a <code>transactions</code> table with random currency codes. It is an effective way to simulate international transactions across multiple currencies.</p> RandomCurrency transformer example<pre><code>- schema: \"public\"\n  name: \"transactions\"\n  transformers:\n    - name: \"RandomCurrency\"\n      params:\n        column: \"currency_code\"\n        keep_null: false\n</code></pre> <p>In this configuration, the <code>currency_code</code> column will be updated with random currency codes for each entry, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, existing NULL values in the column will be preserved, ensuring the integrity of records where currency data may not be applicable.</p>"},{"location":"built_in_transformers/standard_transformers/random_date/","title":"RandomDate","text":"<p>Generate a random date in a specified interval.</p>"},{"location":"built_in_transformers/standard_transformers/random_date/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column Name of the column to be affected Yes date, timestamp, timestamptz min The minimum threshold date for the random value. The format depends on the column type. Yes - max The maximum threshold date for the random value. The format depends on the column type. Yes - truncate Truncate the date to the specified part (<code>nanosecond</code>, <code>microsecond</code>, <code>millisecond</code>, <code>second</code>, <code>minute</code>, <code>hour</code>, <code>day</code>, <code>month</code>, <code>year</code>). The truncate operation is not applied by default. No - keep_null Indicates whether NULL values should be replaced with transformed values or not <code>true</code> No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_date/#dynamic-parameters","title":"Dynamic parameters","text":"Parameter Supported types min date, timestamp, timestamptz max date, timestamp, timestamptz"},{"location":"built_in_transformers/standard_transformers/random_date/#description","title":"Description","text":"<p>The <code>RandomDate</code> transformer generates a random date within the provided interval, starting from <code>min</code> to <code>max</code>. It can also perform date truncation up to the specified part of the date. The format of dates in the <code>min</code> and <code>max</code> parameters must adhere to PostgreSQL types, including <code>DATE</code>, <code>TIMESTAMP WITHOUT TIMEZONE</code>, or <code>TIMESTAMP WITH TIMEZONE</code>.</p> <p>Note</p> <p>The value of <code>min</code> and <code>max</code> parameters depends on the column type. For example, for the <code>date</code> column, the value  should be in the format <code>YYYY-MM-DD</code>, while for the <code>timestamp</code> column, the value should be in the format <code>YYYY-MM-DD HH:MM:SS</code> or <code>YYYY-MM-DD HH:MM:SS.SSSSSS</code>. The <code>timestamptz</code> column requires the value to be in the format <code>YYYY-MM-DD HH:MM:SS.SSSSSS+HH:MM</code>. Read more about date/time formats in  the PostgreSQL documentation.</p> <p>The behaviour for <code>NULL</code> values can be configured using the <code>keep_null</code> parameter. The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_date/#example-generate-modifieddate","title":"Example: Generate <code>modifieddate</code>","text":"<p>In the following example, a random timestamp without timezone is generated for the <code>modifieddate</code> column within the range from <code>2011-05-31 00:00:00</code> to <code>2013-05-31 00:00:00</code>, and the part of the random value after <code>day</code> is truncated.</p> RandomDate transformer example<pre><code>- schema: \"sales\"\n  name: \"salesorderdetail\"\n  transformers:\n    - name: \"RandomDate\"\n      params:\n        column: \"modifieddate\"\n        keep_null: false\n        min: \"2011-05-31 00:00:00\"\n        max: \"2013-05-31 00:00:00\"\n        truncate: \"day\"\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue modifieddate2014-06-30 00:00:002012-07-27 00:00:00"},{"location":"built_in_transformers/standard_transformers/random_date/#example-generate-hiredate-based-on-birthdate-using-two-transformations","title":"Example: Generate <code>hiredate</code> based on <code>birthdate</code> using two transformations","text":"<p>In this example, the <code>RandomDate</code> transformer generates a random date for the <code>birthdate</code> column within the range <code>now - 50 years</code> to <code>now - 18 years</code>. The hire date is generated based on the <code>birthdate</code>, ensuring that the employee is at least 18 years old when hired.</p> <pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"RandomDate\"\n      params:\n        column: \"birthdate\"\n        min: '{{ now | tsModify \"-50 years\" | .EncodeValue }}' # 1994\n        max: '{{ now | tsModify \"-18 years\" | .EncodeValue }}' # 2006\n\n    - name: \"RandomDate\"\n      params:\n        column: \"hiredate\"\n        truncate: \"month\"\n        max: \"{{ now | .EncodeValue }}\"\n      dynamic_params:\n        min:\n          column: \"birthdate\"\n          template: '{{ .GetValue | tsModify \"18 years\" | .EncodeValue }}' # min age 18 years\n</code></pre> <p>Result:</p> ColumnOriginalValueTransformedValue birthdate1969-01-291985-10-29 hiredate2009-01-142023-01-01"},{"location":"built_in_transformers/standard_transformers/random_day_of_month/","title":"RandomDayOfMonth","text":"<p>The <code>RandomDayOfMonth</code> transformer is designed to populate specified database columns with random day-of-the-month values. It is particularly useful for scenarios requiring the simulation of dates, such as generating random event dates, user sign-up dates, or any situation where the specific day of the month is needed without reference to the actual month or year.</p>"},{"location":"built_in_transformers/standard_transformers/random_day_of_month/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar, int2, int4, int8, numeric keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_day_of_month/#description","title":"Description","text":"<p>Utilizing the <code>faker</code> library, the <code>RandomDayOfMonth</code> transformer generates random numerical values representing days of the month, ranging from 1 to 31. This allows for the easy insertion of random but plausible day-of-the-month data into a database, enhancing realism or anonymizing actual dates.</p>"},{"location":"built_in_transformers/standard_transformers/random_day_of_month/#example-populate-random-days-of-the-month-for-the-events-table","title":"Example: Populate random days of the month for the <code>events</code> table","text":"<p>This example illustrates how to configure the <code>RandomDayOfMonth</code> transformer to fill the <code>event_day</code> column in the <code>events</code> table with random day-of-the-month values, facilitating the simulation of varied event scheduling.</p> RandomDayOfMonth transformer example<pre><code>- schema: \"public\"\n  name: \"events\"\n  transformers:\n    - name: \"RandomDayOfMonth\"\n      params:\n        column: \"event_day\"\n        keep_null: false\n</code></pre> <p>With this setup, the <code>event_day</code> column will be updated with random day-of-the-month values, replacing any existing non-NULL values. Setting <code>keep_null</code> to <code>true</code> ensures that NULL values in the column are left unchanged, maintaining any existing gaps in the data.</p>"},{"location":"built_in_transformers/standard_transformers/random_day_of_week/","title":"RandomDayOfWeek","text":"<p>The <code>RandomDayOfWeek</code> transformer is specifically designed to fill specified database columns with random day-of-the-week names. It is particularly useful for applications that require simulated weekly schedules, random event planning, or any scenario where the day of the week is relevant but the specific date is not.</p>"},{"location":"built_in_transformers/standard_transformers/random_day_of_week/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_day_of_week/#description","title":"Description","text":"<p>Utilizing the <code>faker</code> library, the <code>RandomDayOfWeek</code> transformer generates names of days (e. g., Monday, Tuesday) at random. This transformer can be applied to any text or varchar column in a database, introducing variability and realism into data sets that need to represent days of the week in a non-specific manner.</p>"},{"location":"built_in_transformers/standard_transformers/random_day_of_week/#example-populate-random-days-of-the-week-for-the-work_schedule-table","title":"Example: Populate random days of the week for the <code>work_schedule</code> table","text":"<p>This example demonstrates configuring the <code>RandomDayOfWeek</code> transformer to populate the <code>work_day</code> column in the <code>work_schedule</code> table with random days of the week. This setup can help simulate a diverse range of work schedules without tying them to specific dates.</p> RandomDayOfWeek transformer example<pre><code>- schema: \"public\"\n  name: \"work_schedule\"\n  transformers:\n    - name: \"RandomDayOfWeek\"\n      params:\n        column: \"work_day\"\n        keep_null: false\n</code></pre> <p>In this configuration, every entry in the <code>work_day</code> column will be updated with a random day of the week, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, then existing NULL values within the column will remain unchanged.</p>"},{"location":"built_in_transformers/standard_transformers/random_domain_name/","title":"RandomDomainName","text":"<p>The <code>RandomDomainName</code> transformer is designed to populate specified database columns with random domain names. This tool is invaluable for simulating web data, testing applications that interact with domain names, or anonymizing real domain information in datasets.</p>"},{"location":"built_in_transformers/standard_transformers/random_domain_name/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_domain_name/#description","title":"Description","text":"<p>By leveraging an algorithm or library capable of generating believable domain names, the <code>RandomDomainName</code> transformer introduces random domain names into the specified database column. Each generated domain name includes a second-level domain (SLD) and a top-level domain (TLD), such as \"example.com\" or \"website.org,\" providing a wide range of plausible web addresses for database enrichment.</p>"},{"location":"built_in_transformers/standard_transformers/random_domain_name/#example-populate-random-domain-names-for-the-websites-table","title":"Example: Populate random domain names for the <code>websites</code> table","text":"<p>This example demonstrates configuring the <code>RandomDomainName</code> transformer to populate the <code>domain</code> column in the <code>websites</code> table with random domain names. This approach facilitates the creation of a diverse and realistic set of web addresses for testing, simulation, or data anonymization purposes.</p> RandomDomainName transformer example<pre><code>- schema: \"public\"\n  name: \"websites\"\n  transformers:\n    - name: \"RandomDomainName\"\n      params:\n        column: \"domain\"\n        keep_null: false\n</code></pre> <p>In this setup, the <code>domain</code> column will be updated with random domain names for each entry, replacing any existing non-NULL values. If <code>keep_null</code> is set to <code>true</code>, the transformer will preserve existing NULL values in the column, maintaining the integrity of data where domain information is not applicable.</p>"},{"location":"built_in_transformers/standard_transformers/random_e164_phone_number/","title":"RandomE164PhoneNumber","text":"<p>The <code>RandomE164PhoneNumber</code> transformer is developed to populate specified database columns with random E.164 phone numbers. This tool is essential for applications requiring the simulation of contact information, testing phone number validation systems, or anonymizing phone number data in datasets while focusing on E.164 numbers.</p>"},{"location":"built_in_transformers/standard_transformers/random_e164_phone_number/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_e164_phone_number/#description","title":"Description","text":"<p>The <code>RandomE164PhoneNumber</code> transformer utilizes algorithms capable of generating random E.164 phone numbers with the standard international format and injects them into the designated database column. This feature allows for the creation of diverse and realistic contact information in datasets for development, testing, or data anonymization purposes.</p>"},{"location":"built_in_transformers/standard_transformers/random_e164_phone_number/#example-populate-random-e164-phone-numbers-for-the-contact_information-table","title":"Example: Populate random E.164 phone numbers for the <code>contact_information</code> table","text":"<p>This example demonstrates configuring the <code>RandomE164PhoneNumber</code> transformer to populate the <code>phone_number</code> column in the <code>contact_information</code> table with random E.164 phone numbers. It is an effective method for simulating a variety of contact information entries with E.164 numbers.</p> RandomE164PhoneNumber transformer example<pre><code>- schema: \"public\"\n  name: \"contact_information\"\n  transformers:\n    - name: \"RandomE164PhoneNumber\"\n      params:\n        column: \"phone_number\"\n        keep_null: false\n</code></pre> <p>In this configuration, the <code>phone_number</code> column will be updated with random E.164 phone numbers for each contact information entry, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, existing NULL values in the column will be preserved, ensuring the integrity of records where E.164 phone number information is not applicable or provided.</p>"},{"location":"built_in_transformers/standard_transformers/random_email/","title":"RandomEmail","text":"<p>Generate email addresses for a specified column.</p>"},{"location":"built_in_transformers/standard_transformers/random_email/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_original_domain Keep original of the original address <code>false</code> No - local_part_template The template for local part of email No - domain_part_template The template for domain part of email No - domains List of domains for new email <code>[\"gmail.com\", \"yahoo.com\", \"outlook.com\", \"hotmail.com\", \"aol.com\", \"icloud.com\", \"mail.com\", \"zoho.com\", \"yandex.com\", \"protonmail.com\", \"gmx.com\", \"fastmail.com\"]</code> No - validate Validate generated email if using template <code>false</code> No - max_random_length Max length of randomly generated part of the email <code>32</code> No - keep_null Indicates whether NULL values should be preserved <code>false</code> No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_email/#description","title":"Description","text":"<p>The <code>RandomEmail</code> transformer generates random email addresses for the specified database column. By default, the transformer generates random email addresses with a maximum length of 32 characters. The <code>keep_original_domain</code> parameter allows you to preserve the original domain part of the email address. The <code>local_part_template</code> and <code>domain_part_template</code> parameters enable you to specify templates for the local and domain parts of the email address, respectively. If the <code>validate</code> parameter is set to <code>true</code>, the transformer will validate the generated email addresses against the specified templates. The <code>keep_null</code> parameter allows you to preserve existing NULL values in the column.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_email/#templates-parameters","title":"Templates parameters","text":"<p>In each template you have access to the columns of the table by using the <code>{{ .column_name }}</code> syntax. Note that all values are strings. For example, you can use for assembling the email address by accessing to <code>first_name</code> and <code>last_name</code> columns <code>{{ .first_name | lower }}.{{ .last_name | lower }}</code>.</p> <p>The transformer always generates random sequences for the email, and you can use it by accessing the <code>{{ .random_string }}</code> variable. For example, we can add random string in the end of local part <code>{{ .first_name | lower }}.{{ .last_name | lower }}.{{ .random_string }}</code>.</p> <p>Read more about template function Template functions.</p>"},{"location":"built_in_transformers/standard_transformers/random_email/#random-email-generation-using-first-name-and-last-name","title":"Random email generation using first name and last name","text":"<p>In this example, the <code>RandomEmail</code> transformer generates random email addresses for the <code>email</code> column in the <code>account</code> table. The transformer generates email addresses using the <code>first_name</code> and <code>last_name</code> columns as the local part of the email address and adds a random string to the end of the local part with length 10 characters. The original domain part of the email address is preserved.</p> <pre><code>CREATE TABLE account\n(\n    id         SERIAL PRIMARY KEY,\n    gender     VARCHAR(1) NOT NULL,\n    email      TEXT       NOT NULL NOT NULL UNIQUE,\n    first_name TEXT       NOT NULL,\n    last_name  TEXT       NOT NULL,\n    birth_date DATE,\n    created_at TIMESTAMP  NOT NULL DEFAULT NOW()\n);\n\nINSERT INTO account (first_name, gender, last_name, birth_date, email)\nVALUES ('John', 'M', 'Smith', '1980-01-01', 'john.smith@gmail.com');\n</code></pre> RandomEmail transformer example<pre><code>- schema: \"public\"\n  name: \"account\"\n  transformers:\n    - name: \"RandomEmail\"\n      params:\n        column: \"email\"\n        engine: \"hash\"\n        keep_original_domain: true\n        local_part_template: \"{{ first_name | lower }}.{{ last_name | lower }}.{{ .random_string | trunc 10 }}\"\n</code></pre> <p>Result:</p> ColumnOriginalValueTransformedValue emailjohn.smith@gmail.comjohn.smith.a075d99e2d@gmail.com"},{"location":"built_in_transformers/standard_transformers/random_email/#simple-random-email-generation","title":"Simple random email generation","text":"<p>In this example, the <code>RandomEmail</code> transformer generates random email addresses for the <code>email</code> column in the <code>account</code> table. The transformer generates random email addresses with a maximum length of 10 characters.</p> RandomEmail transformer example<pre><code>- schema: \"public\"\n  name: \"account\"\n  transformers:\n    - name: \"RandomEmail\"\n      params:\n        column: \"email\"\n        max_random_length: 10\n</code></pre> <p>Result:</p> ColumnOriginalValueTransformedValue emailjohn.smith@gmail.comjohn.smith.a075d99e2d@gmail.com"},{"location":"built_in_transformers/standard_transformers/random_float/","title":"RandomFloat","text":"<p>Generate a random float within the provided interval.</p>"},{"location":"built_in_transformers/standard_transformers/random_float/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes float4, float8 min The minimum threshold for the random value. The value range depends on the column type. Yes - max The maximum threshold for the random value. The value range depends on the column type. Yes - decimal The decimal of the random float value (number of digits after the decimal point) <code>4</code> No - keep_null Indicates whether NULL values should be replaced with transformed values or not <code>true</code> No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_float/#dynamic-parameters","title":"Dynamic parameters","text":"Parameter Supported types min float4, float8 max float4, float8"},{"location":"built_in_transformers/standard_transformers/random_float/#description","title":"Description","text":"<p>The <code>RandomFloat</code> transformer generates a random float value within the provided interval, starting from <code>min</code> to <code>max</code>, with the option to specify the number of decimal digits by using the <code>decimal</code> parameter. The behaviour for NULL values can be configured using the <code>keep_null</code> parameter.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_float/#example-generate-random-price","title":"Example: Generate random price","text":"<p>In this example, the <code>RandomFloat</code> transformer generates random prices in the range from <code>0.1</code> to <code>7000</code> while maintaining a decimal of up to 2 digits.</p> RandomFloat transformer example<pre><code>- schema: \"sales\"\n  name: \"salesorderdetail\"\n  columns_type_override:  # (1)\n    \"unitprice\": \"float8\"\n  transformers:\n    - name: \"RandomFloat\"\n      params:\n        column: \"unitprice\"\n        min: 0.1\n        max: 7000\n        decimal: 2\n</code></pre> <ol> <li>The type overrides applied for example because the playground database does not contain any tables with float    columns.</li> </ol> <p>Result:</p> ColumnOriginalValueTransformedValue unitprice2024.9944449.7"},{"location":"built_in_transformers/standard_transformers/random_int/","title":"RandomInt","text":"<p>Generate a random integer within the provided interval.</p>"},{"location":"built_in_transformers/standard_transformers/random_int/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes int2, int4, int8 min The minimum threshold for the random value Yes - max The maximum threshold for the random value Yes - keep_null Indicates whether NULL values should be replaced with transformed values or not <code>true</code> No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_int/#dynamic-parameters","title":"Dynamic parameters","text":"Parameter Supported types min int2, int4, int8 max int2, int4, int8"},{"location":"built_in_transformers/standard_transformers/random_int/#description","title":"Description","text":"<p>The <code>RandomInt</code> transformer generates a random integer within the specified <code>min</code> and <code>max</code> thresholds. The behaviour for NULL values can be configured using the <code>keep_null</code> parameter.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_int/#example-generate-random-item-quantity","title":"Example: Generate random item quantity","text":"<p>In the following example, the <code>RandomInt</code> transformer generates a random value in the range from <code>1</code> to <code>30</code> and assigns it to the <code>orderqty</code> column.</p> generate random orderqty in the range from 1 to 30<pre><code>- schema: \"sales\"\n  name: \"salesorderdetail\"\n  transformers:\n    - name: \"RandomInt\"\n      params:\n        column: \"orderqty\"\n        min: 1\n        max: 30\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue orderqty129"},{"location":"built_in_transformers/standard_transformers/random_int/#example-generate-random-sick-leave-hours-based-on-vacation-hours","title":"Example: Generate random sick leave hours based on vacation hours","text":"<p>In the following example, the <code>RandomInt</code> transformer generates a random value in the range from <code>1</code> to the value of the <code>vacationhours</code> column and assigns it to the <code>sickleavehours</code> column. This configuration allows for the simulation of sick leave hours based on the number of vacation hours.</p> RandomInt transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"RandomInt\"\n      params:\n        column: \"sickleavehours\"\n        max: 100\n      dynamic_params:\n        min:\n          column: \"vacationhours\"\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue sickleavehours6999"},{"location":"built_in_transformers/standard_transformers/random_ip/","title":"RandomIP","text":"<p>The <code>RandomIp</code> transformer is designed to populate specified database columns with random IP v4 or V6 addresses. This utility is essential for applications requiring the simulation of network data, testing systems that utilize IP addresses, or anonymizing real IP addresses in datasets.</p>"},{"location":"built_in_transformers/standard_transformers/random_ip/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar, inet subnet Subnet for generating random ip in V4 or V6 format Yes - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_ip/#dynamic-parameters","title":"Dynamic parameters","text":"Name Supported types subnet cidr, text, varchar"},{"location":"built_in_transformers/standard_transformers/random_ip/#description","title":"Description","text":"<p>Utilizing a robust algorithm or library for generating IP addresses, the <code>RandomIp</code> transformer injects random IPv4 or IPv6 addresses into the designated database column, depending on the provided subnet. The transformer automatically detects whether to generate an IPv4 or IPv6 address based on the subnet version specified.</p>"},{"location":"built_in_transformers/standard_transformers/random_ip/#example-generate-a-random-ipv4-address-for-a-1921681024-subnet","title":"Example: Generate a Random IPv4 Address for a 192.168.1.0/24 Subnet","text":"<p>This example demonstrates how to configure the RandomIp transformer to inject a random IPv4 address into the ip_address column for entries in the <code>192.168.1.0/24</code> subnet:</p> Create table ip_networks and insert data<pre><code>CREATE TABLE ip_networks\n(\n    id         SERIAL PRIMARY KEY,\n    ip_address INET,\n    network    CIDR\n);\n\nINSERT INTO ip_networks (ip_address, network)\nVALUES ('192.168.1.10', '192.168.1.0/24'),\n       ('10.0.0.5', '10.0.0.0/16'),\n       ('172.16.254.3', '172.16.0.0/12'),\n       ('192.168.100.14', '192.168.100.0/24'),\n       ('2001:0db8:85a3:0000:0000:8a2e:0370:7334', '2001:0db8:85a3::/64'); -- An IPv6 address and network\n</code></pre> RandomPerson transformer example<pre><code>- schema: public\n  name: ip_networks\n  transformers:\n    - name: \"RandomIp\"\n      params:\n        subnet: \"192.168.1.0/24\"\n        column: \"ip_address\"\n        engine: \"random\"\n</code></pre> <p>Result:</p> ColumnOriginalValueTransformedValue ip_address192.168.1.10192.168.1.28"},{"location":"built_in_transformers/standard_transformers/random_ip/#example-generate-a-random-ip-based-on-the-dynamic-subnet-parameter","title":"Example: Generate a Random IP Based on the Dynamic Subnet Parameter","text":"<p>This configuration illustrates how to use the RandomIp transformer dynamically, where it reads the subnet information from the network column of the database and generates a corresponding random IP address:</p> RandomPerson transformer example with dynamic mode<pre><code>- schema: public\n  name: ip_networks\n  transformers:\n    - name: \"RandomIp\"\n      params:\n        column: \"ip_address\"\n        engine: \"random\"\n      dynamic_params:\n        subnet:\n          column: \"network\"\n</code></pre> <p>Result:</p> ColumnOriginalValueTransformedValue ip_address192.168.1.10192.168.1.111"},{"location":"built_in_transformers/standard_transformers/random_latitude/","title":"RandomLatitude","text":"<p>The <code>RandomLatitude</code> transformer generates random latitude values for specified database columns. It is designed to support geographical data enhancements, particularly useful for applications requiring randomized but plausible geographical coordinates.</p>"},{"location":"built_in_transformers/standard_transformers/random_latitude/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes float4, float8, numeric keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_latitude/#description","title":"Description","text":"<p>The <code>RandomLatitude</code> transformer utilizes the <code>faker</code> library to produce random latitude values within the range of -90 to +90 degrees. This transformer can be applied to columns designated to store geographical latitude information, enhancing data sets with randomized latitude coordinates.</p>"},{"location":"built_in_transformers/standard_transformers/random_latitude/#example-populate-random-latitude-for-the-locations-table","title":"Example: Populate random latitude for the <code>locations</code> table","text":"<p>This example demonstrates configuring the <code>RandomLatitude</code> transformer to populate the <code>latitude</code> column in the <code>locations</code> table with random latitude values.</p> RandomLatitude transformer example<pre><code>- schema: \"public\"\n  name: \"locations\"\n  transformers:\n    - name: \"RandomLatitude\"\n      params:\n        column: \"latitude\"\n        keep_null: false\n</code></pre> <p>With this configuration, the <code>latitude</code> column will be filled with random latitude values, replacing any existing non-NULL values. If <code>keep_null</code> is set to <code>true</code>, existing NULL values will be preserved.</p>"},{"location":"built_in_transformers/standard_transformers/random_longitude/","title":"RandomLongitude","text":"<p>The <code>RandomLongitude</code> transformer is designed to generate random longitude values for specified database columns, enhancing datasets with realistic geographic coordinates suitable for a wide range of applications, from testing location-based services to anonymizing real geographic data.</p>"},{"location":"built_in_transformers/standard_transformers/random_longitude/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes float4, float8, numeric keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_longitude/#description","title":"Description","text":"<p>The <code>RandomLongitude</code> transformer leverages the <code>faker</code> library to produce random longitude values within the globally accepted range of -180 to +180 degrees. This flexibility allows the transformer to be applied to any column intended for storing longitude data, providing a simple yet powerful tool for introducing randomized longitude coordinates into a database.</p>"},{"location":"built_in_transformers/standard_transformers/random_longitude/#example-populate-random-longitude-for-the-locations-table","title":"Example: Populate random longitude for the <code>locations</code> table","text":"<p>This example shows how to use the <code>RandomLongitude</code> transformer to fill the <code>longitude</code> column in the <code>locations</code> table with random longitude values.</p> RandomLongitude transformer example<pre><code>- schema: \"public\"\n  name: \"locations\"\n  transformers:\n    - name: \"RandomLongitude\"\n      params:\n        column: \"longitude\"\n        keep_null: false\n</code></pre> <p>This setup ensures that all entries in the <code>longitude</code> column receive a random longitude value, replacing any existing non-NULL values. If <code>keep_null</code> is set to <code>true</code>, then existing NULL values in the column will remain unchanged.</p>"},{"location":"built_in_transformers/standard_transformers/random_mac/","title":"RandomMac","text":"<p>The <code>RandomMac</code> transformer is designed to populate specified database columns with random MAC addresses.</p>"},{"location":"built_in_transformers/standard_transformers/random_mac/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar, macaddr keep_original_vendor Should the Individual/Group (I/G) and Universal/Local (U/L) bits be preserved from the original MAC address. <code>false</code> No - cast_type Param which allow to set Individual/Group (I/G) bit in MAC Address. Allowed values [any, individual, group]. If this value is <code>individual</code>, the address is meant for a single device (unicast). If it is <code>group</code>, the address is for a group of devices, which can include multicast and broadcast addresses. any No management_type Param which allow to set Universal/Local (U/L) bit in MAC Address. Allowed values [any, universal, local]. If this bit is <code>universal</code>, the address is universally administered (globally unique). If it is <code>local</code>, the address is locally administered (such as when set manually or programmatically on a network device). any No engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_mac/#description","title":"Description","text":"<p>The <code>RandomMac</code> transformer generates a random MAC address and injects it into the specified database column. The transformer can be configured to preserve the Individual/Group (I/G) and Universal/Local (U/L) bits from the original MAC address. You can also keep the original vendor bits in the generated MAC address by setting the <code>keep_original_vendor</code> parameter to <code>true</code>.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_mac/#example-generate-a-random-mac-address","title":"Example: Generate a Random MAC Address","text":"<p>This example demonstrates how to configure the RandomMac transformer to inject a random MAC address into the mac_address column:</p> Create table mac_addresses and insert data<pre><code>CREATE TABLE mac_addresses\n(\n    id          SERIAL PRIMARY KEY,\n    device_name VARCHAR(50),\n    mac_address MACADDR,\n    description TEXT\n);\n\nINSERT INTO mac_addresses (device_name, mac_address, description)\nVALUES ('Device A', '00:1A:2B:3C:4D:5E', 'Description for Device A'),\n       ('Device B', '01:2B:3C:4D:5E:6F', 'Description for Device B'),\n       ('Device C', '02:3C:4D:5E:6F:70', 'Description for Device C'),\n       ('Device D', '03:4D:5E:6F:70:71', 'Description for Device D'),\n       ('Device E', '04:5E:6F:70:71:72', 'Description for Device E');\n</code></pre> RandomPerson transformer example<pre><code>- schema: public\n  name: mac_addresses\n  transformers:\n    - name: \"RandomMac\"\n      params:\n        column: \"mac_address\"\n        engine: \"random\"\n        cast_type: \"any\"\n        management_type: \"any\"\n</code></pre> <p>Result:</p> ColumnOriginalValueTransformedValue mac_address00:1a:2b:3c:4d:5eac:7f:a8:11:4e:0d"},{"location":"built_in_transformers/standard_transformers/random_month_name/","title":"RandomMonthName","text":"<p>The <code>RandomMonthName</code> transformer is crafted to populate specified database columns with random month names. This transformer is especially useful for scenarios requiring the simulation of time-related data, such as user birth months or event months, without relying on specific date values.</p>"},{"location":"built_in_transformers/standard_transformers/random_month_name/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_month_name/#description","title":"Description","text":"<p>The <code>RandomMonthName</code> transformer utilizes the <code>faker</code> library to generate the names of months at random. It can be applied to any textual column in a database to introduce variety and realism into data sets that require representations of months without the need for specific calendar dates.</p>"},{"location":"built_in_transformers/standard_transformers/random_month_name/#example-populate-random-month-names-for-the-user_profiles-table","title":"Example: Populate random month names for the <code>user_profiles</code> table","text":"<p>This example demonstrates how to configure the <code>RandomMonthName</code> transformer to fill the <code>birth_month</code> column in the <code>user_profiles</code> table with random month names, adding a layer of diversity to user data without using actual birthdates.</p> RandomMonthName transformer example<pre><code>- schema: \"public\"\n  name: \"user_profiles\"\n  transformers:\n    - name: \"RandomMonthName\"\n      params:\n        column: \"birth_month\"\n        keep_null: false\n</code></pre> <p>With this setup, the <code>birth_month</code> column will be updated with random month names, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, then existing NULL values within the column will remain untouched.</p>"},{"location":"built_in_transformers/standard_transformers/random_numeric/","title":"RandomNumeric","text":"<p>Generate a random numeric within the provided interval.</p>"},{"location":"built_in_transformers/standard_transformers/random_numeric/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes numeric, decimal min The minimum threshold for the random value. The value range depends on the column type. Yes - max The maximum threshold for the random value. The value range depends on the column type. Yes - decimal The decimal of the random numeric value (number of digits after the decimal point) <code>4</code> No - keep_null Indicates whether NULL values should be replaced with transformed values or not <code>true</code> No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_numeric/#dynamic-parameters","title":"Dynamic parameters","text":"Parameter Supported types min int2, int4, int8, float4, float8, numeric, decimal max int2, int4, int8, float4, float8, numeric, decimal"},{"location":"built_in_transformers/standard_transformers/random_numeric/#description","title":"Description","text":"<p>The <code>RandomNumeric</code> transformer generates a random numeric value within the provided interval, starting from <code>min</code> to <code>max</code>, with the option to specify the number of decimal digits by using the <code>decimal</code> parameter. The behaviour for NULL values can be configured using the <code>keep_null</code> parameter.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_numeric/#example-generate-random-price","title":"Example: Generate random price","text":"<p>In this example, the <code>RandomNumeric</code> transformer generates random prices in the range from <code>0.1</code> to <code>7000</code> while maintaining a decimal of up to 2 digits.</p> RandomNumeric transformer example<pre><code>- schema: \"sales\"\n  name: \"salesorderdetail\"\n  transformers:\n    - name: \"RandomNumeric\"\n      params:\n        column: \"unitprice\"\n        min: 0.1\n        max: 7000\n        decimal: 2\n</code></pre> <ol> <li>The type overrides applied for example because the playground database does not contain any tables with numeric    columns.</li> </ol> <p>Result:</p> ColumnOriginalValueTransformedValue unitprice2024.9944449.7"},{"location":"built_in_transformers/standard_transformers/random_paragraph/","title":"RandomParagraph","text":"<p>The <code>RandomParagraph</code> transformer is crafted to populate specified database columns with random paragraphs. This utility is indispensable for applications that require the generation of extensive textual content, such as simulating articles, enhancing textual datasets for NLP systems, or anonymizing textual content in databases.</p>"},{"location":"built_in_transformers/standard_transformers/random_paragraph/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_paragraph/#description","title":"Description","text":"<p>Employing sophisticated text generation algorithms or libraries, the <code>RandomParagraph</code> transformer generates random paragraphs, injecting them into the designated database column. This transformer is designed to create varied and plausible paragraphs that simulate real-world textual content, providing a valuable tool for database enrichment, testing, and anonymization.</p>"},{"location":"built_in_transformers/standard_transformers/random_paragraph/#example-populate-random-paragraphs-for-the-articles-table","title":"Example: Populate random paragraphs for the <code>articles</code> table","text":"<p>This example illustrates configuring the <code>RandomParagraph</code> transformer to populate the <code>body</code> column in an <code>articles</code> table with random paragraphs. It is an effective way to simulate diverse article content for development, testing, or demonstration purposes.</p> RandomParagraph transformer example<pre><code>- schema: \"public\"\n  name: \"articles\"\n  transformers:\n    - name: \"RandomParagraph\"\n      params:\n        column: \"body\"\n        keep_null: false\n</code></pre> <p>With this setup, the <code>body</code> column will receive random paragraphs for each entry, replacing any existing non-NULL values. Setting the <code>keep_null</code> parameter to <code>true</code> allows for the preservation of existing NULL values within the column, maintaining the integrity of records where article content is not applicable or provided.</p>"},{"location":"built_in_transformers/standard_transformers/random_password/","title":"RandomPassword","text":"<p>The <code>RandomPassword</code> transformer is designed to populate specified database columns with random passwords. This utility is vital for applications that require the simulation of secure user data, testing systems with authentication mechanisms, or anonymizing real passwords in datasets.</p>"},{"location":"built_in_transformers/standard_transformers/random_password/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_password/#description","title":"Description","text":"<p>Employing sophisticated password generation algorithms or libraries, the <code>RandomPassword</code> transformer injects random passwords into the designated database column. This feature is particularly useful for creating realistic and secure user password datasets for development, testing, or demonstration purposes.</p>"},{"location":"built_in_transformers/standard_transformers/random_password/#example-populate-random-passwords-for-the-user_accounts-table","title":"Example: Populate random passwords for the <code>user_accounts</code> table","text":"<p>This example demonstrates how to configure the <code>RandomPassword</code> transformer to populate the <code>password</code> column in the <code>user_accounts</code> table with random passwords.</p> RandomPassword transformer example<pre><code>- schema: \"public\"\n  name: \"user_accounts\"\n  transformers:\n    - name: \"RandomPassword\"\n      params:\n        column: \"password\"\n        keep_null: false\n</code></pre> <p>In this configuration, every entry in the <code>password</code> column will be updated with a random password. Setting the <code>keep_null</code> parameter to <code>true</code> will preserve existing NULL values in the column, accommodating scenarios where password data may not be applicable.</p>"},{"location":"built_in_transformers/standard_transformers/random_person/","title":"RandomPerson","text":"<p>The <code>RandomPerson</code> transformer is designed to populate specified database columns with personal attributes such as first name, last name, title and gender.</p>"},{"location":"built_in_transformers/standard_transformers/random_person/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types columns The name of the column to be affected Yes text, varchar gender set specific gender (possible values: Male, Female, Any) <code>Any</code> No - gender_mapping Specify gender name to possible values when using dynamic mode in \"gender\" parameter <code>Any</code> No - fallback_gender Specify fallback gender if not mapped when using dynamic mode in \"gender\" parameter <code>Any</code> No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_person/#description","title":"Description","text":"<p>The <code>RandomPerson</code> transformer utilizes a comprehensive list of first names to inject random first names into the designated database column. This feature allows for the creation of diverse and realistic user profiles by simulating a variety of first names without using real user data.</p>"},{"location":"built_in_transformers/standard_transformers/random_person/#column-object-attributes","title":"column object attributes","text":"<ul> <li><code>name</code> \u2014 the name of the column where the personal attributes will be stored. This value is required.</li> <li> <p><code>template</code> - the template for the column value.   You can use the next attributes: <code>.FirstName</code>, <code>.LastName</code> or <code>.Title</code>. For example, if you want to generate a full   name, you can use the next template:   <code>\"{{ .FirstName }} {{ .LastName }}\"</code></p> </li> <li> <p><code>hashing</code> - the bool value. Indicates whether the column value must be passed through the hashing function.   The default value is <code>false</code>. If all column has <code>hashing</code> set to <code>false</code> (by default), then all columns will be   hashed.</p> </li> <li><code>keep_null</code> - the bool value. Indicates whether NULL values should be preserved. The default value is <code>true</code></li> </ul>"},{"location":"built_in_transformers/standard_transformers/random_person/#gender_mapping-object-attributes","title":"gender_mapping object attributes","text":"<p><code>gender_mapping</code> - a dictionary that maps the gender value when <code>gender</code> parameters works in dynamic mode. The default value is:</p> <pre><code>{\n  \"Male\": [\n    \"male\",\n    \"M\",\n    \"m\",\n    \"man\",\n    \"Man\"\n  ],\n  \"Female\": [\n    \"female\",\n    \"F\",\n    \"f\",\n    \"w\",\n    \"woman\",\n    \"Woman\"\n  ]\n}\n</code></pre>"},{"location":"built_in_transformers/standard_transformers/random_person/#fallback_gender","title":"fallback_gender","text":"<p>Gender that will be used if <code>gender_mapping</code> was not found. This parameter is optional and required only for <code>gender</code> parameter in dynamic mode. The default value is <code>Any</code>.</p>"},{"location":"built_in_transformers/standard_transformers/random_person/#example-populate-random-first-name-and-last-name-for-table-user_profiles-in-static-mode","title":"Example: Populate random first name and last name for table user_profiles in static mode","text":"<p>This example demonstrates how to use the <code>RandomPerson</code> transformer to populate the <code>name</code> and <code>surname</code> columns in the <code>user_profiles</code> table with random first names, last name, respectively.</p> Create table user_profiles and insert data<pre><code>CREATE TABLE personal_data\n(\n    id      SERIAL PRIMARY KEY,\n    name    VARCHAR(100),\n    surname VARCHAR(100),\n    sex     CHAR(1) CHECK (sex IN ('M', 'F'))\n);\n\n-- Insert sample data into the table\nINSERT INTO personal_data (name, surname, sex)\nVALUES ('John', 'Doe', 'M'),\n       ('Jane', 'Smith', 'F'),\n       ('Alice', 'Johnson', 'F'),\n       ('Bob', 'Lee', 'M');\n</code></pre> RandomPerson transformer example<pre><code>- schema: public\n  name: personal_data\n  transformers:\n    - name: \"RandomPerson\"\n      params:\n        gender: \"Any\"\n        columns:\n          - name: \"name\"\n            template: \"{{ .FirstName }}\"\n          - name: \"surname\"\n            template: \"{{ .LastName }}\"\n        engine: \"hash\"\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue nameJohnZane surnameDoeMcCullough"},{"location":"built_in_transformers/standard_transformers/random_person/#example-populate-random-first-name-and-last-name-for-table-user_profiles-in-dynamic-mode","title":"Example: Populate random first name and last name for table user_profiles in dynamic mode","text":"<p>This example demonstrates how to use the <code>RandomPerson</code> transformer to populate the <code>name</code>, <code>surname</code> using dynamic gender</p> RandomPerson transformer example with dynamic mode<pre><code>- schema: public\n  name: personal_data\n  transformers:\n    - name: \"RandomPerson\"\n      params:\n        columns:\n          - name: \"name\"\n            template: \"{{ .FirstName }}\"\n          - name: \"surname\"\n            template: \"{{ .LastName }}\"\n        engine: \"random\"\n      dynamic_params:\n        gender:\n          column: sex\n</code></pre> <p>Result:</p> ColumnOriginalValueTransformedValue nameJohnMartin surnameDoeMueller"},{"location":"built_in_transformers/standard_transformers/random_phone_number/","title":"RandomPhoneNumber","text":"<p>The <code>RandomPhoneNumber</code> transformer is developed to populate specified database columns with random phone numbers. This tool is essential for applications requiring the simulation of contact information, testing phone number validation systems, or anonymizing phone number data in datasets.</p>"},{"location":"built_in_transformers/standard_transformers/random_phone_number/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_phone_number/#description","title":"Description","text":"<p>The <code>RandomPhoneNumber</code> transformer utilizes algorithms capable of generating random phone numbers with various formats and injects them into the designated database column. This feature allows for the creation of diverse and realistic contact information in datasets for development, testing, or data anonymization purposes.</p>"},{"location":"built_in_transformers/standard_transformers/random_phone_number/#example-populate-random-phone-numbers-for-the-contact_information-table","title":"Example: Populate random phone numbers for the <code>contact_information</code> table","text":"<p>This example demonstrates configuring the <code>RandomPhoneNumber</code> transformer to populate the <code>phone_number</code> column in the <code>contact_information</code> table with random phone numbers. It is an effective method for simulating a variety of contact information entries.</p> RandomPhoneNumber transformer example<pre><code>- schema: \"public\"\n  name: \"contact_information\"\n  transformers:\n    - name: \"RandomPhoneNumber\"\n      params:\n        column: \"phone_number\"\n        keep_null: false\n</code></pre> <p>In this configuration, the <code>phone_number</code> column will be updated with random phone numbers for each contact information entry, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, existing NULL values in the column will be preserved, ensuring the integrity of records where phone number information is not applicable or provided.</p>"},{"location":"built_in_transformers/standard_transformers/random_sentence/","title":"RandomSentence","text":"<p>The <code>RandomSentence</code> transformer is designed to populate specified database columns with random sentences. Ideal for simulating natural language text for user comments, testing NLP systems, or anonymizing textual data in databases.</p>"},{"location":"built_in_transformers/standard_transformers/random_sentence/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_sentence/#description","title":"Description","text":"<p>The <code>RandomSentence</code> transformer employs complex text generation algorithms or libraries to generate random sentences, injecting them into a designated database column without the need for specifying sentence length. This flexibility ensures the creation of varied and plausible text for a wide range of applications.</p>"},{"location":"built_in_transformers/standard_transformers/random_sentence/#example-populate-random-sentences-for-the-comments-table","title":"Example: Populate random sentences for the <code>comments</code> table","text":"<p>This example shows how to configure the <code>RandomSentence</code> transformer to populate the <code>comment</code> column in the <code>comments</code> table with random sentences. It is a straightforward method for simulating diverse user-generated content.</p> RandomSentence transformer example<pre><code>- schema: \"public\"\n  name: \"comments\"\n  transformers:\n    - name: \"RandomSentence\"\n      params:\n        column: \"comment\"\n        keep_null: false\n</code></pre> <p>In this configuration, the <code>comment</code> column will be updated with random sentences for each entry, replacing any existing non-NULL values. If <code>keep_null</code> is set to <code>true</code>, existing NULL values in the column will be preserved, maintaining the integrity of records where comments are not applicable.</p>"},{"location":"built_in_transformers/standard_transformers/random_string/","title":"RandomString","text":"<p>Generate a random string using the provided characters within the specified length range.</p>"},{"location":"built_in_transformers/standard_transformers/random_string/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar min_length The minimum length of the generated string Yes - max_length The maximum length of the generated string Yes - symbols The range of characters that can be used in the random string <code>abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ</code> No - keep_null Indicates whether NULL values should be replaced with transformed values or not <code>true</code> No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_string/#description","title":"Description","text":"<p>The <code>RandomString</code> transformer generates a random string with a length between <code>min_length</code> and <code>max_length</code> using the characters specified in the symbols string as the possible set of characters. The behaviour for NULL values can be configured using the <code>keep_null</code> parameter.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_string/#example-generate-a-random-string-for-accountnumber","title":"Example: Generate a random string for <code>accountnumber</code>","text":"<p>In the following example, a random string is generated for the <code>accountnumber</code> column with a length range from <code>9</code> to <code>12</code>. The character set used for generation includes <code>1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ</code>.</p> RandomString transformer example<pre><code>- schema: \"purchasing\"\n  name: \"vendor\"\n  transformers:\n    - name: \"RandomString\"\n      params:\n        column: \"accountnumber\"\n        min_length: 9\n        max_length: 12\n        symbols: \"1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue accountnumberAUSTRALI00014VUI6P2OZ"},{"location":"built_in_transformers/standard_transformers/random_timezone/","title":"RandomTimezone","text":"<p>The <code>RandomTimezone</code> transformer is designed to populate specified database columns with random timezone strings. This transformer is particularly useful for applications that require the simulation of global user data, testing of timezone-related functionalities, or anonymizing real user timezone information in datasets.</p>"},{"location":"built_in_transformers/standard_transformers/random_timezone/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_timezone/#description","title":"Description","text":"<p>Utilizing a comprehensive library or algorithm for generating timezone data, the <code>RandomTimezone</code> transformer provides random timezone strings (e. g., \"America/New_York\", \"Europe/London\") for database columns. This feature enables the creation of diverse and realistic datasets by simulating timezone information for user profiles, event timings, or any other data requiring timezone context.</p>"},{"location":"built_in_transformers/standard_transformers/random_timezone/#example-populate-random-timezone-strings-for-the-user_accounts-table","title":"Example: Populate random timezone strings for  the <code>user_accounts</code> table","text":"<p>This example demonstrates how to configure the <code>RandomTimezone</code> transformer to populate the <code>timezone</code> column in the <code>user_accounts</code> table with random timezone strings, enhancing the dataset with varied global user representations.</p> RandomTimezone transformer example<pre><code>- schema: \"public\"\n  name: \"user_accounts\"\n  transformers:\n    - name: \"RandomTimezone\"\n      params:\n        column: \"timezone\"\n        keep_null: false\n</code></pre> <p>With this configuration, every entry in the <code>timezone</code> column will be updated with a random timezone string, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, existing NULL values within the column will remain unchanged, preserving the integrity of rows without specified timezone data.</p>"},{"location":"built_in_transformers/standard_transformers/random_toll_free_phone_number/","title":"RandomTollFreePhoneNumber","text":"<p>The <code>RandomTollFreePhoneNumber</code> transformer is designed to populate specified database columns with random toll-free phone numbers. This tool is essential for applications requiring the simulation of contact information, testing phone number validation systems, or anonymizing phone number data in datasets while focusing on toll-free numbers.</p>"},{"location":"built_in_transformers/standard_transformers/random_toll_free_phone_number/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_toll_free_phone_number/#description","title":"Description","text":"<p>The <code>RandomTollFreePhoneNumber</code> transformer utilizes algorithms capable of generating random toll-free phone numbers with various formats and injects them into the designated database column. This feature allows for the creation of diverse and realistic toll-free contact information in datasets for development, testing, or data anonymization purposes.</p>"},{"location":"built_in_transformers/standard_transformers/random_toll_free_phone_number/#example-populate-random-toll-free-phone-numbers-for-the-contact_information-table","title":"Example: Populate random toll-free phone numbers for the <code>contact_information</code> table","text":"<p>This example demonstrates configuring the <code>RandomTollFreePhoneNumber</code> transformer to populate the <code>phone_number</code> column in the <code>contact_information</code> table with random toll-free phone numbers. It is an effective method for simulating a variety of contact information entries with toll-free numbers.</p> RandomTollFreePhoneNumber transformer example<pre><code>- schema: \"public\"\n  name: \"contact_information\"\n  transformers:\n    - name: \"RandomTollFreePhoneNumber\"\n      params:\n        column: \"phone_number\"\n        keep_null: false\n</code></pre> <p>In this configuration, the <code>phone_number</code> column will be updated with random toll-free phone numbers for each contact information entry, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, existing NULL values in the column will be preserved, ensuring the integrity of records where toll-free phone number information is not applicable or provided.</p>"},{"location":"built_in_transformers/standard_transformers/random_unix_timestamp/","title":"RandomUnixTimestamp","text":"<p>The <code>RandomUnixTimestamp</code> transformer generates random Unix time values (timestamps) for specified database columns. It is particularly useful for populating columns with timestamp data, simulating time-related data, or anonymizing actual timestamps in a dataset.</p>"},{"location":"built_in_transformers/standard_transformers/random_unix_timestamp/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes int2, int4, int8 min The minimum threshold date for the random value in unix timestamp format (integer) with <code>sec</code> unit by default Yes - max The maximum threshold date for the random value in unix timestamp format (integer) with <code>sec</code> unit by default Yes - unit Generated unix timestamp value unit. Possible values [<code>second</code>, <code>millisecond</code>, <code>microsecond</code>, <code>nanosecond</code>] <code>second</code> Yes - min_unit Min unix timestamp threshold date unit. Possible values [<code>second</code>, <code>millisecond</code>, <code>microsecond</code>, <code>nanosecond</code>] <code>second</code> Yes - max_unit Min unix timestamp threshold date unit. Possible values [<code>second</code>, <code>millisecond</code>, <code>microsecond</code>, <code>nanosecond</code>] <code>second</code> Yes - keep_null Indicates whether NULL values should be preserved <code>false</code> No - truncate Truncate the date to the specified part (<code>nanosecond</code>, <code>microsecond</code>, <code>millisecond</code>, <code>second</code>, <code>minute</code>, <code>hour</code>, <code>day</code>, <code>month</code>, <code>year</code>). The truncate operation is not applied by default. No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_unix_timestamp/#description","title":"Description","text":"<p>The <code>RandomUnixTimestamp</code> transformer generates random Unix timestamps within the provided interval, starting from <code>min</code> to <code>max</code>. The <code>min</code> and <code>max</code> parameters are expected to be in Unix timestamp format. The <code>min_unit</code> and <code>max_unit</code> parameters specify the unit of the Unix timestamp threshold date. The <code>truncate</code> parameter allows you to truncate the date to the specified part of the date. The keep_null parameter allows you to specify whether NULL values should be preserved or replaced with transformed values.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_unix_timestamp/#example-generate-random-unix-timestamps-with-dynamic-parameters","title":"Example: Generate random Unix timestamps with dynamic parameters","text":"<p>In this example, the <code>RandomUnixTimestamp</code> transformer generates random Unix timestamps using dynamic parameters. The <code>min</code> parameter is set to the <code>created_at</code> column, which is converted to Unix seconds using the <code>TimestampToUnixSec</code>. The <code>max</code> parameter is set to a fixed value. The <code>paid_at</code> column is populated with random Unix timestamps in the range from <code>created_at</code> to <code>1715934239</code> (Unix timestamp for <code>2024-05-17 12:03:59</code>). The <code>unit</code> parameter is set to <code>millisecond</code> because the <code>paid_at</code> column stores timestamps in milliseconds.</p> <pre><code>CREATE TABLE transactions\n(\n    id         SERIAL PRIMARY KEY,\n    kind       VARCHAR(255),\n    total      DECIMAL(10, 2),\n    created_at TIMESTAMP,\n    paid_at    BIGINT -- stores milliseconds since the epoch\n);\n\n-- Inserting data with milliseconds timestamp\nINSERT INTO transactions (kind, total, created_at, paid_at)\nVALUES ('Sale', 199.99, '2023-05-17 12:00:00', (EXTRACT(EPOCH FROM TIMESTAMP '2023-05-17 12:05:00') * 1000)),\n       ('Refund', 50.00, '2023-05-18 15:00:00', (EXTRACT(EPOCH FROM TIMESTAMP '2023-05-18 15:10:00') * 1000)),\n       ('Sale', 129.99, '2023-05-19 10:30:00', (EXTRACT(EPOCH FROM TIMESTAMP '2023-05-19 10:35:00') * 1000));\n</code></pre> RandomUnixTimestamp transformer example<pre><code>- schema: \"public\"\n  name: \"transactions\"\n  transformers:\n    - name: \"RandomUnixTimestamp\"\n      params:\n        column: \"paid_at\"\n        max: 1715934239\n        unit: \"millisecond\"\n        min_unit: \"second\"\n        max_unit: \"second\"\n      dynamic_params:\n        min:\n          column: \"created_at\"\n          cast_to: \"TimestampToUnixSec\"\n</code></pre> <p>Result:</p> ColumnOriginalValueTransformedValue paid_at16843251000001708919030732"},{"location":"built_in_transformers/standard_transformers/random_unix_timestamp/#example-generate-simple-random-unix-timestamps","title":"Example: Generate simple random Unix timestamps","text":"<p>In this example, the <code>RandomUnixTimestamp</code> transformer generates random Unix timestamps for the <code>paid_at</code> column in the range from <code>1615934239</code> (Unix timestamp for <code>2021-03-16 12:03:59</code>) to <code>1715934239</code> (Unix timestamp for <code>2024-05-17 12:03:59</code>). The <code>unit</code> parameter is set to <code>millisecond</code> because the <code>paid_at</code> column stores timestamps in milliseconds.</p> <pre><code>- schema: \"public\"\n  name: \"transactions\"\n  transformers:\n    - name: \"RandomUnixTimestamp\"\n      params:\n        column: \"paid_at\"\n        min: 1615934239\n        max: 1715934239\n        unit: \"millisecond\"\n</code></pre> <p>Result:</p> ColumnOriginalValueTransformedValue paid_at16843251000001655768292548"},{"location":"built_in_transformers/standard_transformers/random_url/","title":"RandomURL","text":"<p>The <code>RandomURL</code> transformer is designed to populate specified database columns with random URL (Uniform Resource Locator) addresses. This tool is highly beneficial for simulating web content, testing applications that require URL input, or anonymizing real web addresses in datasets.</p>"},{"location":"built_in_transformers/standard_transformers/random_url/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_url/#description","title":"Description","text":"<p>Utilizing advanced algorithms or libraries for generating URL strings, the <code>RandomURL</code> transformer injects random, plausible URLs into the designated database column. Each generated URL is structured to include the protocol (e. g., \"http://\", \"https://\"), domain name, and path, offering a realistic range of web addresses for various applications.</p>"},{"location":"built_in_transformers/standard_transformers/random_url/#example-populate-random-urls-for-the-webpages-table","title":"Example: Populate random URLs for the <code>webpages</code> table","text":"<p>This example illustrates how to configure the <code>RandomURL</code> transformer to populate the <code>page_url</code> column in a <code>webpages</code> table with random URLs, providing a broad spectrum of web addresses for testing or data simulation purposes.</p> RandomURL transformer example<pre><code>- schema: \"public\"\n  name: \"webpages\"\n  transformers:\n    - name: \"RandomURL\"\n      params:\n        column: \"page_url\"\n        keep_null: false\n</code></pre> <p>With this configuration, the <code>page_url</code> column will be filled with random URLs for each entry, replacing any existing non-NULL values. Setting the <code>keep_null</code> parameter to <code>true</code> allows for the preservation of existing NULL values within the column, accommodating scenarios where URL data may be intentionally omitted.</p>"},{"location":"built_in_transformers/standard_transformers/random_username/","title":"RandomUsername","text":"<p>The <code>RandomUsername</code> transformer is crafted to populate specified database columns with random usernames. This utility is crucial for applications that require the simulation of user data, testing systems with user login functionality, or anonymizing real usernames in datasets.</p>"},{"location":"built_in_transformers/standard_transformers/random_username/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_username/#description","title":"Description","text":"<p>By employing sophisticated algorithms or libraries capable of generating believable usernames, the <code>RandomUsername</code> transformer introduces random usernames into the specified database column. Each generated username is designed to be unique and plausible, incorporating a mix of letters, numbers, and possibly special characters, depending on the generation logic used.</p>"},{"location":"built_in_transformers/standard_transformers/random_username/#example-populate-random-usernames-for-the-user_accounts-table","title":"Example: Populate random usernames for the <code>user_accounts</code> table","text":"<p>This example demonstrates configuring the <code>RandomUsername</code> transformer to populate the <code>username</code> column in a <code>user_accounts</code> table with random usernames. This setup is ideal for creating a diverse and realistic user base for development, testing, or demonstration purposes.</p> RandomUsername transformer example<pre><code>- schema: \"public\"\n  name: \"user_accounts\"\n  transformers:\n    - name: \"RandomUsername\"\n      params:\n        column: \"username\"\n        keep_null: false\n</code></pre> <p>In this configuration, every entry in the <code>username</code> column will be updated with a random username, replacing any existing non-NULL values. If the <code>keep_null</code> parameter is set to <code>true</code>, then the transformer will preserve existing NULL values within the column, maintaining data integrity where usernames are not applicable or available.</p>"},{"location":"built_in_transformers/standard_transformers/random_uuid/","title":"RandomUuid","text":"<p>Generate random unique user ID using version 4.</p>"},{"location":"built_in_transformers/standard_transformers/random_uuid/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar, uuid keep_null Indicates whether NULL values should be replaced with transformed values or not <code>true</code> No - engine The engine used for generating the values [<code>random</code>, <code>hash</code>]. Use hash for deterministic generation <code>random</code> No -"},{"location":"built_in_transformers/standard_transformers/random_uuid/#description","title":"Description","text":"<p>The <code>RandomUuid</code> transformer generates a random UUID. The behaviour for NULL values can be configured using the <code>keep_null</code> parameter.</p> <p>The <code>engine</code> parameter allows you to choose between random and hash engines for generating values. Read more about the engines in the Transformation engines section.</p>"},{"location":"built_in_transformers/standard_transformers/random_uuid/#example-updating-the-rowguid-column","title":"Example: Updating the <code>rowguid</code> column","text":"<p>The following example replaces original UUID values of the <code>rowguid</code> column to randomly generated ones.</p> RandomUuid transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n  - name: \"RandomUuid\"\n    params:\n      column: \"rowguid\"\n      keep_null: false\n</code></pre> <p>Result</p> ColumnOriginalValueTransformedValue rowguidf01251e5-96a3-448d-981e-0f99d789110d8ed8c4b2-7e7a-1e8d-f0f0-768e0e8ed0d0"},{"location":"built_in_transformers/standard_transformers/random_word/","title":"RandomWord","text":"<p>The <code>RandomWord</code> transformer populates specified database columns with random words. Ideal for simulating textual content, enhancing linguistic datasets, or anonymizing text in databases.</p>"},{"location":"built_in_transformers/standard_transformers/random_word/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_word/#description","title":"Description","text":"<p>The <code>RandomWord</code> transformer employs a mechanism to inject random words into a designated database column, supporting the generation of linguistically plausible and contextually diverse text. This transformer is particularly beneficial for creating rich text datasets for development, testing, or educational purposes without specifying the language, focusing on versatility and ease of use.</p>"},{"location":"built_in_transformers/standard_transformers/random_word/#example-populate-random-words-for-the-content-table","title":"Example: Populate random words for the <code>content</code> table","text":"<p>This example demonstrates configuring the <code>RandomWord</code> transformer to populate the <code>tag</code> column in the <code>content</code> table with random words. It is a straightforward approach to adding varied textual data for tagging or content categorization.</p> RandomWord transformer example<pre><code>- schema: \"public\"\n  name: \"content\"\n  transformers:\n    - name: \"RandomWord\"\n      params:\n        column: \"tag\"\n        keep_null: false\n</code></pre> <p>In this setup, the <code>tag</code> column will be updated with random words for each entry, replacing any existing non-NULL values. If <code>keep_null</code> is set to <code>true</code>, existing NULL values in the column will remain unchanged, maintaining data integrity for records where textual data is not applicable.</p>"},{"location":"built_in_transformers/standard_transformers/random_year_string/","title":"RandomYearString","text":"<p>The <code>RandomYearString</code> transformer is designed to populate specified database columns with random year strings. It is ideal for scenarios that require the representation of years without specific dates, such as manufacturing years of products, birth years of users, or any other context where only the year is relevant.</p>"},{"location":"built_in_transformers/standard_transformers/random_year_string/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar, int2, int4, int8, numeric keep_null Indicates whether NULL values should be preserved <code>false</code> No -"},{"location":"built_in_transformers/standard_transformers/random_year_string/#description","title":"Description","text":"<p>The <code>RandomYearString</code> transformer leverages the <code>faker</code> library to generate strings representing random years. This allows for the easy generation of year data in a string format, adding versatility and realism to datasets that need to simulate or anonymize year-related information.</p>"},{"location":"built_in_transformers/standard_transformers/random_year_string/#example-populate-random-year-strings-for-the-products-table","title":"Example: Populate random year strings for the <code>products</code> table","text":"<p>This example shows how to use the <code>RandomYearString</code> transformer to fill the <code>manufacturing_year</code> column in the <code>products</code> table with random year strings, simulating the diversity of manufacturing dates.</p> RandomYearString transformer example<pre><code>- schema: \"public\"\n  name: \"products\"\n  transformers:\n    - name: \"RandomYearString\"\n      params:\n        column: \"manufacturing_year\"\n        keep_null: false\n</code></pre> <p>In this configuration, the <code>manufacturing_year</code> column will be populated with random year strings, replacing any existing non-NULL values. If <code>keep_null</code> is set to <code>true</code>, then existing NULL values in the column will be preserved.</p>"},{"location":"built_in_transformers/standard_transformers/real_address/","title":"RealAddress","text":"<p>Generates real addresses for specified database columns using the <code>faker</code> library. It supports customization of the generated address format through Go templates.</p>"},{"location":"built_in_transformers/standard_transformers/real_address/#parameters","title":"Parameters","text":"Name Properties Description Default Required Supported DB types columns Specifies the affected column names along with additional properties for each column Yes Various \u221f name The name of the column to be affected Yes string \u221f template A Go template string for formatting real address attributes Yes string \u221f keep_null Indicates whether NULL values should be preserved No bool"},{"location":"built_in_transformers/standard_transformers/real_address/#template-value-descriptions","title":"Template value descriptions","text":"<p>The <code>template</code> parameter allows for the injection of real address attributes into a customizable template. The following values can be included in your template:</p> <ul> <li><code>{{.Address}}</code> \u2014 street address or equivalent</li> <li><code>{{.City}}</code> \u2014 city name</li> <li><code>{{.State}}</code> \u2014 state, province, or equivalent region name</li> <li><code>{{.PostalCode}}</code> \u2014 postal or ZIP code</li> <li><code>{{.Latitude}}</code> \u2014 geographic latitude</li> <li><code>{{.Longitude}}</code> \u2014 geographic longitude</li> </ul> <p>These placeholders can be combined and formatted as desired within the template string to generate custom address formats.</p>"},{"location":"built_in_transformers/standard_transformers/real_address/#description","title":"Description","text":"<p>The <code>RealAddress</code> transformer uses the <code>faker</code> library to generate realistic addresses, which can then be formatted according to a specified template and applied to selected columns in a database. It allows for the generated addresses to replace existing values or to preserve NULL values, based on the transformer's configuration.</p>"},{"location":"built_in_transformers/standard_transformers/real_address/#example-generate-real-addresses-for-the-employee-table","title":"Example: Generate Real addresses for the <code>employee</code> table","text":"<p>This example shows how to configure the <code>RealAddress</code> transformer to generate real addresses for the <code>address</code> column in the <code>employee</code> table, using a custom format.</p> RealAddress transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n    - name: \"RealAddress\"\n      params:\n        columns:\n          - name: \"address\"\n            template: \"{{.Address}}, {{.City}}, {{.State}} {{.PostalCode}}\"\n            keep_null: false\n</code></pre> <p>This configuration will generate real addresses with the format \"Street address, city, state postal code\" and apply them to the <code>address</code> column, replacing any existing non-NULL values.</p>"},{"location":"built_in_transformers/standard_transformers/regexp_replace/","title":"RegexpReplace","text":"<p>Replace a string using a regular expression.</p>"},{"location":"built_in_transformers/standard_transformers/regexp_replace/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes text, varchar regexp The regular expression pattern to search for in the column's value Yes - replace The replacement value. This value may be replaced with a captured group from the <code>regexp</code> parameter. Yes -"},{"location":"built_in_transformers/standard_transformers/regexp_replace/#description","title":"Description","text":"<p>The <code>RegexpReplace</code> transformer replaces a string according to the applied regular expression. The valid regular expressions syntax is the same as the general syntax used by Perl, Python, and other languages. To be precise, it is the syntax accepted by RE2 and described in the Golang documentation, except for <code>\\C</code>.</p>"},{"location":"built_in_transformers/standard_transformers/regexp_replace/#example-removing-leading-prefix-from-loginid-column-value","title":"Example: Removing leading prefix from <code>loginid</code> column value","text":"<p>In the following example, the original values from <code>loginid</code> matching the <code>adventure-works\\{{ id_name }}</code> format are replaced with <code>{{ id_name }}</code>.</p> RegexpReplace transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n  - name: \"RegexpReplace\"\n    params:\n      column: \"loginid\"\n      regexp: \"adventure-works\\\\\\\\(.*)\"\n      replace: \"$1\"\n</code></pre> Expected result<pre><code>| column name | original value       | transformed |\n|-------------|----------------------|-------------|\n| loginid     | adventure-works\\ken0 | ken0        |\n</code></pre> <p>Note</p> <p>YAML has control symbols, and using them without escaping may result in an error. In the example above, the prefix of <code>id</code> is separated by the <code>\\</code> symbol. Since this symbol is a control symbol, we must escape it using <code>\\\\</code>. However, the '\\' symbol is also a control symbol for regular expressions, which is why we need to double-escape it as <code>\\\\\\\\</code>.</p>"},{"location":"built_in_transformers/standard_transformers/replace/","title":"Replace","text":"<p>Replace an original value by the provided one.</p>"},{"location":"built_in_transformers/standard_transformers/replace/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes any replace The value to replace Yes - keep_null Indicates whether NULL values should be replaced with transformed values or not <code>true</code> No - validate Performs a decoding procedure via the PostgreSQL driver using the column type to ensure that values have correct type <code>true</code> No -"},{"location":"built_in_transformers/standard_transformers/replace/#description","title":"Description","text":"<p>The <code>Replace</code> transformer replace an original value from the specified column with the provided one. It can optionally run a validation check with the <code>validate</code> parameter to ensure that the values are of a correct type before starting transformation. The behaviour for NULL values can be configured using the <code>keep_null</code> parameter.</p>"},{"location":"built_in_transformers/standard_transformers/replace/#example-updating-the-jobtitle-column","title":"Example: Updating the <code>jobtitle</code> column","text":"<p>In the following example, the provided <code>value: \"programmer\"</code> is first validated through driver decoding. If the current value of the <code>jobtitle</code> column is not <code>NULL</code>, it will be replaced with <code>programmer</code>. If the current value is <code>NULL</code>, it will remain <code>NULL</code>.</p> Replace transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformers:\n  - name: \"Replace\"\n    params:\n      column: \"jobtitle\"\n      value: \"programmer\"\n      keep_null: false\n      validate: true\n</code></pre> Expected result<pre><code>| column name | original value          | transformed |\n|-------------|-------------------------|-------------|\n| jobtitle    | Chief Executive Officer | programmer  |\n</code></pre>"},{"location":"built_in_transformers/standard_transformers/set_null/","title":"SetNull","text":"<p>Set <code>NULL</code> value to a column.</p>"},{"location":"built_in_transformers/standard_transformers/set_null/#parameters","title":"Parameters","text":"Name Description Default Required Supported DB types column The name of the column to be affected Yes any"},{"location":"built_in_transformers/standard_transformers/set_null/#description","title":"Description","text":"<p>The <code>SetNull</code> transformer assigns <code>NULL</code> value to a column. This transformer generates warning if the affected column has <code>NOT NULL</code> constraint.</p> NULL constraint violation warning<pre><code>{\n  \"hash\": \"5a229ee964a4ba674a41a4d63dab5a8c\",\n  \"meta\": {\n    \"ColumnName\": \"jobtitle\",\n    \"ConstraintType\": \"NotNull\",\n    \"ParameterName\": \"column\",\n    \"SchemaName\": \"humanresources\",\n    \"TableName\": \"employee\",\n    \"TransformerName\": \"SetNull\"\n  },\n  \"msg\": \"transformer may produce NULL values but column has NOT NULL constraint\",\n  \"severity\": \"warning\"\n}\n</code></pre>"},{"location":"built_in_transformers/standard_transformers/set_null/#example-set-null-value-to-updated_at-column","title":"Example: Set NULL value to <code>updated_at</code> column","text":"SetNull transformer example<pre><code>- schema: \"humanresources\"\n  name: \"employee\"\n  transformation:\n    - name: \"SetNull\"\n      params:\n        column: \"jobtitle\"\n</code></pre> Expected result<pre><code>| column name | original value          | transformed |\n|-------------|-------------------------|-------------|\n| jobtitle    | Chief Executive Officer | NULL        |\n</code></pre>"},{"location":"commands/","title":"Commands","text":""},{"location":"commands/#introduction","title":"Introduction","text":"Greenmask available commands<pre><code>greenmask \\\n--log-format=[json|text] \\\n--log-level=[debug|info|error] \\\n--config=config.yml \\\n[dump|list-dumps|delete|list-transformers|show-transformer|restore|show-dump]`\n</code></pre> <p>You can use the following commands within Greenmask:</p> <ul> <li>list-transformers \u2014 displays a list of available transformers along with their documentation</li> <li>show-transformer \u2014 displays information about the specified transformer</li> <li>validate - performs a validation procedure by testing config, comparing transformed data, identifying  potential issues, and checking for schema changes.</li> <li>dump \u2014 initiates the data dumping process</li> <li>restore \u2014 restores data to the target database either by specifying a <code>dumpId</code> or using the latest available dump</li> <li>list-dumps \u2014 lists all available dumps stored in the system</li> <li>show-dump \u2014 provides metadata information about a particular dump, offering insights into its structure and     attributes</li> <li>delete \u2014 deletes a specific dump from the storage</li> </ul> <p>For any of the commands mentioned above, you can include the following common flags:</p> <ul> <li><code>--log-format</code> \u2014 specifies the desired format for log output, which can be either <code>json</code> or <code>text</code>. This parameter is optional, with the default format set to <code>text</code>.</li> <li><code>--log-level</code> \u2014 sets the desired level for log output, which can be one of <code>debug</code>, <code>info</code>, or <code>error</code>. This parameter is optional, with the default log level being <code>info</code>.</li> <li><code>--config</code> \u2014 requires the specification of a configuration file in YAML format. This configuration file is mandatory for Greenmask to operate correctly.</li> <li><code>--help</code> \u2014 displays comprehensive help information for Greenmask, providing guidance on its usage and available commands.</li> </ul>"},{"location":"commands/delete/","title":"delete command","text":"<p>Delete dump from the storage with a specific ID</p> Supported flags<pre><code>Usage:\n  greenmask delete [flags] [dumpId]\n\nFlags:\n      --before-date string   delete dumps older than the specified date in RFC3339Nano format: 2021-01-01T00:00.0:00Z\n      --dry-run              do not delete anything, just show what would be deleted\n      --prune-failed         prune failed dumps\n      --prune-unsafe         prune dumps with \"unknown-or-failed\" statuses. Works only with --prune-failed\n      --retain-for string    retain dumps for the specified duration in format: 1w2d3h4m5s6ms7us8ns\n      --retain-recent int    retain the most recent N completed dumps (default -1)\n</code></pre> delete dump by id<pre><code>greenmask --config config.yml delete 1723643249862\n</code></pre> delete dumps older than the specified date<pre><code>greenmask --config config.yml delete --before-date 2021-01-01T00:00.0:00Z --dry-run \n</code></pre> prune failed dumps<pre><code>greenmask --config config.yml delete --prune-failed --dry-run \n</code></pre> prune dumps with 'unknown-or-failed' statuses<pre><code>greenmask --config config.yml delete --prune-failed --prune-unsafe --dry-run\n</code></pre> retain dumps for the specified duration<pre><code>greenmask --config config.yml delete --retain-for 1w5d --dry-run\n</code></pre> retain the most recent N completed dumps<pre><code>greenmask --config config.yml delete --retain-recent 5 --dry-run\n</code></pre>"},{"location":"commands/dump/","title":"dump","text":""},{"location":"commands/dump/#dump-command","title":"dump command","text":"<p>The <code>dump</code> command operates in the following way:</p> <ol> <li>Dumps the data from the source database.</li> <li>Validates the data for potential issues.</li> <li>Applies the defined transformations.</li> <li>Stores the transformed data in the specified storage location.</li> </ol> <p>Note that the <code>dump</code> command shares the same parameters and environment variables as <code>pg_dump</code>, allowing you to configure the restoration process as needed.</p> <p>Mostly it supports the same flags as the <code>pg_dump</code> utility, with some extra flags for Greenmask-specific features.</p> Supported flags<pre><code>  -b, --blobs                           include large objects in dump\n  -c, --clean                           clean (drop) database objects before recreating\n  -Z, --compress int                    compression level for compressed formats (default -1)\n  -C, --create                          include commands to create database in dump\n  -a, --data-only                       dump only the data, not the schema\n  -d, --dbname string                   database to dump (default \"postgres\")\n      --disable-dollar-quoting          disable dollar quoting, use SQL standard quoting\n      --disable-triggers                disable triggers during data-only restore\n      --enable-row-security             enable row security (dump only content user has access to)\n  -E, --encoding string                 dump the data in encoding ENCODING\n  -N, --exclude-schema strings          dump the specified schema(s) only\n  -T, --exclude-table strings           do NOT dump the specified table(s)\n      --exclude-table-data strings      do NOT dump data for the specified table(s)\n  -e, --extension strings               dump the specified extension(s) only\n      --extra-float-digits string       override default setting for extra_float_digits\n  -f, --file string                     output file or directory name\n  -h, --host string                     database server host or socket directory (default \"/var/run/postgres\")\n      --if-exists                       use IF EXISTS when dropping objects\n      --include-foreign-data strings    use IF EXISTS when dropping objects\n  -j, --jobs int                        use this many parallel jobs to dump (default 1)\n      --load-via-partition-root         load partitions via the root table\n      --lock-wait-timeout int           fail after waiting TIMEOUT for a table lock (default -1)\n  -B, --no-blobs                        exclude large objects in dump\n      --no-comments                     do not dump comments\n  -O, --no-owner string                 skip restoration of object ownership in plain-text format\n  -X, --no-privileges                   do not dump privileges (grant/revoke)\n      --no-publications                 do not dump publications\n      --no-security-labels              do not dump security label assignments\n      --no-subscriptions                do not dump subscriptions\n      --no-sync                         do not wait for changes to be written safely to dis\n      --no-synchronized-snapshots       do not use synchronized snapshots in parallel jobs\n      --no-tablespaces                  do not dump tablespace assignments\n      --no-toast-compression            do not dump TOAST compression methods\n      --no-unlogged-table-data          do not dump unlogged table data\n      --pgzip                           use pgzip compression instead of gzip\n  -p, --port int                        database server port number (default 5432)\n      --quote-all-identifiers           quote all identifiers, even if not key words\n  -n, --schema strings                  dump the specified schema(s) only\n  -s, --schema-only string              dump only the schema, no data\n      --section string                  dump named section (pre-data, data, or post-data)\n      --serializable-deferrable         wait until the dump can run without anomalies\n      --snapshot string                 use given snapshot for the dump\n      --strict-names                    require table and/or schema include patterns to match at least one entity each\n  -S, --superuser string                superuser user name to use in plain-text format\n  -t, --table strings                   dump the specified table(s) only\n      --test string                     connect as specified database user (default \"postgres\")\n      --use-set-session-authorization   use SET SESSION AUTHORIZATION commands instead of ALTER OWNER commands to set ownership\n  -U, --username string                 connect as specified database user (default \"postgres\")\n  -v, --verbose string                  verbose mode\n</code></pre>"},{"location":"commands/dump/#pgzip-compression","title":"Pgzip compression","text":"<p>By default, Greenmask uses gzip compression to restore data. In mist cases it is quite slow and does not utilize all available resources and is a bootleneck for IO operations. To speed up the restoration process, you can use the <code>--pgzip</code> flag to use pgzip compression instead of gzip. This method splits the data into blocks, which are compressed in parallel, making it ideal for handling large volumes of data. The output remains a standard gzip file.</p>"},{"location":"commands/list-dumps/","title":"list-dumps","text":""},{"location":"commands/list-dumps/#list-dumps-command","title":"list-dumps command","text":"<p>The <code>list-dumps</code> command provides a list of all dumps stored in the storage. The list includes the following attributes:</p> <ul> <li><code>ID</code> \u2014 the unique identifier of the dump, used for operations like <code>restore</code>, <code>delete</code>, and <code>show-dump</code></li> <li><code>DATE</code> \u2014 the date when the snapshot was created</li> <li><code>DATABASE</code> \u2014 the name of the database associated with the dump</li> <li><code>SIZE</code> \u2014 the original size of the dump</li> <li><code>COMPRESSED SIZE</code> \u2014 the size of the dump after compression</li> <li><code>DURATION</code> \u2014 the duration of the dump procedure</li> <li><code>TRANSFORMED</code> \u2014 indicates whether the dump has been transformed</li> <li><code>STATUS</code> \u2014 the status of the dump, which can be one of the following:<ul> <li><code>done</code> \u2014 the dump was completed successfully</li> <li><code>in progress</code> \u2014 the dump is currently being created</li> <li><code>failed</code> \u2014 the dump creation process failed</li> <li><code>unknown or failed</code> \u2014 the deprecated status of the dump that is used for failed dumps or dumps in progress for     version v0.1.14 and earlier</li> </ul> </li> </ul> <p>Example of <code>list-dumps</code> output: </p> <p>Info</p> <p>Greenmask uses a heartbeat mechanism to determine the status of a dump. A dump is considered <code>failed</code> if it lacks a \"done\" heartbeat or if the last heartbeat timestamp exceeds 30 minutes. Heartbeats are recorded every 15 minutes by the <code>dump</code> command while it is in progress. If <code>greenmask</code> fails unexpectedly, the heartbeat stops being updated, and after 30 minutes (twice the interval), the dump is classified as <code>failed</code>.  The <code>in progress</code> status indicates that a dump is still ongoing.</p>"},{"location":"commands/list-transformers/","title":"list-transformers","text":""},{"location":"commands/list-transformers/#list-transformers-command","title":"list-transformers command","text":"<p>The <code>list-transformers</code> command provides a list of all the allowed transformers, including both standard and advanced transformers. This list can be helpful for searching for an appropriate transformer for your data transformation needs.</p> <p>To show a list of available transformers, use the following command:</p> <pre><code>greenmask --config=config.yml list-transformers\n</code></pre> <p>Supported flags:</p> <ul> <li><code>--format</code> \u2014 allows to select the output format. There are two options available: <code>text</code> or <code>json</code>. The   default setting is <code>text</code>.</li> </ul> <p>Example of <code>list-transformers</code> output:</p> <p></p> <p>When using the <code>list-transformers</code> command, you receive a list of available transformers with essential information about each of them. Below are the key parameters for each transformer:</p> <ul> <li><code>NAME</code> \u2014 the name of the transformer</li> <li><code>DESCRIPTION</code> \u2014 a brief description of what the transformer does</li> <li><code>COLUMN PARAMETER NAME</code> \u2014 name of a column or columns affected by transformation</li> <li><code>SUPPORTED TYPES</code> \u2014 list the supported value types</li> </ul> <p>The JSON call <code>greenmask --config=config.yml list-transformers --format=json</code> has the same attributes:</p> JSON format output<pre><code>[\n  {\n    \"name\": \"Cmd\",\n    \"description\": \"Transform data via external program using stdin and stdout interaction\",\n    \"parameters\": [\n      {\n        \"name\": \"columns\",\n        \"supported_types\": [\n          \"any\"\n        ]\n      }\n    ]\n  },\n  {\n    \"name\": \"Dict\",\n    \"description\": \"Replace values matched by dictionary keys\",\n    \"parameters\": [\n      {\n        \"name\": \"column\",\n        \"supported_types\": [\n          \"any\"\n        ]\n      }\n    ]\n  }\n]\n</code></pre>"},{"location":"commands/restore/","title":"restore","text":""},{"location":"commands/restore/#restore-command","title":"restore command","text":"<p>The <code>restore</code> command is used to restore a database from a previously created dump. You can specify the dump to restore by providing the dump ID or use the <code>latest</code> keyword to restore the latest completed dump.</p> <pre><code>greenmask --config=config.yml restore DUMP_ID\n</code></pre> <p>Alternatively, to restore the latest completed dump, use the following command:</p> <pre><code>greenmask --config=config.yml restore latest\n</code></pre> <p>Note that the <code>restore</code> command shares the same parameters and environment variables as <code>pg_restore</code>, allowing you to configure the restoration process as needed.</p> <p>Mostly it supports the same flags as the <code>pg_restore</code> utility, with some extra flags for Greenmask-specific features.</p> Supported flags<pre><code>      --batch-size int                  the number of rows to insert in a single batch during the COPY command (0 - all rows will be inserted in a single batch)\n  -c, --clean                           clean (drop) database objects before recreating\n  -C, --create                          create the target database\n  -a, --data-only                       restore only the data, no schema\n  -d, --dbname string                   connect to database name (default \"postgres\")\n      --disable-triggers                disable triggers during data-only restore\n      --enable-row-security             enable row security\n  -N, --exclude-schema strings          do not restore objects in this schema\n  -e, --exit-on-error                   exit on error, default is to continue\n  -f, --file string                     output file name (- for stdout)\n  -P, --function strings                restore named function\n  -h, --host string                     database server host or socket directory (default \"/var/run/postgres\")\n      --if-exists                       use IF EXISTS when dropping objects\n  -i, --index strings                   restore named index\n      --inserts                         restore data as INSERT commands, rather than COPY\n  -j, --jobs int                        use this many parallel jobs to restore (default 1)\n      --list-format string              use table of contents in format of text, json or yaml (default \"text\")\n      --no-comments                     do not restore comments\n      --no-data-for-failed-tables       do not restore data of tables that could not be created\n  -O, --no-owner string                 skip restoration of object ownership\n  -X, --no-privileges                   skip restoration of access privileges (grant/revoke)\n      --no-publications                 do not restore publications\n      --no-security-labels              do not restore security labels\n      --no-subscriptions                ddo not restore subscriptions\n      --no-table-access-method          do not restore table access methods\n      --no-tablespaces                  do not restore tablespace assignments\n      --on-conflict-do-nothing          add ON CONFLICT DO NOTHING to INSERT commands\n      --overriding-system-value         use OVERRIDING SYSTEM VALUE clause for INSERTs\n      --pgzip                           use pgzip decompression instead of gzip\n  -p, --port int                        database server port number (default 5432)\n      --restore-in-order                restore tables in topological order, ensuring that dependent tables are not restored until the tables they depend on have been restored\n  -n, --schema strings                  restore only objects in this schema\n  -s, --schema-only                     restore only the schema, no data\n      --section string                  restore named section (pre-data, data, or post-data)\n  -1, --single-transaction              restore as a single transaction\n      --strict-names                    restore named section (pre-data, data, or post-data) match at least one entity each\n  -S, --superuser string                superuser user name to use for disabling triggers\n  -t, --table strings                   restore named relation (table, view, etc.)\n  -T, --trigger strings                 restore named trigger\n  -L, --use-list string                 use table of contents from this file for selecting/ordering output\n      --use-set-session-authorization   use SET SESSION AUTHORIZATION commands instead of ALTER OWNER commands to set ownership\n  -U, --username string                 connect as specified database user (default \"postgres\")\n  -v, --verbose string                  verbose mode\n</code></pre>"},{"location":"commands/restore/#extra-features","title":"Extra features","text":""},{"location":"commands/restore/#inserts-and-error-handling","title":"Inserts and error handling","text":"<p>Warning</p> <p>Insert commands are a lot slower than <code>COPY</code> commands. Use this feature only when necessary.</p> <p>By default, Greenmask restores data using the <code>COPY</code> command. If you prefer to restore data using <code>INSERT</code> commands, you can use the <code>--inserts</code> flag. This flag allows you to manage errors that occur during the execution of INSERT commands. By configuring an error and constraint exclusion list in the config, you can skip certain errors and continue inserting subsequent rows from the dump.</p> <p>This can be useful when adding new records to an existing dump, but you don't want the process to stop if some records already exist in the database or violate certain constraints.</p> <p>By adding the <code>--on-conflict-do-nothing</code> flag, it generates <code>INSERT</code> statements with the ON <code>CONFLICT DO NOTHING</code> clause, similar to the original pg_dump option. However, this approach only works for unique or exclusion constraints. If a foreign key is missing in the referenced table or any other constraint is violated, the insertion will still fail. To handle these issues, you can define anexclusion list in the config.</p> example with inserts and error handling<pre><code>```shell title=\"example with inserts and on conflict do nothing\"\ngreenmask --config=config.yml restore DUMP_ID --inserts --on-conflict-do-nothing\n</code></pre> <p>By adding the <code>--overriding-system-value</code> flag, it generates <code>INSERT</code> statements with the <code>OVERRIDING SYSTEM VALUE</code> clause, which allows you to insert data into identity columns. </p> example of GENERATED ALWAYS AS IDENTITY column<pre><code>CREATE TABLE people (\n    id integer GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n    generated text GENERATED ALWAYS AS (id || first_name) STORED,\n    first_name text\n);\n</code></pre> example with inserts<pre><code>greenmask --config=config.yml restore DUMP_ID --inserts --overriding-system-value\n</code></pre>"},{"location":"commands/restore/#restoration-in-topological-order","title":"Restoration in topological order","text":"<p>By default, Greenmask restores tables in the order they are listed in the dump file. To restore tables in topological order, use the <code>--restore-in-order</code> flag. This flag ensures that dependent tables are not restored until the tables they depend on have been restored.</p> <p>This is useful when you have the schema already created with foreign keys and other constraints, and you want to insert data into the tables in the correct order or catch-up the target database with the new data.</p> <p>Warning</p> <p>Greenmask cannot guarantee restoration in topological order when the schema contains cycles. The only way to restore tables with cyclic dependencies is to temporarily remove the foreign key constraint (to break the cycle), restore the data, and then re-add the foreign key constraint once the data restoration is complete.</p> <p>If your database has cyclic dependencies you will be notified about it but the restoration will continue.</p> <pre><code>2024-08-16T21:39:50+03:00 WRN cycle between tables is detected: cannot guarantee the order of restoration within cycle cycle=[\"public.employees\",\"public.departments\",\"public.projects\",\"public.employees\"]\n</code></pre>"},{"location":"commands/restore/#pgzip-decompression","title":"Pgzip decompression","text":"<p>By default, Greenmask uses gzip decompression to restore data. In mist cases it is quite slow and does not utilize all available resources and is a bootleneck for IO operations. To speed up the restoration process, you can use the <code>--pgzip</code> flag to use pgzip decompression instead of gzip. This method splits the data into blocks, which are decompressed in parallel, making it ideal for handling large volumes of data.</p> example with pgzip decompression<pre><code>greenmask --config=config.yml restore latest --pgzip\n</code></pre>"},{"location":"commands/restore/#restore-data-batching","title":"Restore data batching","text":"<p>The COPY command returns the error only on transaction commit. This means that if you have a large dump and an error occurs, you will have to wait until the end of the transaction to see the error message. To avoid this, you can use the <code>--batch-size</code> flag to specify the number of rows to insert in a single batch during the COPY command. If an error occurs during the batch insertion, the error message will be displayed immediately. The data will be committed only  if all batches are inserted successfully.</p> <p>This is useful when you want to be notified of errors as immediately as possible without waiting for the entire table to be restored.</p> <p>Warning</p> <p>The batch size should be chosen carefully. If the batch size is too small, the restoration process will be slow. If the batch size is too large, you may not be able to identify the error row.</p> <p>In the example below, the batch size is set to 1000 rows. This means that 1000 rows will be inserted in a single batch, so you will be notified of any errors immediately after each batch is inserted.</p> example with batch size<pre><code>greenmask --config=config.yml restore latest --batch-size 1000\n</code></pre>"},{"location":"commands/show-dump/","title":"show-dump","text":""},{"location":"commands/show-dump/#show-dump-command","title":"show-dump command","text":"<p>This command provides details about all objects and data that can be restored, similar to the <code>pg_restore -l</code> command in PostgreSQL. It helps you inspect the contents of the dump before performing the actual restoration.</p> <p>Parameters:</p> <ul> <li><code>--format</code> \u2014 format of printing. Can be <code>text</code> or <code>json</code>.</li> </ul> <p>To display metadata information about a dump, use the following command:</p> <pre><code>greenmask --config=config.yml show-dump dumpID\n</code></pre> Text output example <pre><code>;\n; Archive created at 2023-10-30 12:52:38 UTC\n; dbname: demo\n; TOC Entries: 17\n; Compression: -1\n; Dump Version: 15.4\n; Format: DIRECTORY\n; Integer: 4 bytes\n; Offset: 8 bytes\n; Dumped from database version: 15.4\n; Dumped by pg_dump version: 15.4\n;\n;\n; Selected TOC Entries:\n;\n3444; 0 0 ENCODING - ENCODING\n3445; 0 0 STDSTRINGS - STDSTRINGS\n3446; 0 0 SEARCHPATH - SEARCHPATH\n3447; 1262 24970 DATABASE - demo postgres\n3448; 0 0 DATABASE PROPERTIES - demo postgres\n222; 1259 24999 TABLE bookings flights postgres\n223; 1259 25005 SEQUENCE bookings flights_flight_id_seq postgres\n3460; 0 0 SEQUENCE OWNED BY bookings flights_flight_id_seq postgres\n3281; 2604 25030 DEFAULT bookings flights flight_id postgres\n3462; 0 24999 TABLE DATA bookings flights postgres\n3289; 2606 25044 CONSTRAINT bookings flights flights_flight_no_scheduled_departure_key postgres\n3291; 2606 25046 CONSTRAINT bookings flights flights_pkey postgres\n3287; 1259 42848 INDEX bookings flights_aircraft_code_status_idx postgres\n3292; 1259 42847 INDEX bookings flights_status_aircraft_code_idx postgres\n3293; 2606 25058 FK CONSTRAINT bookings flights flights_aircraft_code_fkey postgres\n3294; 2606 25063 FK CONSTRAINT bookings flights flights_arrival_airport_fkey postgres\n3295; 2606 25068 FK CONSTRAINT bookings flights flights_departure_airport_fkey postgres\n</code></pre> JSON output example <p><pre><code>{\n  \"startedAt\": \"2023-10-29T20:50:19.948017+02:00\", // (1)\n  \"completedAt\": \"2023-10-29T20:50:22.19333+02:00\", // (2)\n  \"originalSize\": 4053842, // (3)\n  \"compressedSize\": 686557, // (4)\n  \"transformers\": [ // (5)\n    {\n      \"Schema\": \"bookings\", // (6)\n      \"Name\": \"flights\", // (7)\n      \"Query\": \"\", // (8)\n      \"Transformers\": [ // (9)\n        {\n          \"Name\": \"RandomDate\", // (10)\n          \"Params\": { // (11)\n            \"column\": \"c2NoZWR1bGVkX2RlcGFydHVyZQ==\",\n            \"max\": \"MjAyMy0wMS0wMiAwMDowMDowMC4wKzAz\",\n            \"min\": \"MjAyMy0wMS0wMSAwMDowMDowMC4wKzAz\"\n          }\n        }\n      ],\n      \"ColumnsTypeOverride\": null // (12)\n    }\n  ],\n  \"header\": { // (13)\n    \"creationDate\": \"2023-10-29T20:50:20+02:00\",\n    \"dbName\": \"demo\",\n    \"tocEntriesCount\": 15,\n    \"dumpVersion\": \"16.0 (Homebrew)\",\n    \"format\": \"TAR\",\n    \"integer\": 4,\n    \"offset\": 8,\n    \"dumpedFrom\": \"16.0 (Debian 16.0-1.pgdg120+1)\",\n    \"dumpedBy\": \"16.0 (Homebrew)\",\n    \"tocFileSize\": 8090,\n    \"compression\": 0\n  },\n  \"entries\": [ // (14)\n    {\n      \"dumpId\": 3416,\n      \"databaseOid\": 0,\n      \"objectOid\": 0,\n      \"objectType\": \"ENCODING\",\n      \"schema\": \"\",\n      \"name\": \"ENCODING\",\n      \"owner\": \"\",\n      \"section\": \"PreData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": null\n    },\n    {\n      \"dumpId\": 3417,\n      \"databaseOid\": 0,\n      \"objectOid\": 0,\n      \"objectType\": \"STDSTRINGS\",\n      \"schema\": \"\",\n      \"name\": \"STDSTRINGS\",\n      \"owner\": \"\",\n      \"section\": \"PreData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": null\n    },\n    {\n      \"dumpId\": 3418,\n      \"databaseOid\": 0,\n      \"objectOid\": 0,\n      \"objectType\": \"SEARCHPATH\",\n      \"schema\": \"\",\n      \"name\": \"SEARCHPATH\",\n      \"owner\": \"\",\n      \"section\": \"PreData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": null\n    },\n    {\n      \"dumpId\": 3419,\n      \"databaseOid\": 16384,\n      \"objectOid\": 1262,\n      \"objectType\": \"DATABASE\",\n      \"schema\": \"\",\n      \"name\": \"demo\",\n      \"owner\": \"postgres\",\n      \"section\": \"PreData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": null\n    },\n    {\n      \"dumpId\": 3420,\n      \"databaseOid\": 0,\n      \"objectOid\": 0,\n      \"objectType\": \"DATABASE PROPERTIES\",\n      \"schema\": \"\",\n      \"name\": \"demo\",\n      \"owner\": \"postgres\",\n      \"section\": \"PreData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": null\n    },\n    {\n      \"dumpId\": 222,\n      \"databaseOid\": 16414,\n      \"objectOid\": 1259,\n      \"objectType\": \"TABLE\",\n      \"schema\": \"bookings\",\n      \"name\": \"flights\",\n      \"owner\": \"postgres\",\n      \"section\": \"PreData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": null\n    },\n    {\n      \"dumpId\": 223,\n      \"databaseOid\": 16420,\n      \"objectOid\": 1259,\n      \"objectType\": \"SEQUENCE\",\n      \"schema\": \"bookings\",\n      \"name\": \"flights_flight_id_seq\",\n      \"owner\": \"postgres\",\n      \"section\": \"PreData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": [\n        222\n      ]\n    },\n    {\n      \"dumpId\": 3432,\n      \"databaseOid\": 0,\n      \"objectOid\": 0,\n      \"objectType\": \"SEQUENCE OWNED BY\",\n      \"schema\": \"bookings\",\n      \"name\": \"flights_flight_id_seq\",\n      \"owner\": \"postgres\",\n      \"section\": \"PreData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": [\n        223\n      ]\n    },\n    {\n      \"dumpId\": 3254,\n      \"databaseOid\": 16445,\n      \"objectOid\": 2604,\n      \"objectType\": \"DEFAULT\",\n      \"schema\": \"bookings\",\n      \"name\": \"flights flight_id\",\n      \"owner\": \"postgres\",\n      \"section\": \"PreData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": [\n        223,\n        222\n      ]\n    },\n    {\n      \"dumpId\": 3434,\n      \"databaseOid\": 16414,\n      \"objectOid\": 0,\n      \"objectType\": \"TABLE DATA\",\n      \"schema\": \"\\\"bookings\\\"\",\n      \"name\": \"\\\"flights\\\"\",\n      \"owner\": \"\\\"postgres\\\"\",\n      \"section\": \"Data\",\n      \"originalSize\": 4045752,\n      \"compressedSize\": 678467,\n      \"fileName\": \"3434.dat.gz\",\n      \"dependencies\": []\n    },\n    {\n      \"dumpId\": 3261,\n      \"databaseOid\": 16461,\n      \"objectOid\": 2606,\n      \"objectType\": \"CONSTRAINT\",\n      \"schema\": \"bookings\",\n      \"name\": \"flights flights_flight_no_scheduled_departure_key\",\n      \"owner\": \"postgres\",\n      \"section\": \"PostData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": [\n        222,\n        222\n      ]\n    },\n    {\n      \"dumpId\": 3263,\n      \"databaseOid\": 16463,\n      \"objectOid\": 2606,\n      \"objectType\": \"CONSTRAINT\",\n      \"schema\": \"bookings\",\n      \"name\": \"flights flights_pkey\",\n      \"owner\": \"postgres\",\n      \"section\": \"PostData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": [\n        222\n      ]\n    },\n    {\n      \"dumpId\": 3264,\n      \"databaseOid\": 16477,\n      \"objectOid\": 2606,\n      \"objectType\": \"FK CONSTRAINT\",\n      \"schema\": \"bookings\",\n      \"name\": \"flights flights_aircraft_code_fkey\",\n      \"owner\": \"postgres\",\n      \"section\": \"PostData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": [\n        222\n      ]\n    },\n    {\n      \"dumpId\": 3265,\n      \"databaseOid\": 16482,\n      \"objectOid\": 2606,\n      \"objectType\": \"FK CONSTRAINT\",\n      \"schema\": \"bookings\",\n      \"name\": \"flights flights_arrival_airport_fkey\",\n      \"owner\": \"postgres\",\n      \"section\": \"PostData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": [\n        222\n      ]\n    },\n    {\n      \"dumpId\": 3266,\n      \"databaseOid\": 16487,\n      \"objectOid\": 2606,\n      \"objectType\": \"FK CONSTRAINT\",\n      \"schema\": \"bookings\",\n      \"name\": \"flights flights_departure_airport_fkey\",\n      \"owner\": \"postgres\",\n      \"section\": \"PostData\",\n      \"originalSize\": 0,\n      \"compressedSize\": 0,\n      \"fileName\": \"\",\n      \"dependencies\": [\n        222\n      ]\n    }\n  ]\n}\n</code></pre></p> <ol> <li>The date when the backup has been initiated, also indicating the snapshot date.</li> <li>The date when the backup process was successfully completed.</li> <li>The original size of the backup in bytes.</li> <li>The size of the backup after compression in bytes.</li> <li>A list of tables that underwent transformation during the backup.</li> <li>The schema name of the table.</li> <li>The name of the table.</li> <li>Custom query override, if applicable.</li> <li>A list of transformers that were applied during the backup.</li> <li>The name of the transformer.</li> <li>The parameters provided for the transformer.</li> <li>A mapping of overridden column types.</li> <li>The header information in the table of contents file. This provides the same details as the <code>--format=text</code> output in the previous snippet.</li> <li>The list of restoration entries. This offers the same information as the <code>--format=text</code> output in the previous snippet.</li> </ol> <p>Note</p> <p>The <code>json</code> format provides more detailed information compared to the <code>text</code> format. The <code>text</code> format is primarily used for backward compatibility and for generating a restoration list that can be used with <code>pg_restore -L listfile</code>. On the other hand, the <code>json</code> format provides comprehensive metadata about the dump, including information about the applied transformers and their parameters. The <code>json</code> format is especially useful for detailed dump introspection.</p>"},{"location":"commands/show-transformer/","title":"show-transformer","text":""},{"location":"commands/show-transformer/#show-transformer-command","title":"show-transformer command","text":"<p>This command prints out detailed information about a transformer by a provided name, including specific attributes to help you understand and configure the transformer effectively.</p> <p>To show detailed information about a transformer, use the following command:</p> <pre><code>greenmask --config=config.yml show-transformer TRANSFORMER_NAME\n</code></pre> <p>Supported flags:</p> <ul> <li><code>--format</code> \u2014 allows to select the output format. There are two options available: <code>text</code> or <code>json</code>. The   default setting is <code>text</code>.</li> </ul> <p>Example of <code>show-transformer</code> output:</p> <p></p> <p>When using the <code>show-transformer</code> command, you receive detailed information about the transformer and its parameters and their possible attributes. Below are the key parameters for each transformer:</p> <ul> <li><code>Name</code> \u2014 the name of the transformer</li> <li><code>Description</code> \u2014 a brief description of what the transformer does</li> <li> <p><code>Parameters</code> \u2014 a list of transformer parameters, each with its own set of attributes. Possible attributes include:</p> <ul> <li><code>description</code> \u2014 a brief description of the parameter's purpose</li> <li><code>required</code> \u2014 a flag indicating whether the parameter is required when configuring the transformer</li> <li><code>link_parameter</code> \u2014 specifies whether the value of the parameter will be encoded using a specific parameter type   encoder. For example, if a parameter named <code>column</code> is linked to another parameter <code>start</code>, the <code>start</code>   parameter's value will be encoded according to the <code>column</code> type when the transformer is initialized.</li> <li><code>cast_db_type</code> \u2014 indicates that the value should be encoded according to the database type. For example, when   dealing with the INTERVAL data type, you must provide the interval value in PostgreSQL format.</li> <li><code>default_value</code> \u2014 the default value assigned to the parameter if it's not provided during configuration.</li> <li><code>column_properties</code> \u2014 if a parameter represents the name of a column, it may contain additional properties,   including:<ul> <li><code>nullable</code> \u2014 indicates whether the transformer may produce NULL values, potentially violating the NOT NULL   constraint</li> <li><code>unique</code> \u2014 specifies whether the transformer guarantees unique values for each call. If set to <code>true</code>, it   means that the transformer cannot produce duplicate values, ensuring compliance with the UNIQUE constraint.</li> <li><code>affected</code> \u2014 indicates whether the column is affected during the transformation process. If not affected, the   column's value might still be required for transforming another column.</li> <li><code>allowed_types</code> \u2014 a list of data types that are compatible with this parameter</li> <li><code>skip_original_data</code> \u2014 specifies whether the original value of the column, before transformation, is relevant   for the transformation process</li> <li><code>skip_on_null</code> \u2014 indicates whether the transformer should skip the transformation when the input column value   is NULL. If the column value is NULL, interaction with the transformer is unnecessary.</li> </ul> </li> </ul> </li> </ul> <p>Warning</p> <p>The default value in JSON format is base64 encoded. This might be changed in later version of Greenmask.</p> JSON output example<pre><code>[\n  {\n    \"properties\": {\n      \"name\": \"NoiseFloat\",\n      \"description\": \"Make noise float for int\",\n      \"is_custom\": false\n    },\n    \"parameters\": [\n      {\n        \"name\": \"column\",\n        \"description\": \"column name\",\n        \"required\": true,\n        \"is_column\": true,\n        \"is_column_container\": false,\n        \"column_properties\": {\n          \"max_length\": -1,\n          \"affected\": true,\n          \"allowed_types\": [\n            \"float4\",\n            \"float8\",\n            \"numeric\"\n          ],\n          \"skip_on_null\": true\n        }\n      },\n      {\n        \"name\": \"ratio\",\n        \"description\": \"max random percentage for noise\",\n        \"required\": false,\n        \"is_column\": false,\n        \"is_column_container\": false,\n        \"default_value\": \"MC4x\"\n      },\n      {\n        \"name\": \"decimal\",\n        \"description\": \"decimal of noised float value (number of digits after coma)\",\n        \"required\": false,\n        \"is_column\": false,\n        \"is_column_container\": false,\n        \"default_value\": \"NA==\"\n      }\n    ]\n  }\n]\n</code></pre>"},{"location":"commands/validate/","title":"validate command","text":"<p>The <code>validate</code> command allows you to perform a validation procedure and compare transformed data.</p> <p>Below is a list of all supported flags for the <code>validate</code> command:</p> Supported flags<pre><code>Usage:\n  greenmask validate [flags]\n\nFlags:\n      --data                  Perform test dump for --rows-limit rows and print it pretty\n      --diff                  Find difference between original and transformed data\n      --format string         Format of output. possible values [text|json] (default \"text\")\n      --rows-limit uint       Check tables dump only for specific tables (default 10)\n      --schema                Make a schema diff between previous dump and the current state\n      --table strings         Check tables dump only for specific tables\n      --table-format string   Format of table output (only for --format=text). Possible values [vertical|horizontal] (default \"vertical\")\n      --transformed-only      Print only transformed column and primary key\n      --warnings              Print warnings\n</code></pre> <p>Validate command can exit with non-zero code when:</p> <ul> <li>Any error occurred</li> <li>Validate was called with <code>--warnings</code> flag and there are warnings</li> <li>Validate was called with <code>--schema</code> flag and there are schema differences</li> </ul> <p>All of those cases may be used for CI/CD pipelines to stop the process when something went wrong. This is especially useful when <code>--schema</code> flag is used - this allows to avoid data leakage when schema changed.</p> <p>You can use the <code>--table</code> flag multiple times to specify the tables you want to check. Tables can be written with or without schema names (e. g., <code>public.table_name</code> or <code>table_name</code>). If you specify multiple tables from different schemas, an error will be thrown.</p> <p>To start validation, use the following command:</p> <pre><code>greenmask --config=config.yml validate \\\n  --warnings \\\n  --data \\\n  --diff \\\n  --schema \\\n  --format=text \\\n  --table-format=vertical \\\n  --transformed-only \\\n  --rows-limit=1\n</code></pre> Validation output example<pre><code>2024-03-15T19:46:12+02:00 WRN ValidationWarning={\"hash\":\"aa808fb574a1359c6606e464833feceb\",\"meta\":{\"ColumnName\":\"birthdate\",\"ConstraintDef\":\"CHECK (birthdate \\u003e= '1930-01-01'::date AND birthdate \\u003c= (now() - '18 years'::interval))\",\"ConstraintName\":\"humanresources\",\"ConstraintSchema\":\"humanresources\",\"ConstraintType\":\"Check\",\"ParameterName\":\"column\",\"SchemaName\":\"humanresources\",\"TableName\":\"employee\",\"TransformerName\":\"NoiseDate\"},\"msg\":\"possible constraint violation: column has Check constraint\",\"severity\":\"warning\"}\n</code></pre> <p>The validation output will provide detailed information about potential constraint violations and schema issues. Each line contains nested JSON data under the <code>ValidationWarning</code> key, offering insights into the affected part of the configuration and potential constraint violations.</p> <p>Pretty formatted validation warning<pre><code>{ \n  \"hash\": \"aa808fb574a1359c6606e464833feceb\", // (13)\n  \"meta\": { // (1)\n    \"ColumnName\": \"birthdate\", // (2)\n    \"ConstraintDef\": \"CHECK (birthdate &gt;= '1930-01-01'::date AND birthdate &lt;= (now() - '18 years'::interval))\", // (3)\n    \"ConstraintName\": \"humanresources\", // (4)\n    \"ConstraintSchema\": \"humanresources\", // (5)\n    \"ConstraintType\": \"Check\", // (6)\n    \"ParameterName\": \"column\", // (7)\n    \"SchemaName\": \"humanresources\", // (8)\n    \"TableName\": \"employee\", // (9)\n    \"TransformerName\": \"NoiseDate\" // (10)\n  },\n  \"msg\": \"possible constraint violation: column has Check constraint\", // (11)\n  \"severity\": \"warning\" // (12)\n}\n</code></pre></p> <ol> <li>Detailed metadata. The validation output provides comprehensive metadata to pinpoint the source of problems.</li> <li>Column name indicates the name of the affected column.</li> <li>Constraint definition specifies the definition of the constraint that may be violated.</li> <li>Constraint name identifies the name of the constraint that is potentially violated.</li> <li>Constraint schema name indicates the schema in which the constraint is defined.</li> <li>Type of constraint represents the type of constraint and can be one of the following:    <pre><code>* ForeignKey\n* Check\n* NotNull\n* PrimaryKey\n* PrimaryKeyReferences\n* Unique\n* Length\n* Exclusion\n* TriggerConstraint\n</code></pre></li> <li>Table schema name specifies the schema name of the affected table.</li> <li>Table name identifies the name of the table where the problem occurs.</li> <li>Transformer name indicates the name of the transformer responsible for the transformation.</li> <li>Name of affected parameter typically, this is the name of the column parameter that is relevant to the     validation warning.</li> <li>Validation warning description provides a detailed description of the validation warning and the reason behind     it.</li> <li>Severity of validation warning indicates the severity level of the validation warning and can be one of the     following:     <pre><code>* error\n* warning\n* info\n* debug\n</code></pre></li> <li>Hash is a unique identifier of the validation warning. It is used to resolve the warning in the config file</li> </ol> <p>Note</p> <p>A validation warning with a severity level of <code>\"error\"</code> is considered critical and must be addressed before the dump operation can proceed. Failure to resolve such warnings will prevent the dump operation from being executed.</p> Schema diff changed output example<pre><code>2024-03-15T19:46:12+02:00 WRN Database schema has been changed Hint=\"Check schema changes before making new dump\" PreviousDumpId=1710520855501\n2024-03-15T19:46:12+02:00 WRN Column renamed Event=ColumnRenamed Signature={\"CurrentColumnName\":\"id1\",\"PreviousColumnName\":\"id\",\"TableName\":\"test\",\"TableSchema\":\"public\"}\n2024-03-15T19:46:12+02:00 WRN Column type changed Event=ColumnTypeChanged Signature={\"ColumnName\":\"id\",\"CurrentColumnType\":\"bigint\",\"CurrentColumnTypeOid\":\"20\",\"PreviousColumnType\":\"integer\",\"PreviousColumnTypeOid\":\"23\",\"TableName\":\"test\",\"TableSchema\":\"public\"}\n2024-03-15T19:46:12+02:00 WRN Column created Event=ColumnCreated Signature={\"ColumnName\":\"name\",\"ColumnType\":\"text\",\"TableName\":\"test\",\"TableSchema\":\"public\"}\n2024-03-15T19:46:12+02:00 WRN Table created Event=TableCreated Signature={\"SchemaName\":\"public\",\"TableName\":\"test1\",\"TableOid\":\"20563\"}\n</code></pre> <p>Example of validation diff:</p> <p></p> <p>The validation diff is presented in a neatly formatted table. In this table:</p> <ul> <li>Columns that are affected by the transformation are highlighted with a red background.</li> <li>The pre-transformation values are displayed in green.</li> <li>The post-transformation values are shown in red.</li> <li>The result in <code>--format=text</code> can be displayed in either horizontal (<code>--table-format=horizontal</code>) or    vertical (<code>--table-format=vertical</code>) format, making it easy to visualize and understand the    differences between the original and transformed data.</li> </ul> <p>The whole validate command may be run in json format including logging making easy to parse the structure. </p> <pre><code>greenmask --config=config.yml validate \\\n  --warnings \\\n  --data \\\n  --diff \\\n  --schema \\\n  --format=json \\\n  --table-format=vertical \\\n  --transformed-only \\\n  --rows-limit=1 \\\n  --log-format=json\n</code></pre> <p>The json object result</p> The validation warningSchema diff eventsTransformation diff line <pre><code>{\n  \"level\": \"warn\",\n  \"ValidationWarning\": {\n    \"msg\": \"possible constraint violation: column has Check constraint\",\n    \"severity\": \"warning\",\n    \"meta\": {\n      \"ColumnName\": \"birthdate\",\n      \"ConstraintDef\": \"CHECK (birthdate &gt;= '1930-01-01'::date AND birthdate &lt;= (now() - '18 years'::interval))\",\n      \"ConstraintName\": \"humanresources\",\n      \"ConstraintSchema\": \"humanresources\",\n      \"ConstraintType\": \"Check\",\n      \"ParameterName\": \"column\",\n      \"SchemaName\": \"humanresources\",\n      \"TableName\": \"employee\",\n      \"TransformerName\": \"NoiseDate\"\n    },\n    \"hash\": \"aa808fb574a1359c6606e464833feceb\"\n  },\n  \"time\": \"2024-03-15T20:01:51+02:00\"\n}\n</code></pre> <pre><code>{\n  \"level\": \"warn\",\n  \"PreviousDumpId\": \"1710520855501\",\n  \"Diff\": [\n    {\n      \"event\": \"ColumnRenamed\",\n      \"signature\": {\n        \"CurrentColumnName\": \"id1\",\n        \"PreviousColumnName\": \"id\",\n        \"TableName\": \"test\",\n        \"TableSchema\": \"public\"\n      }\n    },\n    {\n      \"event\": \"ColumnTypeChanged\",\n      \"signature\": {\n        \"ColumnName\": \"id\",\n        \"CurrentColumnType\": \"bigint\",\n        \"CurrentColumnTypeOid\": \"20\",\n        \"PreviousColumnType\": \"integer\",\n        \"PreviousColumnTypeOid\": \"23\",\n        \"TableName\": \"test\",\n        \"TableSchema\": \"public\"\n      }\n    },\n    {\n      \"event\": \"ColumnCreated\",\n      \"signature\": {\n        \"ColumnName\": \"name\",\n        \"ColumnType\": \"text\",\n        \"TableName\": \"test\",\n        \"TableSchema\": \"public\"\n      }\n    },\n    {\n      \"event\": \"TableCreated\",\n      \"signature\": {\n        \"SchemaName\": \"public\",\n        \"TableName\": \"test1\",\n        \"TableOid\": \"20563\"\n      }\n    }\n  ],\n  \"Hint\": \"Check schema changes before making new dump\",\n  \"time\": \"2024-03-15T20:01:51+02:00\",\n  \"message\": \"Database schema has been changed\"\n}\n</code></pre> <pre><code>{\n  \"schema\": \"humanresources\",\n  \"name\": \"employee\",\n  \"primary_key_columns\": [\n    \"businessentityid\"\n  ],\n  \"with_diff\": true,\n  \"transformed_only\": true,\n  \"records\": [\n    {\n      \"birthdate\": {\n        \"original\": \"1969-01-29\",\n        \"transformed\": \"1964-10-20\",\n        \"equal\": false,\n        \"implicit\": true\n      },\n      \"businessentityid\": {\n        \"original\": \"1\",\n        \"transformed\": \"1\",\n        \"equal\": true,\n        \"implicit\": true\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"release_notes/greenmask_0_1_0/","title":"Greenmask 0.1.0","text":"<p>We are excited to announce the release of Greenmask v0.1.0, marking the first production-ready version. This release addresses various bug fixes, introduces improvements, and includes documentation refactoring for enhanced clarity.</p>"},{"location":"release_notes/greenmask_0_1_0/#new-features","title":"New features","text":"<ul> <li> <p>Added positional arguments for the list-transformers command, allowing specific transformer information retrieval (e.g., <code>greenmask list-transformers RandomDate</code>).</p> </li> <li> <p>Added a version parameter <code>--version</code> that prints Greenmask version.</p> </li> <li> <p>Added numeric parameters support for <code>-Int</code> and <code>-Float</code> transformers.</p> </li> </ul>"},{"location":"release_notes/greenmask_0_1_0/#improvements","title":"Improvements","text":"<ul> <li> <p>Improved verbosity in custom transformer interaction, accumulating <code>stderr</code> data and forwarding it in batches instead of writing it one by one.</p> </li> <li> <p>Updated dependencies to newer versions.</p> </li> <li> <p>Enhanced the stability of the JSON line interaction protocol by utilizing the stdlib JSON encoder/decoder.</p> </li> <li> <p>Modified the method for sending table metadata to custom transformers; now, it is sent via <code>stdin</code> in the first line in JSON format instead of providing it via command arguments.</p> </li> <li> <p>Refactored template functions naming.</p> </li> <li> <p>Refactored <code>NoiseDate</code> transformer implementation for improved stability and predictability.</p> </li> <li> <p>Changed the default value for the <code>Dict</code> transformer: <code>fail_not_matched parameter: true</code>.</p> </li> <li> <p>Refactored the <code>Hash</code> transformer to provide a salt parameter and receive a base64 encoded salt. If salt is not provided, it generates one randomly.</p> </li> <li> <p>Added validation for the truncate parameter of <code>NoiseDate</code> and <code>RandomDate</code> transformers that issues a warning if the provided value is invalid.</p> </li> <li> <p>Increased verbosity of parameter validation warnings, now properly forwarding warnings to <code>stdout</code>.</p> </li> </ul>"},{"location":"release_notes/greenmask_0_1_0/#fixes","title":"Fixes","text":"<ul> <li> <p>Resolved <code>pgx</code> driver connection leakage issue.</p> </li> <li> <p>Fixed deletion failure of dumps for S3 storage.</p> </li> <li> <p>Corrected cobra autocompletion for the Greenmask utility.</p> </li> <li> <p>Fixed NOT NULL constraint validation.</p> </li> <li> <p>Addressed JSON API interaction issues that previously caused deadlocks and timeouts.</p> </li> <li> <p>Fixed encode-decoding for binary parameters, ensuring accurate forwarding of values to custom transformers.</p> </li> <li> <p>Fixed the <code>RandomChoice</code> transformer to correctly marshal and unmarshal values during validation.</p> </li> <li> <p>Introduced the nullable property for the <code>SetNull</code> transformer to enhance NOT NULL constraint validation.</p> </li> <li> <p>Resolved text wrapping issues for the <code>validate</code> command.</p> </li> <li> <p>Fixed build failures on Windows due to Linux platform dependencies.</p> </li> <li> <p>Corrected <code>stdout</code> readline buffer reading during interaction with custom transformers.</p> </li> <li> <p>Fixed integration tests.</p> </li> </ul>"},{"location":"release_notes/greenmask_0_1_0/#ecosystem-changes","title":"Ecosystem changes","text":"<ul> <li> <p>Implemented CI/CD pipelines for the entire project.</p> </li> <li> <p>Established a user-friendly playground in Docker compose, including:</p> </li> <li> <p>Deployed Minio storage container.</p> </li> <li>PostgreSQL container containing both the original database (Adventure Works) and the transformed (empty DB).</li> <li> <p>Greenmask container itself.</p> </li> <li> <p>Refactored current readme files.</p> </li> </ul>"},{"location":"release_notes/greenmask_0_1_0/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_0_beta/","title":"Greenmask 0.0.1 Beta","text":"<p>We are excited to announce the beta release of Greenmask, a versatile and open-source utility for PostgreSQL logical backup dumping, anonymization, and restoration. Greenmask is perfect for routine backup and restoration tasks. It facilitates anonymization and data masking for staging environments and analytics.</p> <p>This release introduces a range of features aimed at enhancing database management and security.</p>"},{"location":"release_notes/greenmask_0_1_0_beta/#key-features","title":"Key features","text":"<ul> <li>Cross-platform support \u2014 fully written in Go without platform dependencies.</li> <li>Type-safe database operations \u2014 validates and encodes data, maintaining integrity.</li> <li>Transformation validation \u2014 ensures data transformations are correct and maintainable.</li> <li>Partitioned table support \u2014 simplifies configuration for partitioned tables.</li> <li>Stateless and backward compatible \u2014 works alongside standard PostgreSQL utilities.</li> <li>Parallel execution \u2014 enhances efficiency in dumping and restoration processes.</li> <li>Multiple storage options \u2014 supports both local (directory) and remote (S3-like) storage solutions.</li> </ul>"},{"location":"release_notes/greenmask_0_1_0_beta/#download","title":"Download","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_1/","title":"Greenmask 0.1.1","text":"<p>This release introduces a suite of new transformers, significantly enhancing Greenmask's capabilities for obfuscating PostgreSQL databases.</p>"},{"location":"release_notes/greenmask_0_1_1/#new-features","title":"New features","text":"<p>Added the following new transformers:</p> Transformer Description RandomLatitude Generates a random latitude value RandomLongitude Generates a random longitude value RandomUnixTime Generates a random Unix timestamp RandomMonthName Generates the name of a random month RandomYearString Generates a random year as a string RandomDayOfWeek Generates a random day of the week RandomDayOfMonth Generates a random day of the month RandomCentury Generates a random century RandomTimezone Generates a random timezone RandomEmail Generates a random email address RandomMacAddress Generates a random MAC address RandomDomainName Generates a random domain name RandomURL Generates a random URL RandomUsername Generates a random username RandomIPv4 Generates a random IPv4 address RandomIPv6 Generates a random IPv6 address RandomPassword Generates a random password RandomWord Generates a random word RandomSentence Generates a random sentence RandomParagraph Generates a random paragraph RandomCCType Generates a random credit card type RandomCCNumber Generates a random credit card number RandomCurrency Generates a random currency code RandomAmountWithCurrency Generates a random monetary amount with currency RandomTitleMale Generates a random title for males RandomTitleFemale Generates a random title for females RandomFirstName Generates a random first name RandomFirstNameMale Generates a random male first name RandomFirstNameFemale Generates a random female first name RandomLastName Generates a random last name RandomName Generates a full random name RandomPhoneNumber Generates a random phone number RandomTollFreePhoneNumber Generates a random toll-free phone number RandomE164PhoneNumber Generates a random phone number in E.164 format RealAddress Generates a real address"},{"location":"release_notes/greenmask_0_1_1/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_10/","title":"Greenmask 0.1.10","text":"<p>This release introduces improvements and bug fixes</p>"},{"location":"release_notes/greenmask_0_1_10/#changes","title":"Changes","text":"<ul> <li>Fixed panic caused in <code>RandomString</code> transformer</li> <li>Fixed wrong table size calculation. Now the table size includes TOAST table size</li> <li>Added custom transformer interaction API defaults if not set</li> <li>Changed docker workdir to greenmask home</li> <li>Removed bucket name from object path prefix</li> </ul>"},{"location":"release_notes/greenmask_0_1_10/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_11/","title":"Greenmask 0.1.11","text":"<p>This release introduces improvements and bug fixes</p>"},{"location":"release_notes/greenmask_0_1_11/#changes","title":"Changes","text":"<ul> <li>Added support for generated columns in the table</li> <li>Fixed transformer parameters encoding issue caused by spf13/viper</li> <li>Fixed table scoring for transformed table</li> <li>Refactored connection management logic in restore command - fixes connection idle timeout</li> </ul>"},{"location":"release_notes/greenmask_0_1_11/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_12/","title":"Greenmask 0.1.12","text":"<p>This release introduces improvements and bug fixes</p>"},{"location":"release_notes/greenmask_0_1_12/#changes","title":"Changes","text":"<ul> <li>Fixed config decoding issue caused</li> <li>Fixed TOC entries merge behavior when data section is empty</li> <li>Fixed integration tests for S3 storage</li> </ul>"},{"location":"release_notes/greenmask_0_1_12/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_13/","title":"Greenmask 0.1.13","text":"<p>This release introduces only improvements in documentation deployment. The core greenmask utility does not contain any changes.</p>"},{"location":"release_notes/greenmask_0_1_13/#changes","title":"Changes","text":"<ul> <li>Added documentation deployment with versioning</li> </ul>"},{"location":"release_notes/greenmask_0_1_13/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_14/","title":"Greenmask 0.1.14","text":"<p>This release introduces bug fixes.</p>"},{"location":"release_notes/greenmask_0_1_14/#changes","title":"Changes","text":"<ul> <li>Fixed large panic caused in Large Object dumper</li> </ul>"},{"location":"release_notes/greenmask_0_1_14/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_2/","title":"Greenmask 0.1.2","text":"<p>This release introduces bug fixes.</p>"},{"location":"release_notes/greenmask_0_1_2/#fixes","title":"Fixes","text":"<ul> <li>Fixed bug when raw COPY lines were parsed incorrectly</li> <li>Fixed <code>--version</code> parameter behavior</li> <li>Fixed <code>--dbname</code> parameter - now it correctly works with PostgreSQL connection string in URI format <code>postgresql:///</code></li> </ul>"},{"location":"release_notes/greenmask_0_1_2/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_3/","title":"Greenmask 0.1.3","text":"<p>This release introduces bug fixes.</p>"},{"location":"release_notes/greenmask_0_1_3/#fixes","title":"Fixes","text":"<ul> <li>Fixed the JSON transformer's parsing for the <code>operations</code> fields</li> <li>Fixed database connection string builder in <code>pg_restore</code> and <code>pg_dump</code></li> </ul>"},{"location":"release_notes/greenmask_0_1_3/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_4/","title":"Greenmask 0.1.4","text":"<p>This release introduces bug fixes.</p>"},{"location":"release_notes/greenmask_0_1_4/#fixes","title":"Fixes","text":"<ul> <li>Fixed database connection string behavior fields</li> </ul>"},{"location":"release_notes/greenmask_0_1_4/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_5/","title":"Greenmask 0.1.5","text":"<p>This release introduces a new Greenmask command, improvements, bug fixes, and numerous documentation updates.</p>"},{"location":"release_notes/greenmask_0_1_5/#new-features","title":"New features","text":"<p>Added a new Greenmask CLI command\u2014show-transformer that shows detailed information about a specified transformer.</p>"},{"location":"release_notes/greenmask_0_1_5/#improvements","title":"Improvements","text":"<ul> <li>The Hash transformer has been completely remastered and now has the <code>function</code> parameter to choose from several hash algorithm options and the <code>max_length</code> parameter to truncate the hash tail.</li> <li>Split information about transformers between the <code>list-transformers</code> and new <code>show-transformer</code> CLI commands, which allows for more comprehensible and useful outputs for both commands</li> <li>Added error severity for the <code>Cmd</code> parameter validator</li> <li>Improved UX for the Greenmask release binaries</li> </ul>"},{"location":"release_notes/greenmask_0_1_5/#fixes","title":"Fixes","text":"<ul> <li>Fixed metadata enrichment for validation warnings caused by <code>RawValueValidator</code></li> <li>Fixed a typo in the <code>credit_card</code> value for the <code>type</code> parameter of the <code>Masking</code> transformer</li> <li>Fixed Greenmask Playground environment variables and the <code>cleanup</code> command</li> <li>Fixed <code>list-dump</code>, <code>list-transformers</code>, and <code>restore</code> commands exit code on error</li> </ul>"},{"location":"release_notes/greenmask_0_1_5/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_6/","title":"Greenmask 0.1.6","text":"<p>This is a minor release that introduces a bug hotfix</p>"},{"location":"release_notes/greenmask_0_1_6/#fixes","title":"Fixes","text":"<ul> <li>Fixed uncontrolled buffer growth in the restore command</li> </ul>"},{"location":"release_notes/greenmask_0_1_6/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_7/","title":"Greenmask 0.1.7","text":"<p>This release introduces a new Greenmask command, improvements, bug fixes, and documentation update.</p>"},{"location":"release_notes/greenmask_0_1_7/#new-features","title":"New features","text":"<ul> <li>Added restoration filtering by <code>--table</code>, <code>--schema</code> and <code>--exclude-schema</code> parameters</li> <li>Validate parameters without parameters validates only configuration file</li> <li>Added the <code>--schema</code> parameter, which allows to make a schema diff between the previous dump and the current. This    is useful when you want to check if the schema has changed after the migration. By controlling it we can exclude    data leakage after migration</li> <li>Validate command divided by many stages that can be controlled using parameters<ul> <li>Configuration validation</li> <li>Transformer validation</li> <li>Constraint violation check</li> <li>Data difference check</li> </ul> </li> </ul>"},{"location":"release_notes/greenmask_0_1_7/#improvements","title":"Improvements","text":"<ul> <li>Improved Hash transformer <ul> <li>Added salt parameter that can be set via config or via <code>GREENMASK_GLOBAL_SALT</code></li> <li>Added sha3 functions support in different modes (sha3-224, sha3-256, sha3-384, sha3-512)</li> </ul> </li> <li>Refactored <code>Cmd</code> transformer logic<ul> <li>Json API: Now it allows to use of column names instead of column indexes in JSON format</li> <li>Csv API: Now it can use the column order from config via column remapping</li> </ul> </li> <li>The <code>validate</code> command was rewritten almost from scratch.<ul> <li>New option <code>--transformed-only</code> - displays only columns that are transformed with primary key (if exists). This   allows to reduce the output data and make it more readable</li> <li>Implemented <code>json</code> format for output</li> <li>Added the <code>--table-format</code> parameter which is responsible for the <code>vertical</code> and <code>horizontal</code> table orientation.   This works only when <code>--format=text</code></li> <li>Added the <code>--warnings</code> parameter, if it is specified then not only fatal-warnings will be displayed, but also   those with a lower severity</li> </ul> </li> </ul>"},{"location":"release_notes/greenmask_0_1_7/#fixes","title":"Fixes","text":"<ul> <li>Fixed <code>--use-list</code> option - now it applies toc entries according to the order in list file</li> <li>Fixed <code>--use-list</code> option behaviour together with <code>--list-format</code> option (<code>json</code> or <code>text</code>). Now it   generates temporal list file in text format for providing it to the pg_restore call</li> <li>Updated documentation according to the latest changes</li> </ul>"},{"location":"release_notes/greenmask_0_1_7/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_8/","title":"Greenmask 0.1.8","text":"<p>This release introduces improvements and bug fixes</p>"},{"location":"release_notes/greenmask_0_1_8/#improvements","title":"Improvements","text":"<ul> <li>Implemented <code>--exit-on-error</code> parameter for <code>pg_restore</code> run. But it does not play for \"data\" section restoration now. If any error is caused in <code>data</code> section greenmask exits with the error whether <code>--exit-on-error</code> was provided or not. This might be fixed later</li> </ul>"},{"location":"release_notes/greenmask_0_1_8/#fixes","title":"Fixes","text":"<ul> <li>Fixed dependent objects dropping when running with the <code>restore</code> command with the <code>--clean</code> parameter. Useful when restoring and overriding only required tables</li> <li>Fixed <code>show-dump</code> command output in text mode</li> <li>Disabled CGO. Fixes problem when downloaded binary from repo cannot run</li> <li>Fixed <code>delete</code> dump operation</li> </ul>"},{"location":"release_notes/greenmask_0_1_8/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_1_9/","title":"Greenmask 0.1.9","text":"<p>This release introduces improvements and bug fixes</p>"},{"location":"release_notes/greenmask_0_1_9/#improvements","title":"Improvements","text":"<ul> <li>Implemented tables scoring according to the table size and transformation costs. This correctly spread the tables   dumping between the requested workers pool and reduces the execution time. Now greenmask introspects the table size,   adds the transformation scoring using the formula   <code>score = tableSizeInBytes + (tableSizeInBytes * 0.03 * tableTransformationsCount)</code>, and uses the strategy \"Largest   First\". The problem is described here</li> <li>Introduced <code>no_verify_ssl</code> parameter for S3 storage</li> <li>Adjusted Dockerfile<ul> <li>Changed entrypoint to <code>greenmask</code> binary</li> <li>The <code>greenmask</code> container now runs under <code>greenmask</code> user and groups</li> </ul> </li> <li>Refactored storage config structure. Now it contains the <code>type</code> that is used for the storage type determination</li> <li>Most of the attributes may be overridden with environment variables where the letters are capitalized and the dots   are replaced with underscores. For instance, the setting <code>storage.type</code> might be represented with the environment   variable <code>STORAGE_TYPE</code></li> <li>Parameter <code>--config</code> is not required anymore. This simplifies the greenmask utility user experience</li> <li>Directory storage set as the default</li> <li>Set the default temporary directory as <code>/tmp</code></li> <li>Added environment variable section to the configuration docs</li> </ul>"},{"location":"release_notes/greenmask_0_1_9/#fixes","title":"Fixes","text":"<ul> <li>Fixed <code>S3_REGION</code> environment variable usage. Tested cases where the S3 storage is set up using <code>S3</code> variables that   uses by <code>github.com/aws/aws-sdk-go</code></li> <li>Updated project dependencies to the latest version</li> </ul>"},{"location":"release_notes/greenmask_0_1_9/#assets","title":"Assets","text":"<p>To download the Greenmask binary compatible with your system, see the release's assets list.</p>"},{"location":"release_notes/greenmask_0_2_0/","title":"Greenmask 0.2.0","text":"<p>This is one of the biggest releases since Greenmask was founded. We've been in close contact with our users, gathering feedback, and working hard to make Greenmask more flexible, reliable, and user-friendly.</p> <p>This major release introduces exciting new features such as database subsetting, pgzip support, restoration in topological order, and refactored transformers, significantly enhancing Greenmask's flexibility to better meet business needs. It also includes several fixes and improvements.</p>"},{"location":"release_notes/greenmask_0_2_0/#preface","title":"Preface","text":"<p>This release is a major milestone that significantly expands Greenmask's functionality, transforming it into a simple, extensible, and reliable solution for database security, data anonymization, and everyday operations. Our goal is to create a core system that can serve as a foundation for comprehensive dynamic staging environments and robust data security.</p>"},{"location":"release_notes/greenmask_0_2_0/#notable-changes","title":"Notable changes","text":"<ul> <li> <p>PostgreSQL 17 support - revised ported library to support PostgreSQL 17</p> </li> <li> <p>Database Subset - a new feature that allows you to define a subset of the database,   allowing you to scale down the dump size (#110). This is   robust for multipurpose and especially useful for testing and development environments. It supports:</p> <ul> <li>References with NULL values - generate the LEFT JOIN query   for the FK reference with NULL values to include them in the subset.</li> <li>Supports virtual references (virtual foreign keys) - create a logical   FK in Greenmask that will be used for subset dependencies graph. The virtual reference can be defined for a column   or an expression, allowing you to get the value from JSON and similar.</li> <li>Supports circular references - Greenmask will automatically resolve   circular dependencies in the subset by generating a recursive query. The query is generated with integrity checks   of the subset ensuring that the data gathered from circular dependencies is consistent.</li> <li>Fully covered with documentation including troubleshooting   and examples.</li> <li>Supports FK and PK that have more than one column (or expression).</li> <li>Multi-cycles resolution in one strong connected component (SCC) is supported - Greenmask will generate a   recursive query for the SCC whether it is a single cycle or multiple cycles, making the subset system universal   for any database schema.</li> <li>Supports polymorphic relationships - You can define a virtual reference for a table with polymorphic references using <code>polymorphic_exprs</code> attribute and use greenmask to generate a subset for such tables.</li> </ul> </li> <li> <p>pgzip support for faster compression   and decompression \u2014 setting <code>--pgzip</code> can speed up the dump and   restoration processes through parallel compression. In some tests, it shows up to 5x faster dump and restore   operations.</p> </li> <li>Restoration in topological order - This flag ensures   that dependent tables are not restored until the tables they depend on have been restored. This is useful when you   want to be notified of errors as immediately as possible without waiting for the entire table to be restored.</li> <li> <p>Insert format restoration - For a flexible restoration   process, Greenmask now supports data restoration in the <code>INSERT</code> format. It generates the insert statements based on   <code>COPY</code> records from the dump. You do not need to re-dump your data to use this feature; it can be defined in the   <code>restore</code> command. The list of new features related to the <code>INSERT</code> format:</p> <ul> <li>Generate <code>INSERT</code> statements with the <code>**ON CONFLICT DO NOTHING**</code> clause if the flag <code>--on-conflict-do-nothing</code>   is set.</li> <li>Error exclusion list in the config to skip   certain errors and continue inserting subsequent rows from the dump.</li> <li>Use cases - incremental dump and restoration for logical data. For example, if you have a database, and you   want to insert data periodically from another source, this can be used together with the database subset and   transformations to catch up the target database.</li> </ul> </li> <li> <p>Restore data batching (#173) -   By default, the COPY protocol returns the error only on transaction commit. To override this behavior, use the   <code>--batch-size</code> flag to specify the number of rows to insert in a single batch during the COPY command. This is useful   when you want to control the transaction size and commit.</p> </li> <li> <p>Introduced <code>keep_null</code> parameter for <code>RandomPerson</code> transformer.</p> </li> <li> <p>Introduced dynamic parameters in the transformers</p> <ul> <li>Most transformers now support dynamic parameters where applicable.</li> <li>Dynamic parameters are strictly enforced. If you need to cast values to another type, Greenmask provides templates   and predefined cast functions accessible via <code>cast_to</code>. These functions cover frequent operations such as   <code>UnixTimestampToDate</code> and <code>IntToBool</code>.</li> </ul> </li> <li>The transformation logic has been significantly refactored, making transformers more customizable and flexible than   before.</li> <li> <p>Introduced transformation engines</p> <ul> <li><code>random</code> - generates transformer values based on pseudo-random algorithms.</li> <li><code>hash</code> - generates transformer values using hash functions. Currently, it utilizes <code>sha3</code> hash functions, which   are secure but perform slowly. In the stable release, there will be an option to choose between <code>sha3</code> and   <code>SipHash</code>.</li> </ul> </li> <li> <p>Introduced static parameters value template</p> </li> <li> <p>Dumps retention management - Introduced retention   parameters (#201) for the delete command. Introduced two new   statuses: failed and in progress. A dump is considered failed if it lacks a \"done\" heartbeat or   if the last heartbeat timestamp exceeds 30 minutes. The delete command now supports the following retention   parameters:</p> <ul> <li><code>--dry-run</code>: Runs the deletion operation in test mode with verbose output, without actually deleting anything.</li> <li><code>--before-date 2024-08-27T23:50:54+00:00</code>: Deletes dumps older than the specified date. The date must be provided   in RFC3339Nano format, for example: <code>2021-01-01T00:00:00Z</code>.</li> <li><code>--retain-recent 10</code>: Retains the N most recent dumps, where N is specified by the user.</li> <li><code>--retain-for 1w2d3h4m5s6ms7us8ns</code>: Retains dumps for the specified duration. The format supports weeks (w),   days (d), hours (h), minutes (m), seconds (s), milliseconds (ms), microseconds (us), and nanoseconds (ns).</li> <li><code>--prune-failed</code>: Prunes (removes) all dumps that have failed.</li> <li><code>--prune-unsafe</code>: Prunes dumps with \"unknown-or-failed\" statuses. This option only works in conjunction with   <code>--prune-failed</code>.</li> </ul> </li> <li> <p>Docker image mirroring into the GitHub Container Registry</p> </li> </ul>"},{"location":"release_notes/greenmask_0_2_0/#core","title":"Core","text":"<ul> <li>Introduced the <code>Parametrizer</code> interface, now implemented for both dynamic and static parameters.</li> <li>Renamed most of the toolkit types for enhanced clarity and comprehensive documentation coverage.</li> <li>Refactored the <code>Driver</code> initialization logic.</li> <li>Added validation warnings for overridden types in the <code>Driver</code>.</li> <li>Migrated existing built-in transformers to utilize the new <code>Parametrizer</code> interface.</li> <li>Implemented a new abstraction, <code>TransformationContext</code>, as the first step towards enabling new feature transformation   conditions (#34).</li> <li>Optimized most transformers for performance in both dynamic and static modes. While dynamic mode offers flexibility,   static mode ensures performance remains high. Using only the necessary transformation features helps keep   transformation time predictable.</li> </ul>"},{"location":"release_notes/greenmask_0_2_0/#transformers","title":"Transformers","text":"<ul> <li> <p>RandomEmail - Introduces a new transformer that   supports both random and deterministic engines. It allows for flexible email value generation; you can use column   values in the template and choose to keep the original domain or select any from the <code>domains</code> parameter.</p> </li> <li> <p>NoiseDate, NoiseFloat, NoiseInt -   These transformers support both random and deterministic engines, offering dynamic mode parameters that control the   noise thresholds within the <code>min</code> and <code>max</code> range. Unlike previous implementations which used a single <code>ratio</code>   parameter, the new release features <code>min_ratio</code> and <code>max_ratio</code> parameters to define noise values more precisely.   Utilizing the <code>hash</code> engine in these transformers enhances security by complicating statistical analysis for   attackers, especially when the same salt is used consistently over long periods.</p> </li> <li> <p>NoiseNumeric - A newly implemented transformer,   sharing features with <code>NoiseInt</code> and <code>NoiseFloat</code>, but specifically designed for numeric values (large integers or   floats). It provides a <code>decimal</code> parameter to handle values with fractions.</p> </li> <li> <p>RandomChoice - Now supports the <code>hash</code> engine</p> </li> <li> <p>RandomDate, RandomFloat, RandomInt -   Now enhanced with hash engine support. Threshold parameters <code>min</code> and <code>max</code> have been updated to support dynamic mode,   allowing for more flexible configurations.</p> </li> <li> <p>RandomNumeric - A new transformer specifically   designed for numeric types (large integers or floats), sharing similar features with <code>RandomInt</code> and <code>RandomFloat</code>,   but tailored for handling huge numeric values.</p> </li> <li> <p>RandomString - Now supports hash engine mode</p> </li> <li> <p>RandomUnixTimestamp - This new transformer   generates Unix timestamps with selectable units (<code>second</code>, <code>millisecond</code>, <code>microsecond</code>, <code>nanosecond</code>). Similar in   function to <code>RandomDate</code>, it supports the hash engine and dynamic parameters for <code>min</code> and <code>max</code> thresholds, with the   ability to override these units using <code>min_unit</code> and <code>max_unit</code> parameters.</p> </li> <li> <p>RandomUuid - Added hash engine support</p> </li> <li> <p>RandomPerson - Implemented a new transformer that   replaces <code>RandomName</code>, <code>RandomLastName</code>, <code>RandomFirstName</code>, <code>RandomFirstNameMale</code>, <code>RandomFirstNameFemale</code>,   <code>RandomTitleMale</code>, and <code>RandomTitleFemale</code>. This new transformer offers enhanced customizability while providing   similar functionalities as the previous versions. It generates personal data such as <code>FirstName</code>, <code>LastName</code>, and   <code>Title</code>, based on the provided <code>gender</code> parameter, which now supports dynamic mode. Future minor versions will allow   for overriding the default names database.</p> </li> <li> <p>Added tsModify - a new   template function for time.Time objects modification</p> </li> <li> <p>Introduced a new RandomIp transformer capable of   generating a random IP address based on the specified netmask.</p> </li> <li> <p>Added a new RandomMac transformer for generating   random Mac addresses.</p> </li> <li> <p>Deleted transformers include <code>RandomMacAddress</code>, <code>RandomIPv4</code>, <code>RandomIPv6</code>, <code>RandomUnixTime</code>, <code>RandomTitleMale</code>,   <code>RandomTitleFemale</code>, <code>RandomFirstName</code>, <code>RandomFirstNameMale</code>, <code>RandomFirstNameFemale</code>, <code>RandomLastName</code>, and   <code>RandomName</code> due to the introduction of more flexible and unified options.</p> </li> </ul>"},{"location":"release_notes/greenmask_0_2_0/#fixes-and-improvements","title":"Fixes and improvements","text":"<ul> <li>Fixed <code>validate</code> command with the <code>--table</code> flag, which had the   wrong order of the table name representation <code>{{ table_name }}.{{ schema }}</code> instead of   <code>{{ schema }}.{{ table_name }}</code>.</li> <li>Fixed <code>Row.SetColumn</code> out of range validation.</li> <li>Fixed <code>restoreWorker</code> panic caused when the worker received an error from pgx.</li> <li>Fixed error   handling in the <code>restore</code> command.</li> <li>Fixed restore   jobs now start a transaction for each table restoration and commit it after the table restoration is done.</li> <li>Fixed <code>--exit-on-error</code> works incorrectly in the <code>restore</code> command. Now, the <code>--exit-on-error</code> flag works correctly with the   <code>data</code> section.</li> <li>Fixed transaction rollback in the <code>validate</code> command.</li> <li>Fixed typo in documentation.</li> <li>Fixed a CI/CD bug related to retrieving current tags.</li> <li>Fixed the Docker image tag for <code>latest</code> to exclude specific   keywords.</li> <li>Fixed a case where the hashing value was not set for each column   in the <code>RandomPerson</code> transformer.</li> <li>Fixed original email value parsing conditions.</li> <li>Subset docs revision.</li> <li>Fixes a case where data entries were excluded by exclusion   parameters such as <code>--exclude-table</code>, <code>--table</code>, etc.</li> <li>Fixed zero bytes that were written in the buffer due to the wrong   buffer limit in the <code>Email</code> transformer.</li> <li>Fixed a case where the overridden type of column via   <code>columns_type_override</code> did not work.</li> <li>Fixed a case where an unknown option provided in the config was   just ignored instead of throwing an error.</li> <li>Fixed a case where <code>min</code> and <code>max</code> parameter values were ignored   in transformers <code>NoiseDate</code>, <code>NoiseNumeric</code>, <code>NoiseFloat</code>, <code>NoiseInt</code>, <code>RandomNumeric</code>, <code>RandomFloat</code>, and   <code>RandomInt</code>.</li> <li>Fixed TOC entry COPY restoration statement - added missing   newline and semicolon. Now backward pg_dump call <code>pg_restore 1724504511561 --file 1724504511561.sql</code> is backward   compatible and works as expected.</li> <li>Fixed a case where dump/restore fails when masking tables with a   generated column.</li> <li>Updated go version (v1.22) and dependencies</li> <li>Revised installation section of doc</li> <li>PostgreSQL 17 support - revised ported library to support PostgreSQL 17</li> <li>Fixed integration tests - reset the go test cache on each iteration</li> <li>Push docker images to ghcr.io registry</li> <li>A bunch of refactoring and code cleanup to make the codebase more maintainable and readable.</li> </ul>"},{"location":"release_notes/greenmask_0_2_0/#full-changelog-v0114v020","title":"Full Changelog: v0.1.14...v0.2.0","text":""},{"location":"release_notes/greenmask_0_2_0/#links","title":"Links","text":"<p>Feel free to reach out to us if you have any questions or need assistance:</p> <ul> <li>Greenmask Roadmap</li> <li>Email</li> <li>Twitter</li> <li>Telegram</li> <li>Discord</li> <li>DockerHub</li> </ul>"},{"location":"release_notes/greenmask_0_2_0_b1/","title":"Greenmask 0.2.0b1 (pre-release)","text":"<p>This major beta release introduces new features and refactored transformers, significantly enhancing Greenmask's flexibility to better meet business needs.</p>"},{"location":"release_notes/greenmask_0_2_0_b1/#changes-overview","title":"Changes overview","text":"<ul> <li>Introduced dynamic parameters in the transformers<ul> <li>Most transformers now support dynamic parameters where applicable.</li> <li>Dynamic parameters are strictly enforced. If you need to cast values to another type, Greenmask provides templates   and predefined cast functions accessible via <code>cast_to</code>. These functions cover frequent operations such as   <code>UnixTimestampToDate</code> and <code>IntToBool</code>.</li> </ul> </li> <li>The transformation logic has been significantly refactored, making transformers more customizable and flexible than   before.</li> <li> <p>Introduced transformation engines</p> <ul> <li><code>random</code> - generates transformer values based on pseudo-random algorithms.</li> <li><code>hash</code> - generates transformer values using hash functions. Currently, it utilizes <code>sha3</code> hash functions, which   are secure but perform slowly. In the stable release, there will be an option to choose between <code>sha3</code> and   <code>SipHash</code>.</li> </ul> </li> <li> <p>Introduced static parameters value template</p> </li> </ul>"},{"location":"release_notes/greenmask_0_2_0_b1/#notable-changes","title":"Notable changes","text":""},{"location":"release_notes/greenmask_0_2_0_b1/#core","title":"Core","text":"<ul> <li>Introduced the <code>Parametrizer</code> interface, now implemented for both dynamic and static parameters.</li> <li>Renamed most of the toolkit types for enhanced clarity and comprehensive documentation coverage.</li> <li>Refactored the <code>Driver</code> initialization logic.</li> <li>Added validation warnings for overridden types in the <code>Driver</code>.</li> <li>Migrated existing built-in transformers to utilize the new <code>Parametrizer</code> interface.</li> <li>Implemented a new abstraction, <code>TransformationContext</code>, as the first step towards enabling new feature transformation   conditions (#34).</li> <li>Optimized most transformers for performance in both dynamic and static modes. While dynamic mode offers flexibility,   static mode ensures performance remains high. Using only the necessary transformation features helps keep   transformation time predictable.</li> </ul>"},{"location":"release_notes/greenmask_0_2_0_b1/#documentation","title":"Documentation","text":"<p>Documentation has been significantly refactored. New information about features and updates to transformer descriptions have been added.</p>"},{"location":"release_notes/greenmask_0_2_0_b1/#transformers","title":"Transformers","text":"<ul> <li> <p>RandomEmail - Introduces a new transformer that   supports both random and deterministic engines. It allows for flexible email value generation; you can use column   values in the template and choose to keep the original domain or select any from the <code>domains</code> parameter.</p> </li> <li> <p>NoiseDate, NoiseFloat, NoiseInt -   These transformers support both random and deterministic engines, offering dynamic mode parameters that control the   noise thresholds within the <code>min</code> and <code>max</code> range. Unlike previous implementations which used a single <code>ratio</code>   parameter, the new release features <code>min_ratio</code> and <code>max_ratio</code> parameters to define noise values more precisely.   Utilizing the <code>hash</code> engine in these transformers enhances security by complicating statistical analysis for   attackers, especially when the same salt is used consistently over long periods.</p> </li> <li> <p>NoiseNumeric - A newly implemented transformer,   sharing features with <code>NoiseInt</code> and <code>NoiseFloat</code>, but specifically designed for numeric values (large integers or   floats). It provides a <code>decimal</code> parameter to handle values with fractions.</p> </li> <li> <p>RandomChoice - Now supports the <code>hash</code> engine</p> </li> <li> <p>RandomDate, RandomFloat, RandomInt -   Now enhanced with hash engine support. Threshold parameters <code>min</code> and <code>max</code> have been updated to support dynamic mode,   allowing for more flexible configurations.</p> </li> <li> <p>RandomNumeric - A new transformer specifically   designed for numeric types (large integers or floats), sharing similar features with <code>RandomInt</code> and <code>RandomFloat</code>,   but tailored for handling huge numeric values.</p> </li> <li> <p>RandomString - Now supports hash engine mode</p> </li> <li> <p>RandomUnixTimestamp - This new transformer   generates Unix timestamps with selectable units (<code>second</code>, <code>millisecond</code>, <code>microsecond</code>, <code>nanosecond</code>). Similar in   function to <code>RandomDate</code>, it supports the hash engine and dynamic parameters for <code>min</code> and <code>max</code> thresholds, with the   ability to override these units using <code>min_unit</code> and <code>max_unit</code> parameters.</p> </li> <li> <p>RandomUuid - Added hash engine support</p> </li> <li> <p>RandomPerson - Implemented a new transformer that   replaces <code>RandomName</code>, <code>RandomLastName</code>, <code>RandomFirstName</code>, <code>RandomFirstNameMale</code>, <code>RandomFirstNameFemale</code>,   <code>RandomTitleMale</code>, and <code>RandomTitleFemale</code>. This new transformer offers enhanced customizability while providing   similar functionalities as the previous versions. It generates personal data such as <code>FirstName</code>, <code>LastName</code>, and   <code>Title</code>, based on the provided <code>gender</code> parameter, which now supports dynamic mode. Future minor versions will allow   for overriding the default names database.</p> </li> <li> <p>Added tsModify - a new   template function for time.Time objects modification</p> </li> <li> <p>Introduced a new RandomIp transformer capable of   generating a random IP address based on the specified netmask.</p> </li> <li> <p>Added a new RandomMac transformer for generating   random Mac addresses.</p> </li> <li> <p>Deleted transformers include <code>RandomMacAddress</code>, <code>RandomIPv4</code>, <code>RandomIPv6</code>, <code>RandomUnixTime</code>, <code>RandomTitleMale</code>,   <code>RandomTitleFemale</code>, <code>RandomFirstName</code>, <code>RandomFirstNameMale</code>, <code>RandomFirstNameFemale</code>, <code>RandomLastName</code>, and   <code>RandomName</code> due to the introduction of more flexible and unified options.</p> </li> </ul>"},{"location":"release_notes/greenmask_0_2_0_b1/#full-changelog-v0114v020b1","title":"Full Changelog: v0.1.14...v0.2.0b1","text":""},{"location":"release_notes/greenmask_0_2_0_b1/#playground-usage-for-beta-version","title":"Playground usage for beta version","text":"<p>If you want to run a Greenmask playground for the beta version v0.2.0b1 execute:</p> <pre><code>git checkout tags/v0.2.0b1 -b v0.2.0b1\ndocker-compose run greenmask-from-source\n</code></pre>"},{"location":"release_notes/greenmask_0_2_0_b1/#links","title":"Links","text":"<p>Feel free to reach out to us if you have any questions or need assistance:</p> <ul> <li>Greenmask Roadmap</li> <li>Email</li> <li>Twitter</li> <li>Telegram</li> <li>Discord</li> <li>DockerHub</li> </ul>"},{"location":"release_notes/greenmask_0_2_0_b2/","title":"Greenmask 0.2.0b2 (pre-release)","text":"<p>This major beta release introduces new features such as the database subset, pgzip support, restoration in topological and many more. It also includes fixes and improvements.</p>"},{"location":"release_notes/greenmask_0_2_0_b2/#preface","title":"Preface","text":"<p>This release is a major milestone that significantly expands Greenmask's functionality, transforming it into a simple, extensible, and reliable solution for database security, data anonymization, and everyday operations. Our goal is to create a core system that can serve as a foundation for comprehensive dynamic staging environments and robust data security.</p>"},{"location":"release_notes/greenmask_0_2_0_b2/#notable-changes","title":"Notable changes","text":"<ul> <li> <p>Database Subset - a new feature that allows you to define a subset of the database,   allowing you to scale down the dump size (#110). This is   robust for multipurpose and especially useful for testing and development environments. It supports:</p> <ul> <li>References with NULL values - generate the LEFT JOIN query   for the FK reference with NULL values to include them in the subset.</li> <li>Supports virtual references (virtual foreign keys) - create a logical   FK in Greenmask that will be used for subset dependencies graph. The virtual reference can be defined for a column   or an expression, allowing you to get the value from JSON and similar.</li> <li>Supports circular references - Greenmask will automatically resolve   circular dependencies in the subset by generating a recursive query. The query is generated with integrity checks   of the subset ensuring that the data gathered from circular dependencies is consistent.</li> <li>Fully covered with documentation including troubleshooting   and examples.</li> <li>Supports FK and PK that have more than one column (or expression).</li> <li>Multi-cycles resolution in one strong connected component (SCC) is supported - Greenmask will generate a   recursive query for the SCC whether it is a single cycle or multiple cycles, making the subset system universal   for any database schema.</li> </ul> </li> <li> <p>pgzip support for faster compression   and decompression \u2014 setting <code>--pgzip</code> can speed up the dump and   restoration processes through parallel compression. In some tests, it shows up to 5x faster dump and restore   operations.</p> </li> <li>Restoration in topological order - This flag ensures   that dependent tables are not restored until the tables they depend on have been restored. This is useful when you   want to be notified of errors as immediately as possible without waiting for the entire table to be restored.</li> <li> <p>Insert format restoration - For a flexible restoration   process, Greenmask now supports data restoration in the <code>INSERT</code> format. It generates the insert statements based on   <code>COPY</code> records from the dump. You do not need to re-dump your data to use this feature; it can be defined in the   <code>restore</code> command. The list of new features related to the <code>INSERT</code> format:</p> <ul> <li>Generate <code>INSERT</code> statements with the <code>**ON CONFLICT DO NOTHING**</code> clause if the flag <code>--on-conflict-do-nothing</code>   is set.</li> <li>Error exclusion list in the config to skip   certain errors and continue inserting subsequent rows from the dump.</li> <li>Use cases - incremental dump and restoration for logical data. For example, if you have a database, and you   want to insert data periodically from another source, this can be used together with the database subset and   transformations to catch up the target database.</li> </ul> </li> <li> <p>Restore data batching (#173) -   By default, the COPY protocol returns the error only on transaction commit. To override this behavior, use the   <code>--batch-size</code> flag to specify the number of rows to insert in a single batch during the COPY command. This is useful   when you want to control the transaction size and commit.</p> </li> <li>Introduced <code>keep_null</code> parameter for <code>RandomPerson</code> transformer.</li> </ul>"},{"location":"release_notes/greenmask_0_2_0_b2/#fixes-and-improvements","title":"Fixes and improvements","text":"<ul> <li>Fixed <code>validate</code> command with the <code>--table</code> flag, which had the   wrong order of the table name representation <code>{{ table_name }}.{{ schema }}</code> instead of   <code>{{ schema }}.{{ table_name }}</code>.</li> <li>Fixed <code>Row.SetColumn</code> out of range validation.</li> <li>Fixed <code>restoreWorker</code> panic caused when the worker received an error from pgx.</li> <li>Fixed error   handling in the <code>restore</code> command.</li> <li>Fixed restore   jobs now start a transaction for each table restoration and commit it after the table restoration is done.</li> <li>Fixed <code>--exit-on-error</code> works incorrectly in the <code>restore</code> command. Now, the <code>--exit-on-error</code> flag works correctly with the   <code>data</code> section.</li> <li>Fixed transaction rollback in the <code>validate</code> command.</li> <li>Fixed typo in documentation.</li> <li>Fixed a CI/CD bug related to retrieving current tags.</li> <li>Fixed the Docker image tag for <code>latest</code> to exclude specific   keywords.</li> <li>Fixed a case where the hashing value was not set for each column   in the <code>RandomPerson</code> transformer.</li> <li>Fixed original email value parsing conditions.</li> <li>Subset docs revision.</li> <li>Fixes a case where data entries were excluded by exclusion   parameters such as <code>--exclude-table</code>, <code>--table</code>, etc.</li> <li>Fixed zero bytes that were written in the buffer due to the wrong   buffer limit in the <code>Email</code> transformer.</li> <li>Fixed a case where the overridden type of column via   <code>columns_type_override</code> did not work.</li> <li>Fixed a case where an unknown option provided in the config was   just ignored instead of throwing an error.</li> <li>Fixed a case where <code>min</code> and <code>max</code> parameter values were ignored   in transformers <code>NoiseDate</code>, <code>NoiseNumeric</code>, <code>NoiseFloat</code>, <code>NoiseInt</code>, <code>RandomNumeric</code>, <code>RandomFloat</code>, and   <code>RandomInt</code>.</li> <li>Fixed TOC entry COPY restoration statement - added missing   newline and semicolon. Now backward pg_dump call <code>pg_restore 1724504511561 --file 1724504511561.sql</code> is backward   compatible and works as expected.</li> <li>Fixed a case where dump/restore fails when masking tables with a   generated column.</li> <li>Updated go version (v1.22) and dependencies</li> <li>Revised installation section of doc</li> <li>A bunch of refactoring and code cleanup to make the codebase more maintainable and readable.</li> </ul>"},{"location":"release_notes/greenmask_0_2_0_b2/#full-changelog-v020b1v020b2","title":"Full Changelog: v0.2.0b1...v0.2.0b2","text":""},{"location":"release_notes/greenmask_0_2_0_b2/#playground-usage-for-beta-version","title":"Playground usage for beta version","text":"<p>If you want to run a Greenmask playground for the beta version v0.2.0b2 execute:</p> <pre><code>git checkout tags/v0.2.0b2 -b v0.2.0b2\ndocker-compose run greenmask-from-source\n</code></pre>"},{"location":"release_notes/greenmask_0_2_0_b2/#links","title":"Links","text":"<p>Feel free to reach out to us if you have any questions or need assistance:</p> <ul> <li>Greenmask Roadmap</li> <li>Email</li> <li>Twitter</li> <li>Telegram</li> <li>Discord</li> <li>DockerHub</li> </ul>"},{"location":"release_notes/greenmask_0_2_1/","title":"Greenmask 0.2.1","text":"<p>This release introduces two new features transformation conditions and transformation inheritance for primary and foreign keys. It also includes several bug fixes and improvements.</p>"},{"location":"release_notes/greenmask_0_2_1/#changes","title":"Changes","text":"<ul> <li>Feat: Transformation conditions   execute a defined transformation only if a specified condition is   met. #133</li> <li>Feat: Transformation inheritance - transformation    inheritance for partitioned tables and tables with foreign keys. Define once and apply to all.</li> <li>CI/CD: Add golangci-lint job to pull request check #223</li> <li>CI/CD: Deploy development version of the documentation (main branch) and divided jobs into separate blocks and made them   reusable #212</li> <li>Fix: Fixed type in subset documentation #211</li> <li>Fix: Bump go and python dependencies #219</li> <li>Fix: Fatal validation error in playground #224</li> <li>Fix: Code refactoring and golangci-lint warns fixes #226</li> <li>Docs: Revised README.md - added badges, updated the description, added getting started section, added greenmask design   schema #216 #217 #218</li> <li>Docs: main page errors in docs #221</li> <li>Docs: Revised README.md according to the latest changes #225</li> <li>Docs: moved documentation to docs.greenmask.io, added feedback form and GA   integration #220</li> </ul>"},{"location":"release_notes/greenmask_0_2_1/#full-changelog-v020v021","title":"Full Changelog: v0.2.0...v0.2.1","text":""},{"location":"release_notes/greenmask_0_2_1/#links","title":"Links","text":"<p>Feel free to reach out to us if you have any questions or need assistance:</p> <ul> <li>Greenmask Roadmap</li> <li>Email</li> <li>Twitter</li> <li>Telegram</li> <li>Discord</li> <li>DockerHub</li> </ul>"},{"location":"release_notes/greenmask_0_2_2/","title":"Greenmask 0.2.2","text":"<p>This release introduces bug fixes.</p>"},{"location":"release_notes/greenmask_0_2_2/#changes","title":"Changes","text":"<ul> <li>Fixed a case when apply_for_references applies validation to all transformations even if they are not    marked as apply_for_references #236.</li> <li>Fixed issue with the latest tag disappearing in the documentation #234.</li> </ul>"},{"location":"release_notes/greenmask_0_2_2/#full-changelog-v021v022","title":"Full Changelog: v0.2.1...v0.2.2","text":""},{"location":"release_notes/greenmask_0_2_2/#links","title":"Links","text":"<p>Feel free to reach out to us if you have any questions or need assistance:</p> <ul> <li>Greenmask Roadmap</li> <li>Email</li> <li>Twitter</li> <li>Telegram</li> <li>Discord</li> <li>DockerHub</li> </ul>"}]}